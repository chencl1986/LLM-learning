{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90c6730f-5d76-450b-9788-ec883d024f57",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调训练入门\n",
    "\n",
    "本示例将介绍基于 Transformers 实现模型微调训练的主要流程，包括：\n",
    "- 数据集下载\n",
    "- 数据预处理\n",
    "- 训练超参数配置\n",
    "- 训练评估指标设置\n",
    "- 训练器基本介绍\n",
    "- 实战训练\n",
    "- 模型保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b1e12-1921-4438-8d5d-9760a629dcfe",
   "metadata": {},
   "source": [
    "## YelpReviewFull 数据集\n",
    "\n",
    "**Hugging Face 数据集：[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**\n",
    "\n",
    "### 数据集摘要\n",
    "\n",
    "Yelp评论数据集包括来自Yelp的评论。它是从Yelp Dataset Challenge 2015数据中提取的。\n",
    "\n",
    "### 支持的任务和排行榜\n",
    "文本分类、情感分类：该数据集主要用于文本分类：给定文本，预测情感。\n",
    "\n",
    "### 语言\n",
    "这些评论主要以英语编写。\n",
    "\n",
    "### 数据集结构\n",
    "\n",
    "#### 数据实例\n",
    "一个典型的数据点包括文本和相应的标签。\n",
    "\n",
    "来自YelpReviewFull测试集的示例如下：\n",
    "\n",
    "```json\n",
    "{\n",
    "    'label': 0,\n",
    "    'text': 'I got \\'new\\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\\\nI took the tire over to Flynn\\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\\'d give me a new tire \\\\\"this time\\\\\". \\\\nI will never go back to Flynn\\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'\n",
    "}\n",
    "```\n",
    "\n",
    "#### 数据字段\n",
    "\n",
    "- 'text': 评论文本使用双引号（\"）转义，任何内部双引号都通过2个双引号（\"\"）转义。换行符使用反斜杠后跟一个 \"n\" 字符转义，即 \"\\n\"。\n",
    "- 'label': 对应于评论的分数（介于1和5之间）。\n",
    "\n",
    "#### 数据拆分\n",
    "\n",
    "Yelp评论完整星级数据集是通过随机选取每个1到5星评论的130,000个训练样本和10,000个测试样本构建的。总共有650,000个训练样本和50,000个测试样本。\n",
    "\n",
    "## 下载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf72d6c-7ea5-4ee1-969a-c5060b9cb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec6fc806-1395-42dd-8121-a6e98a95cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c94ad529-1604-48bd-8c8d-aa2f3bca6200",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 3,\n",
       " 'text': \"All in favor of a deep dish pizza say I!.......IIIIIII,  ok now that i have that out of my system. This place is such a great hangout/eat-in spot. I hadn't been here and years and some friends invited us out for the evening. I was so glad they were paying cause  I was low on funds at the time.\\\\n\\\\nWe arrived on a friday night and of course it was busy there. We waited about 10 minutes to get a table which wasn't bad considering the crowd. We looked over the menu and they have so many great choices. Pizza, pasta, appetizers, seafood, burgers, salads and sandwiches. \\\\n\\\\nAfter ordering two mango lemonades that were wayyyyy over sweetened we ordered our food. We both are going gluten free which is tough but UNO's gave us a nice selection of dishes to choose from. Plus! They make a thin crust gluten free pizza which taste great. My hubby ordered the mediterrean thin crust because he loves kalamata olives and I ordered the Guac-alicious burger with a Caesar side salad. My salad came out pretty quick which was nice but it had a little too much dressing on it. I didn't complain, it still tasted great.\\\\n\\\\nI'm not into red meat so I tried to order a black bean burger or get chicken instead of beef, but the ran out of black bean and they couldn't get the chicken so i just ordered it anyway. The burger was piled really high with all the toppings including guacamole and it was very creamy but i couldn't get over the taste of the burger because it just didn't have any flavor. Very saddening. I ended up just eating the veggies and discarding the meat. I snacked on some of my hubbies pizza even though it was only a small amount. \\\\n\\\\nWe came here twice in one week. The second time we ordered the 9-grain deep dish with mushrooms, parmesan and a garlic white sauce. Was this pizza amazing or what?? I will probably always eat this pizza whenever I come. \\\\n\\\\nOnly down side is slow service. It took 20 minutes for our pizza to come out and my hubbies was a little over cooked. He got the numero uno which was ok but mine had way more flavor!!\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe7f2a6-d64c-489e-af1d-d6aeaed6b87d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 2,\n",
       " 'text': 'Just ate there, right next to GameStop & Google, has 3 small booths, & ordered the pepper steak w/ onion ($10.95). Food is fast fresh & hot, but mine had too much onion & not enough steak. At the end of the meal I was just eating onions with rice, though I hear this is healthy for you. Counter lady was cordial, but didn\\'t reply when customers told her, \\\\\"Have a nice day\\\\\" #awkward. I know that English isn\\'t her first language but she needs to catch on that people are wishing her well. Wasn\\'t stuffed full either despite having eaten a large plate (I usually get this feeling eating Asian). This is basically a nice place to go for lunch that won\\'t ruin your appetite for dinner. (Side note: Food is very clean. Brushed my teeth an hour before w/ Tom\\'s of Maine fluoride-free peppermint & still had minty fresh breath an hour after eating)'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dc45997-e391-456f-b0b9-d3193b0f6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e2ecebb-d5d1-456d-967c-842a79fdd622",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_random_elements(dataset, num_examples=15):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af560b6-7d21-499e-9b82-114be371a98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 star</td>\n",
       "      <td>My first experience at \\\"Sugar Factory\\\" was the one at Paris Hotel! Remembered great service and good food!\\n\\nMy visit this time was because of a Groupon deal - $30 for $62 worth of dinner/drinks.\\n\\nWe made reservations but was running a little late. When I came up to the hostess and told her that, she pretty much just threw me at the worst possible seat outside. My mistake for assuming that I should've been asked if I wanted indoors or outdoors, or if I wanted to wait for indoor seats. \\n\\nTook about 10 mins for us to see our waiter, Justin. He took our water order, for we were waiting for our friend, and told us about their half-off special for the night on drinks.\\nTook another 10-15 mins to ACTUALLY get our water. \\n\\nBecause we were waiting for our friend still, we decided to order a couple things - we got a $29 White Gummi Goblet, my $18 3-Piece Chicken Fingers and Onion Rings and my friend's $8 4-Piece, Cracker-Sized Bruschetta.\\n\\nOur goblet arrives.. As they're suppose to, they bring the cup filled with ice and they poor the drink in front of you. Justin pours the drink .. Then DEMANDS for my friend to sip it because it was going to over flow.. Like DEMANDED, as in told her \\\"hurry up, sip it because it's going to spill all over the table\\\" ... W T F?!\\n\\nOur food comes shortly after the uncomfortable incident, it was good and all but just not worth what we paid for..\\n\\nWe had ordered another goblet (Berry Bliss) and for what they're worth, they are definitely good drinks. Although they were sweet and fruity, they pack the alcohol punch that comes creepin' after a while of drinking it!\\n\\nSomewhere along that time period Justin had made really awkward, not so funny jokes.. It made us feel so uncomfortable and he did it in no flirty, nor professional manner!\\n\\nAs closing time approaches, Justin had asked for my phone for the Groupon voucher, he took it so our discount can be applied to the bill. He came back, dropped the check, and literally ran off .. \\nSooooo, where's my phone?!?!? Seriously took a few minutes to see him.. When he finally noticed I was eyeing him down, I asked \\\"so where's my phone?!\\\".\\nHe ran towards his manager, dropped off my phone and AGAIN awkwardly, unprofessionally joked about his manager looking through my pictures.. True or not, I shouldn't have had to ask for my phone! Nor did he have to make that stupid joke! \\nI've seriously have had it with him that night and wonder how and why he works there.\\n\\nTo end the night, we saw on our bill that gratuity was included... For THREE people ?!? Because we had our Groupon deal??? I honestly didn't understand but I would've really liked for our \\\"funny\\\" waiter, Justin, to have explained things..\\n\\nFor the record, I don't think I'd be coming back ... If I do ..... Nvm yeah def not, thanks to Justin!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Cool little taco shop in a questionable part of town.  we went early and the place was dead. The waitress was very nice and helpful even going in to the kitchen to get us samples of the meats they offer.  The salsa bar was huge with lots of interesting choices, nothing rocked my world but the cilantro cream was tasty.  we ordered a variety of items, my least favorite being the quesadilla, just tasted like fried greasy tortilla couldn't get past it to eat more than a bite or two.  the tacos were good I had the shark and hubby had the pork it was his favorite. we had a carne asada burrito as well and the meat was tasty.  It was good not great but I think we will be back to try it again.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Burgers.  Great!!  Those chips they serve not as much.  Our portion  unedible  greasy thru and thru. And mentioned to server  she sent manager over. Very nice guy brought us a fresh order and they were not much better.  The server also mixed up the placement of the burger on the table and we had no idea til half way thru when my mom commented the burger was spicy. \\nHad the Monty. Great burger nice portion. My niece had the buffalo and the taste was great.  Would be a 4 but for $12 burger the whole meal should be good.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 star</td>\n",
       "      <td>I have never written a bad review before - however, I cannot not write this.  I am still shocked at the treatment I received from Gary, supposedly a service oriented nail 'professional.'    He is rude (to say the least), arrogant (without reason), his Groupon is a lie (he doesn't even do Gel nails).  He does NOT even deserve to be in business.  Again, I have NEVER felt so strongly about such horrible customer service.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Best pizza in Vegas! Vegas--you're in trouble if that's true. It's not bad pizza, I just think that the election is rigged. The owners are smart and humor runs high in their marketing ploys. Gimmicks like getting a discount on certain day if you're name is Mark are pretty brilliant. \\n\\nThe service is also pretty on point and upbeat. The place itself is designed to feel homey like a pizza joint should and succeeds. I am especially fond of the mammoth booths. Extraneous seating is always a winner in my book.  \\n\\nThe special on my day is that you got free Nuke fries with your order. The spicy Nuke fries are o.k., but I think I would be upset if I paid for them. Gesture appreciated though. Metro has four Eastside specialties on their menu--so my friend and I went with the Four Corners, which gives you two slices of each Eastsides. \\n\\nWith exciting toppings like eggplant and ricotta in the mix--I thought the Four Corners was going to be a knockout, but the uppercut never came. I got rabbit punched instead. The idea is good topping-wise on these pizzas, but the ingredients all feel very commercial--like straight out of a can or factory. \\n\\nAlso, the Large pizza is pretty large--so only order thick crust if you are starving or if you want to take some with you. The Four Corners hit the spot as far as my pizza craving, but surely there is better pizza in town. That being said, the pizza hounds have been let loose and the hunt begins!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Having moved from Utah, where we first found this place, I was very sad to think we'd never be back to it. We were thrilled when one opened up so close to us. And we were not disappointed with the quality. \\n\\nI usually get a combo with soup and sandwich, though I've recently switched to getting just a regular soup. I never finish the whole sandwich and it feels like a waste to leave half of it there. \\nMy husband's favorite sandwich is gone now, replaced by a similar-but-not-the-same option.  Not bad though. \\nMy favorite soup by far is the chicken enchilada chili, but I've liked every other one I've tried as well. My parents also love the place. \\nThe chocolate covered strawberries are a wonderful touch. I've gotten two desserts so far and they were pretty good. Love the beverage options with the syrups - sprite with cherry and blackberry is my favorite. \\nI do wish the bread given with the soups seemed a little more fresh though.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1 star</td>\n",
       "      <td>I recently moved to the area and have been looking for a nail place close by. Let me just preface this by saying I am the type of customer who never goes into a business requesting extended services close to closing time. That being said, I drove up at 3:55pm on a Sunday (their posted closing time is 5pm) I had planned a mani/pedi but with only an hour I thought I'd guage how many services to get once I walked in.\\n\\nThe place is pretty and well decorated. It smelled nice and I was promptly greeted as I walked in. The place was empty except for one customer getting a manicure and they were already painting her nails so it was safe to assume she was almost done with her manicure. \\n\\nThe lady who greeted me asked what I would like. In the interest of time I decided to only get a pedicure and told her I would like a pedicure. She looked at the clock, grimaced and said \\\"oh, sorry...you wouldnt have enough time for a pedicure before closing\\\"\\n\\nI was floored. An hour and 10 mins in an empty salon isn't enough time for a pedicure? ? ? Did they send their staff home early? Were they trying to close early? Not sure...\\n\\nBecause of the reviews I was expecting a great experience. Not the case! \\n\\nI went across the street (Albertson's shopping plaza) to Nice Nails and got a mani/pedi and some much needed relaxation :-)\\n\\nNot sure if I'll give nail room another try, didn't feel like they wanted my business....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2 star</td>\n",
       "      <td>$3.95 for a soda?  I should have known right there they weren't interested in locals (yes, I'm familiar with the prices at other local restaurants this is, after all, my neighborhood).  My brisket sandwich had so much fat that I gave up and took the rest home for the dog.  Too bad, because I was looking forward to having a high quality BBQ restaurant, nearby.  I will watch to see if there are any substantial changes in their approach, before returning.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4 stars</td>\n",
       "      <td>Love this park! it's a nice green space in the midle of downtown. every afternoon you will find runners jogging the path around the park, kids and parents playing on the playground, people playing volleyball on the sand court, and people walking their dogs. compared to where I used to live in Austin, dog owners here are VERY responsible. I rarely ever see someone who doesn't pick up after their pet. (note: the park does not have stations that provide doggy poop bags, you have to bring your own)\\n\\nonly reason this park doesn't get five stars is because there are quite a few homeless people who use the area for sleeping or hanging out. if you walk under the bridge toward the library you walk right into a HUGE crowd of homeless people hanging out outside the library. it can be a little intimidating.\\n\\nthere are two shaded ramadas in the park which you can reserve for parties or other events for a fee. the information for how to reserve one is posted on the ramadas themselves.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2 star</td>\n",
       "      <td>My family arrived to celebrate my moms birthday! The hosting staff did a great job of seating us in a minimal amount of time. We had an emergency change in plans and they accommodated us quickly. They also accommodated the temperature of the room for our easily cooled friends. Everyone enjoyed their food. And the server was great UNTIL......two spiders crawled up onto our table right between me and my son. When I complained, I got some half assed excuse that they had just sprayed the night before from the manager. My husband, not wanting to look douchey and not like we were just trying to get a free meal, told them not to worry about it. But personally, I think they still should have done something about it. When you pay for a $500 meal, you don't want to deal with bugs. I was in the service industry for nearly 10 years, I understand the prejudice that goes on behind closed doors. I understand that they probably thought I was lying, but frankly, I couldn't care less. That's gross. I don't want to pay for a meal with bugs.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2 star</td>\n",
       "      <td>I got a promotion in the mail for ridiculously low rates so I had to take them up on the offer. I kind of wish I hadn't. The hotel, or at least my room has seen better days. It was really worn down. Everything was just good enough nothing more nothing less. The towels were a little rough and the bed was a bit lumpy. \\n\\nThe casino is actually kind of small and limited, it seems this place lives off more from its nightlife. I don't think I would stay at the Palms again. There are so many other places in Vegas that you can get a whatever experience.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2 star</td>\n",
       "      <td>I lived here from 2008-2010 and did genuinely enjoy my experience with the apartment itself.  The building is old construction which was nice because I didn't have an issue hearing my neighbors.  Also didn't have a big issue with the heat/cold as the place was well insulated.  The location is also great as you are close to Old Town and the 101.\\n\\nThe issues I did have were with the management company.  Rent is due by the 2nd and HAS to be paid online.  The office doesn't tell you that it has to be paid by 12 AM Central Standard time otherwise you will get a $50 late charge.  I incurred a late charge and asked the office to reverse it since I had paid rent on time but was told since it was paid at 12:05 AM on the 3rd CST, they would not reverse the charges.  \\n\\nAt first when I moved in there were around 4 washers/dryers for 40 units.  The laundry room was remodeled and the washer/dryers were reduced down to 2 units each.  The explanation was that the new models were \\\"higher efficiency\\\" and would be much faster but just because the wash cycle is faster doesn't mean your neighbor will be courteous and remove their laundry on time.  The worst part was I had someone steal my underwear on 3 separate occasions.  When I called the office to report it I was told \\\"maybe you should just stay with your laundry so that people don't steal your personal items.\\\"  It was only after the 3rd time that I filed a police report and found out that the SDPD had had SEVERAL complaints about this issue and the office chose to over look them.   \\n\\nI also had issues with people parking in my assigned space and even had a glove box full of \\\"nasty grams\\\" to put on the offenders windshield.  One time I called the # listed on the carport to have the car towed and was told I needed a special \\\"code\\\" to have them come out.  I called the office and asked for the code and was told they don't give that out to residents but I could call during office hours and they would be happy to help with my parking issue.  I explained it was happening at 11 PM and the office wasn't open but was told to just find an uncovered spot to park in.\\n\\nThe kicker was when I moved out.  I had lived in the apartment for 2 years and did my best on upkeep.  Once I moved out I got a bill saying I owed them $200 over other $200 refundable deposit I had put down when I moved in.  I asked for an itemized list of what I was charged for and found out I was charged $180 to repaint my 500 sq foot apartment which I found odd since there were no holes in the wall nor had I made any paint color changes.  Was also charged to replace the carpet which was funny because it wasn't new when I moved in, just shampooed.\\n\\nOh yeah, dumpster divers totally true.  Happened at least once a week</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3 stars</td>\n",
       "      <td>Quick review\\nI expected the wait for seating, but not the long wait for the food (it's only burgers).\\nAfter the wait, still got my temp wrong  on my burger (more waiting)\\nTaste was good but portion were small for the high price on Maui onion rings, fries and burgers.\\nService was real good. \\nI believe KGB (Kerry Gourmet burger) and Burger bar are much better burger places.\\nStill a fan of Chef Ramsey.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>This is my favorite Asian restaurant in the Valley. This place is not much for ambiance, but it features wonderfully authentic Cantonese- Hong Kong cuisine at outrageously cheap prices, served by very friendly and helpful staff. The menu of Asian Cafe Express is extensive, but skip the first few pages (these list the \\\"Americanized Chinese\\\" dishes that most people are familiar with) and go directly to the pages with the items categorized as \\\"Hong Kong style\\\" or \\\"HK style.\\\" If you're not sure what to order, you can browse the huge pictures of popular dishes, on the wall. But it will be best to ask the waitpersons for suggestions -- they'll be more than happy to point out dishes in the menu that are favorites of the many regular customers there. Portions are more than ample, even for the soups: the smallest serving (\\\"medium\\\"  in the menu) of soup serves two generously.  And they have several varieties of congee, including my favorite: with pork and preserved egg -- comfort food like no other for this Asian ;-)\\n\\nFor those who use the light rail, this is a convenient dining destination because it is so close to the Mesa terminus of the light rail. Be prepared for a taste treat, and don't act too surprised when the bill arrives and you find that you don't pay much for such great food.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2 star</td>\n",
       "      <td>Am I the ONLY person in the state who doesn't need to eat here?  Can someone tell me what I am missing?  I just don't get the hype.  The food is ok, nothing stands out as \\\"must come back for\\\".  I will keep coming, because my sister and mother are addicted, and I am sure there is something on the menu I can find that I will like.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df7cd0-23cd-458f-b2b5-f025c3b9fe62",
   "metadata": {},
   "source": [
    "## 预处理数据\n",
    "\n",
    "下载数据集到本地后，使用 Tokenizer 来处理文本，对于长度不等的输入数据，可以使用填充（padding）和截断（truncation）策略来处理。\n",
    "\n",
    "Datasets 的 `map` 方法，支持一次性在整个数据集上应用预处理函数。\n",
    "\n",
    "下面使用填充到最大长度的策略，处理整个数据集："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c04545-352f-4ae0-8f4c-1e6f753d3685",
   "metadata": {},
   "source": [
    "1. **为什么要处理文本长度？**  \n",
    "   就像衣服有尺码，神经网络模型也有固定的\"输入尺寸\"。比如BERT模型最多\"吃\"512个单词片段。太长的文本会被截断（切掉尾巴），太短的会补零（相当于给衣服加填充物）。\n",
    "\n",
    "2. **分词器在做什么？**  \n",
    "   把文字转换成数字密码（如\"你好\"→[101, 2345])，同时：\n",
    "   • 自动加特殊符号：比如[CLS]开头、[SEP]分隔\n",
    "   • 记录哪些是真实内容（attention_mask里1表示真实，0是填充的）\n",
    "\n",
    "3. **map方法的神奇之处**  \n",
    "   这个操作就像流水线作业，把整个数据集批量送进处理函数。假设数据集有1万条文本，用`batched=True`参数，可能分100批次处理（每批100条），效率比逐条处理高得多。\n",
    "\n",
    "4. **处理后的数据结构**  \n",
    "   每个样本会变成包含多个数组的字典：\n",
    "   ```python\n",
    "   {\n",
    "     'input_ids': [101, 2345, 1032, 0, 0],  # 数字化的文本\n",
    "     'token_type_ids': [0,0,..],            # 区分句子（用于问答任务）\n",
    "     'attention_mask': [1,1,..0,0]          # 标记有效内容位置\n",
    "   }\n",
    "   ```\n",
    "\n",
    "举个生活化的例子：\n",
    "原始句子：\"我爱吃披萨\" → 处理后会变成类似：\n",
    "```\n",
    "[CLS] 我 爱 吃 披萨 [PAD] [PAD] [PAD]...\n",
    "对应的数字：[101, 2769, 3342, 1563, 5643, 0, 0, 0...]\n",
    "注意力的遮罩：[1,1,1,1,1,0,0,0...]\n",
    "```\n",
    "其中：\n",
    "• [CLS]是BERT要求的起始符号\n",
    "• [PAD]是填充的占位符（实际用0表示）\n",
    "• 注意力遮罩告诉模型哪些位置需要关注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bf2b342-e1dd-4ab6-ad57-28eb2513ae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 从transformers库导入自动分词器\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 加载预训练的分词器（这里用的是BERT的区分大小写版本）\n",
    "# [2,4](@ref)：Hugging Face的Tokenizer支持填充和截断策略\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# 定义一个处理数据集的函数\n",
    "def tokenize_function(examples):\n",
    "    # 对文本进行分词，并应用两个重要策略：\n",
    "    # 1. padding=\"max_length\"：将所有文本填充到模型允许的最大长度（如512）\n",
    "    # 2. truncation=True：超过最大长度的部分会被截断\n",
    "    # [2,4](@ref)：这是Hugging Face推荐的标准化处理方式\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# 将处理函数应用到整个数据集（支持批量处理加速）\n",
    "# batched=True表示一次性处理多个样本，比逐条处理快10倍以上\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d100a866-b92e-44c2-aab7-8ad078a6e449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': [{'filename': '/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/0.0.0/c1f9ee939b7d05667af864ee1cb066393154bf85/cache-42c6b839c042ef53.arrow'}], 'test': [{'filename': '/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/0.0.0/c1f9ee939b7d05667af864ee1cb066393154bf85/cache-90992f974cd05082.arrow'}]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets.cache_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47a415a8-cd15-4a8c-851b-9b4740ef8271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5 stars</td>\n",
       "      <td>I went to the Charleston location yesterday and the staff was a bit rude and didn't know their own product. Today I decided to go to the Ft. Apache and Trop location and the ladies were just darling and were great help. Ashley was wonderful and helpful. The staff went out of their way to help. One of the girls helped even tie the bowl part to the roof of my car since it wouldn't fit anywhere into the car. Thank you guys. You all ROCK :) I totally love my new papasan chair.</td>\n",
       "      <td>[101, 146, 1355, 1106, 1103, 10874, 2450, 8128, 1105, 1103, 2546, 1108, 170, 2113, 14708, 1105, 1238, 112, 189, 1221, 1147, 1319, 3317, 119, 3570, 146, 1879, 1106, 1301, 1106, 1103, 143, 1204, 119, 16995, 1105, 157, 12736, 2450, 1105, 1103, 8564, 1127, 1198, 18556, 1105, 1127, 1632, 1494, 119, 9017, 1108, 7310, 1105, 14739, 119, 1109, 2546, 1355, 1149, 1104, 1147, 1236, 1106, 1494, 119, 1448, 1104, 1103, 2636, 2375, 1256, 5069, 1103, 7329, 1226, 1106, 1103, 3664, 1104, 1139, 1610, 1290, 1122, 2010, 112, 189, 4218, 5456, 1154, 1103, 1610, 119, 4514, 1128, 3713, 119, 1192, 1155, 155, ...]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 随机展示处理后的样本（假设show_random_elements是自定义的检查函数）\n",
    "# 通过这个可以查看处理后的数据结构，例如：\n",
    "# {\n",
    "#   'input_ids': [101, 2345, 1032, ..., 0, 0], \n",
    "#   'attention_mask': [1,1,..1,0,0]\n",
    "# }\n",
    "show_random_elements(tokenized_datasets[\"train\"], num_examples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a3f21-9fb6-40d9-a760-85962cc76ef8",
   "metadata": {},
   "source": [
    "以下是该表格中各个字段的详细解释，按NLP处理流程分阶段说明：\n",
    "\n",
    "---\n",
    "\n",
    "### ▋ 字段结构解析 (针对BERT类模型)\n",
    "\n",
    "| 字段名称          | 示例值片段                     | 作用层级      | 技术细节                                                                 |\n",
    "|-------------------|------------------------------|--------------|--------------------------------------------------------------------------|\n",
    "| **label**         | \"1 star\"                     | 业务标签层    | 原始业务标签（此处展示为可读形式，实际训练需转换为数值如0-4对应1-5星）    |\n",
    "| **text**          | 用户评论原文                 | 原始数据层    | 未处理的原始文本输入                                                     |\n",
    "| **input_ids**     | [101, 23158, 1204,...]       | Token编码层   | 将文本转换为模型可识别的数字ID序列                                        |\n",
    "| **token_type_ids**| [0, 0, 0,...]                | 句子分段层     | 标识token属于哪个句子（单句子任务全为0）                                  |\n",
    "| **attention_mask**| [1, 1, 1,...]                | 注意力机制层   | 控制模型关注有效内容（1=有效token，0=填充位）                             |\n",
    "\n",
    "---\n",
    "\n",
    "### ▋ 关键技术点详解\n",
    "\n",
    "#### 1. **label字段的特殊处理**\n",
    "```python\n",
    "# 实际训练时应转换为数值标签\n",
    "label_mapping = {\"1 star\": 0, \"2 stars\": 1, ..., \"5 stars\": 4}\n",
    "dataset = dataset.map(lambda x: {\"label\": label_mapping[x[\"label\"]]})\n",
    "```\n",
    "\n",
    "#### 2. **input_ids的构造过程**\n",
    "- **特殊标记说明**：\n",
    "  - `101`: [CLS] 分类标记（BERT等模型的起始符）\n",
    "  - `102`: [SEP] 分隔标记（此例未出现，因单句输入）\n",
    "  - `0`: [PAD] 填充标记（此例未出现，因已用max_length填充）\n",
    "\n",
    "#### 3. **token_type_ids的扩展应用**\n",
    "```python\n",
    "# 双句子任务时的典型结构（如QA）\n",
    "tokenizer(\"How are you?\", \"I'm fine\", return_token_type_ids=True)\n",
    "# 输出：\n",
    "# token_type_ids = [0,0,0,0,0, 1,1,1,1]\n",
    "```\n",
    "\n",
    "#### 4. **attention_mask的动态性**\n",
    "```python\n",
    "# 实际处理变长文本时的mask示例：\n",
    "原始文本: \"Hello world\"\n",
    "填充后: \"Hello world [PAD] [PAD]\"\n",
    "attention_mask: [1,1,0,0]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ▋ 数据处理流程可视化\n",
    "```\n",
    "原始文本\n",
    "   ↓ (分词器处理)\n",
    "[CLS] Went there to... [SEP] → 分词结果\n",
    "   ↓ (词汇表映射)\n",
    "101 23158 1204 ... 102 → input_ids\n",
    "   ↓ (句子标识)\n",
    "0   0     0    ... 0   → token_type_ids\n",
    "   ↓ (有效标识)\n",
    "1   1     1    ... 1   → attention_mask\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ▋ 最佳实践建议\n",
    "1. **动态填充策略**：\n",
    "```python\n",
    "# 替代固定长度填充，提升效率\n",
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer)\n",
    "```\n",
    "\n",
    "2. **验证字段一致性**：\n",
    "```python\n",
    "# 检查各字段长度是否匹配\n",
    "assert len(input_ids) == len(token_type_ids) == len(attention_mask)\n",
    "```\n",
    "\n",
    "3. **解码验证**：\n",
    "```python\n",
    "# 反向验证编码正确性\n",
    "decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
    "assert decoded_text == original_text\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "是否需要进一步了解如何将这些预处理后的数据输入模型进行训练？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66db1f32-6210-41eb-a296-f435f5d66917",
   "metadata": {},
   "source": [
    "Wed Mar  5 15:34:53 2025       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       On  | 00000000:00:07.0 Off |                    0 |\n",
    "| N/A   38C    P0              26W /  70W |    985MiB / 15360MiB |      0%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A    103559      C   /root/miniconda3/envs/peft/bin/python       982MiB |\n",
    "+---------------------------------------------------------------------------------------+"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc801faf-adec-4571-aee1-0c5d432febb1",
   "metadata": {},
   "source": [
    "根据提供的日志和硬件监控信息，当前系统状态可从以下角度分析：\n",
    "\n",
    "---\n",
    "\n",
    "### **一、文件下载与模型加载**\n",
    "1. **Tokenizer 相关文件处理**  \n",
    "   • `tokenizer_config.json`（已完成100%下载）：该文件定义了分词器的配置参数（如是否区分大小写、特殊标记映射路径等）。例如，`do_lower_case=True` 表示输入文本会被统一转为小写。\n",
    "   • `vocab.txt`（下载中）：词汇表文件，包含所有标记及其唯一索引，用于将文本转化为模型可识别的数字序列。例如，`[CLS]`可能对应索引0，`[SEP]`对应索引1。\n",
    "   • `tokenizer.json`（下载中）：包含分词器的完整配置和模型类型（如BPE、WordPiece），是分词器的核心文件。\n",
    "\n",
    "2. **模型配置文件加载**  \n",
    "   • `config.json`（下载中）：定义模型架构参数，如隐藏层维度（`hidden_size`）、注意力头数（`num_attention_heads`）、层数（`num_hidden_layers`）等。例如，`hidden_size=768`表示每层有768个神经元。\n",
    "\n",
    "3. **进度解读**  \n",
    "   • `Map: 35%` 可能表示模型权重正在从文件映射到内存，或分词器初始化完成35%。\n",
    "\n",
    "---\n",
    "\n",
    "### **二、GPU资源占用**\n",
    "• **Tesla T4使用情况**  \n",
    "  • **显存占用**：985MiB/15360MiB，占比约6.4%，显示当前任务对GPU压力较低。\n",
    "  • **进程信息**：Python进程（PID 103559）正在运行，可能与模型推理或训练相关。例如，加载模型权重（如`model.safetensors`）或执行前向计算。\n",
    "  • **计算模式**：`Compute M.`显示为`Default`，表明未启用特定计算模式（如MIG多实例GPU）。\n",
    "\n",
    "---\n",
    "\n",
    "### **三、综合行为推断**\n",
    "当前系统可能正在执行以下操作之一：\n",
    "1. **模型初始化**  \n",
    "   • 通过Hugging Face的`from_pretrained()`方法加载预训练模型，自动下载配置文件（如`config.json`）和分词器文件。\n",
    "   • 示例代码类似：\n",
    "     ```python\n",
    "     from transformers import AutoTokenizer, AutoModel\n",
    "     tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "     model = AutoModel.from_pretrained(\"bert-base-uncased\").cuda()\n",
    "     ```\n",
    "\n",
    "2. **文本预处理**  \n",
    "   • 使用分词器将输入文本（如用户提问）转换为Token ID序列，需依赖`vocab.txt`和`tokenizer.json`。\n",
    "\n",
    "3. **轻量级推理任务**  \n",
    "   • 显存占用较低可能表明任务规模较小（如短文本分类或问答），未涉及全量训练。\n",
    "\n",
    "---\n",
    "\n",
    "### **四、潜在风险与优化建议**\n",
    "• **显存利用率低**：Tesla T4的显存使用率不足10%，可考虑批量处理任务或启用混合精度训练（`fp16`/`bf16`）以提升吞吐量。\n",
    "• **下载速度限制**：`5.64kB/s`的下载速率可能受网络带宽影响，建议检查代理设置或切换至本地缓存模型。\n",
    "\n",
    "---\n",
    "\n",
    "**总结**：系统正在加载一个基于Transformer架构的预训练模型（如BERT或GPT），完成分词器和模型配置的初始化，并利用GPU执行轻量级计算任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c33d153-f729-4f04-972c-a764c1cbbb8b",
   "metadata": {},
   "source": [
    "### 数据抽样\n",
    "\n",
    "使用 1000 个数据样本，在 BERT 上演示小规模训练（基于 Pytorch Trainer）\n",
    "\n",
    "`shuffle()`函数会随机重新排列列的值。如果您希望对用于洗牌数据集的算法有更多控制，可以在此函数中指定generator参数来使用不同的numpy.random.Generator。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a17317d8-3c6a-467f-843d-87491f600db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 从完整训练集中创建小型训练子集（1000条样本）\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "\"\"\"\n",
    "执行步骤：\n",
    "1. shuffle(seed=42): 先对训练集进行随机打乱（设置随机种子保证可复现性）\n",
    "2. select(range(1000)): 选取前1000条打乱后的样本\n",
    "作用：\n",
    "- 创建小规模训练集，加速实验迭代\n",
    "- 保持数据分布的随机性\n",
    "- 固定随机种子保证每次运行结果一致\n",
    "\"\"\"\n",
    "\n",
    "# 从完整测试集中创建小型验证子集（1000条样本） \n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\"\"\"\n",
    "典型应用场景：\n",
    "1. 快速验证模型是否能过拟合（用少量数据测试学习能力）\n",
    "2. 资源有限时进行超参数调试\n",
    "3. 原型开发阶段的快速实验\n",
    "4. 教学演示场景（缩短训练时间）\n",
    "\n",
    "注意事项（使用时需知）：\n",
    "- 小样本可能无法代表完整数据分布\n",
    "- 评估指标会有较大方差\n",
    "- 正式训练时建议使用完整数据集\n",
    "- 生产环境需要更严谨的验证集划分\n",
    "\"\"\"\n",
    "\n",
    "# 扩展：查看数据集结构示例\n",
    "print(small_train_dataset)\n",
    "# 输出示例：Dataset(features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'], num_rows: 1000)\n",
    "\n",
    "print(small_eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b65d63-2d3a-4a56-bc31-6e88a29e9dec",
   "metadata": {},
   "source": [
    "## 微调训练配置\n",
    "\n",
    "### 加载 BERT 模型\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d2af4df-abd4-4a4b-94b6-b0e7375304ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 关键参数解析：\n",
    "# \"bert-base-cased\" - 使用区分大小写的BERT基础版\n",
    "# num_labels=5       - 五分类任务（对应Yelp的1-5星评分）\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-cased\", \n",
    "    num_labels=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44014df-b52c-4c72-9e9f-54424725a473",
   "metadata": {},
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "\n",
    "完整配置参数与默认值：https://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "源代码定义：https://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161\n",
    "\n",
    "**最重要配置：模型权重保存路径(output_dir)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c01d5c-de72-4ff0-b11d-e07ac5346888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 🎯 优化器配置\\nlearning_rate=5e-5,                     # BERT常用初始学习率（5e-5 ~ 3e-5）\\nweight_decay=0.01,                      # 权重衰减防过拟合\\n\\n# 📉 学习率调度\\nwarmup_steps=500,                       # 预热步数（从小学习率逐步上升）\\n\\n# 💽 内存优化\\nfp16=True,                              # 启用混合精度训练（显存减半）\\ngradient_accumulation_steps=2,          # 梯度累积步数（模拟更大批次）\\n\\n# 🧪 验证配置\\nevaluation_strategy=\"steps\",             # 每eval_steps评估一次\\neval_steps=200,                         # 评估频率\\nload_best_model_at_end=True,            # 训练结束时加载最佳模型\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# 💾 模型保存路径配置（最重要参数！所有训练产出物都会保存到这里）\n",
    "model_dir = \"models/bert-base-cased-finetune-yelp\"  # 推荐使用模型名+任务名的目录结构\n",
    "\n",
    "# 🎛️ 创建训练参数配置实例\n",
    "training_args = TrainingArguments(\n",
    "    # 必须参数\n",
    "    output_dir=model_dir,                   # 模型/日志/检查点的保存根目录\n",
    "                                          # 📂 目录将包含：\n",
    "                                          #   |- config.json\n",
    "                                          #   |- trainer_state.json\n",
    "                                          #   |- checkpoint-100/...\n",
    "    \n",
    "    # ⚡ 训练效率参数\n",
    "    per_device_train_batch_size=16,       # 每个GPU的批次大小（调整依据显存）\n",
    "                                          # 💡 3080显卡建议值：16-32\n",
    "                                          # ❗ 总批次大小 = 该值 * GPU数 * gradient_accumulation_steps\n",
    "    \n",
    "    # ⏱️ 训练时长控制\n",
    "    num_train_epochs=5,                   # 训练总轮次（建议先用1轮试跑，再全量训练）\n",
    "                                          # 🔄 1个epoch = 完整过一遍训练集\n",
    "    \n",
    "    # 📊 日志配置\n",
    "    logging_steps=100,                    # 每训练100步记录一次日志（默认500）\n",
    "                                          # 📈 设小值可更密集监控训练过程\n",
    "                                          # 💻 控制台将输出：\n",
    "                                          #    Step | Loss   | Learning Rate\n",
    "                                          #    100  | 0.532  | 0.00005\n",
    ")\n",
    "\n",
    "# 扩展建议参数（根据需求添加）：\n",
    "\"\"\"\n",
    "# 🎯 优化器配置\n",
    "learning_rate=5e-5,                     # BERT常用初始学习率（5e-5 ~ 3e-5）\n",
    "weight_decay=0.01,                      # 权重衰减防过拟合\n",
    "\n",
    "# 📉 学习率调度\n",
    "warmup_steps=500,                       # 预热步数（从小学习率逐步上升）\n",
    "\n",
    "# 💽 内存优化\n",
    "fp16=True,                              # 启用混合精度训练（显存减半）\n",
    "gradient_accumulation_steps=2,          # 梯度累积步数（模拟更大批次）\n",
    "\n",
    "# 🧪 验证配置\n",
    "evaluation_strategy=\"steps\",             # 每eval_steps评估一次\n",
    "eval_steps=200,                         # 评估频率\n",
    "load_best_model_at_end=True,            # 训练结束时加载最佳模型\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ce03480-3aaa-48ea-a0c6-a177b8d8e34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=no,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased-finetune-yelp/runs/Mar06_16-27-33_deepseek-r1-t4-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=models/bert-base-cased-finetune-yelp,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased-finetune-yelp,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=steps,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092ff8f4-ba7a-4617-860c-0fa37a40eb27",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "以下是对 `TrainingArguments` 配置的通俗解释，按功能分类并添加注释：\n",
    "\n",
    "---\n",
    "\n",
    "### 🌟 **核心训练参数**\n",
    "```python\n",
    "output_dir=\"models/bert-base-cased-finetune-yelp\"  # 模型保存路径（最重要！训练结果全存在这）\n",
    "per_device_train_batch_size=16   # 每个GPU的批次大小（显存不足时调小此值）\n",
    "num_train_epochs=5               # 训练总轮次（通常3-5轮足够微调）\n",
    "learning_rate=5e-05              # 学习率（BERT常用5e-5, 太大容易震荡，太小收敛慢）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 💻 **资源相关参数**\n",
    "```python\n",
    "fp16=False                       # 是否启用混合精度训练（True可省显存，需GPU支持）\n",
    "gradient_accumulation_steps=1    # 梯度累积步数（模拟更大批次，显存不足时使用）\n",
    "optim=\"adamw_torch\"              # 优化器类型（Adam的改进版，适合深度学习）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ⏱️ **训练过程控制**\n",
    "```python\n",
    "logging_steps=100                # 每100步打印一次日志（默认500，调小可更频繁监控）\n",
    "save_steps=500                   # 每500步保存一次模型（频繁保存会占用磁盘）\n",
    "evaluation_strategy=\"no\"         # 评估策略（\"no\"不评估，\"steps\"按步评估，\"epoch\"每轮评估）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ **优化相关参数**\n",
    "```python\n",
    "weight_decay=0.0                 # 权重衰减系数（防过拟合，常用0.01）\n",
    "warmup_steps=0                   # 预热步数（初始阶段用小学习率）\n",
    "max_grad_norm=1.0                # 梯度裁剪阈值（防梯度爆炸）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 **日志与保存**\n",
    "```python\n",
    "logging_dir=\"models/.../runs/...\" # TensorBoard日志路径\n",
    "report_to=[]                     # 上报平台（例如[\"wandb\"]接入可视化）\n",
    "save_total_limit=None            # 最大保存检查点数（设为3只保留最新3个模型）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 **其他实用参数**\n",
    "```python\n",
    "seed=42                          # 随机种子（固定后结果可复现）\n",
    "disable_tqdm=False               # 是否禁用进度条（True时更简洁）\n",
    "remove_unused_columns=True       # 自动删除模型不需要的列（节省内存）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🚀 **参数选择建议**\n",
    "1. **学习率**：从 `5e-5` 开始尝试，观察损失变化\n",
    "2. **批次大小**：在显存允许范围内尽量调大（如16→32）\n",
    "3. **训练轮次**：用早停法（`EarlyStoppingCallback`）防过拟合\n",
    "4. **混合精度**：设置 `fp16=True` 可减少30%显存占用\n",
    "5. **多GPU支持**：无需修改代码，启动时加 `--nproc_per_node=GPU数量`\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ **特别注意项**\n",
    "```python\n",
    "do_train=False  # 当前配置未启用训练！（需设为True才会开始训练）\n",
    "do_eval=False   # 当前未启用验证！（需配合eval_dataset使用）\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🔄 **完整训练启动示例**\n",
    "```python\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,       # 需要评估时添加\n",
    "    compute_metrics=compute_metrics  # 自定义评估函数\n",
    ")\n",
    "\n",
    "# 开始训练（do_train=True时才生效）\n",
    "trainer.train() \n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "通过合理配置这些参数，可以平衡训练速度、资源消耗和模型性能。建议先用小数据子集调试参数，再全量训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c281ed96-969c-490b-a0b4-a3cd1f14e05e",
   "metadata": {},
   "source": [
    "## 什么是学习率，什么是震荡？\n",
    "\n",
    "### 📚 学习率（Learning Rate）通俗解释\n",
    "\n",
    "**学习率就像「下山的步长」**  \n",
    "想象你蒙着眼从山顶往下走，要找到最低点（模型的最优参数）。学习率就是你每步迈的幅度：\n",
    "- **太大步（高学习率）**：容易一步跨过山谷，在对面的山坡来回跳（震荡）\n",
    "- **太小步（低学习率）**：要走很久才能到底，甚至卡在半山腰（收敛慢）\n",
    "\n",
    "![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*kA38KJq9aeZkBW-0rH2yCg.gif)\n",
    "\n",
    "---\n",
    "\n",
    "### 💥 什么是震荡（Oscillation）？\n",
    "\n",
    "**震荡就像「在山谷两边反复横跳」**  \n",
    "当学习率太大时，参数更新会像这样：\n",
    "1. 当前点：A（损失较高）\n",
    "2. 计算梯度：指向谷底方向\n",
    "3. 大步更新：直接跳到对面的B点\n",
    "4. 再计算梯度：又指向另一个方向\n",
    "5. 结果：在谷底两侧反复跳动，无法稳定到最低点\n",
    "\n",
    "![](https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooLarge.svg)\n",
    "\n",
    "---\n",
    "\n",
    "### 🌰 具体案例对比\n",
    "| 学习率 | 训练表现 | 损失曲线 | 适用场景 |\n",
    "|-------|---------|---------|---------|\n",
    "| 0.1   | 剧烈震荡 | 锯齿状波动 | ❌ 几乎不用 |\n",
    "| 1e-3  | 偶尔震荡 | 波动下降 | 简单任务 |\n",
    "| 5e-5  | 平稳下降 | 平滑收敛 | ✅ BERT微调 |\n",
    "| 1e-6  | 缓慢下降 | 近乎水平 | 精细调优 |\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 如何避免震荡？\n",
    "1. **学习率预热**：前1000步从小学习率逐步增大\n",
    "2. **梯度裁剪**：限制单步更新幅度（`max_grad_norm=1.0`）\n",
    "3. **自适应优化器**：使用AdamW而不是SGD\n",
    "4. **监控损失曲线**：出现震荡时立即降低学习率\n",
    "\n",
    "---\n",
    "\n",
    "### 🛠️ BERT的实践经验\n",
    "```python\n",
    "# 安全的学习率范围建议\n",
    "learning_rate = 5e-5  # 默认安全值（适合大部分情况）\n",
    "learning_rate = 3e-5  # 更保守的选择（数据量小时）\n",
    "learning_rate = 1e-4  # 高风险！需配合梯度裁剪使用\n",
    "```\n",
    "\n",
    "通过合理控制学习率，可以让模型既快速收敛，又不会「跑过头」。就像开车时找到合适的油门力度，既不会急刹急停，也不会龟速前进。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd3365-d359-4ab4-a300-4717590cc240",
   "metadata": {},
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "\n",
    "**[Hugging Face Evaluate 库](https://huggingface.co/docs/evaluate/index)** 支持使用一行代码，获得数十种不同领域（自然语言处理、计算机视觉、强化学习等）的评估方法。 当前支持 **完整评估指标：https://huggingface.co/evaluate-metric**\n",
    "\n",
    "训练器（Trainer）在训练过程中不会自动评估模型性能。因此，我们需要向训练器传递一个函数来计算和报告指标。 \n",
    "\n",
    "Evaluate库提供了一个简单的准确率函数，您可以使用`evaluate.load`函数加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a8ef138-5bf2-41e5-8c68-df8e11f4e98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前HF端点: https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "# 确认环境变量设置\n",
    "print(\"当前HF端点:\", os.getenv('HF_ENDPOINT', '默认（未设置）'))  # 应该显示 https://hf-mirror.com\n",
    "\n",
    "metric = evaluate.load(\"./accuracy.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d406c0-56d0-4a54-9c6c-e126ab7f5254",
   "metadata": {},
   "source": [
    "\n",
    "接着，调用 `compute` 函数来计算预测的准确率。\n",
    "\n",
    "在将预测传递给 compute 函数之前，我们需要将 logits 转换为预测值（**所有Transformers 模型都返回 logits**）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f46d2e59-1ebf-43d2-bc86-6b57a4d24d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. 多指标计算：\\n   f1 = evaluate.load(\"f1\")\\n   return {\\n       \"accuracy\": metric.compute(...),\\n       \"f1\": f1.compute(...)\\n   }\\n\\n2. 处理多维度输出（如NER）：\\n   predictions = np.argmax(logits, axis=-1)  # shape=(batch_size, seq_len)\\n\\n3. 概率校准：\\n   probs = softmax(logits, axis=-1)\\n   return {\"roc_auc\": roc_auc_score(labels, probs[:,1])}\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"计算模型评估指标的核心函数\n",
    "    \n",
    "    Args:\n",
    "        eval_pred (tuple): 包含模型输出logits和真实标签的元组\n",
    "            logits (np.ndarray): 模型输出的未归一化概率分布，shape=(batch_size, num_classes)\n",
    "            labels (np.ndarray): 真实标签，shape=(batch_size,)\n",
    "    \n",
    "    Returns:\n",
    "        dict: 包含评估指标的字典，例如 {'accuracy': 0.85}\n",
    "    \n",
    "    Notes:\n",
    "        - 适用于分类任务，多分类场景需确保axis参数正确\n",
    "        - 多标签分类需改用sigmoid +阈值处理\n",
    "    \"\"\"\n",
    "    # 解包模型输出和标签\n",
    "    logits, labels = eval_pred  # 等价于 logits = eval_pred, labels = eval_pred\n",
    "    \n",
    "    # 将logits转换为预测类别（取概率最大的类别）\n",
    "    predictions = np.argmax(logits, axis=-1)  # axis=-1表示最后一个维度（分类维度）\n",
    "    # 示例：logits.shape=(32,5)→predictions.shape=(32,)\n",
    "    \n",
    "    # 调用评估指标计算（假设metric是accuracy）\n",
    "    return metric.compute(\n",
    "        predictions=predictions,  # 模型预测的类别索引\n",
    "        references=labels         # 真实的类别索引\n",
    "    )\n",
    "\n",
    "\n",
    "# 典型应用场景示例\n",
    "\"\"\"\n",
    "输入样例：\n",
    "eval_pred = (\n",
    "    np.array([[1.2, -0.5], [0.3, 2.1]], dtype=np.float32),  # logits（batch_size=2, num_classes=2）\n",
    "    np.array([0, 1], dtype=np.int32)                        # labels\n",
    ")\n",
    "输出结果：\n",
    "{'accuracy': 0.5}  # 第0个样本预测正确，第1个预测错误（argmax([0.3,2.1])=1，但标签是1，实际应该正确？这里可能需要检查示例数据）\n",
    "\"\"\"\n",
    "\n",
    "# 扩展功能建议\n",
    "\"\"\"\n",
    "1. 多指标计算：\n",
    "   f1 = evaluate.load(\"f1\")\n",
    "   return {\n",
    "       \"accuracy\": metric.compute(...),\n",
    "       \"f1\": f1.compute(...)\n",
    "   }\n",
    "\n",
    "2. 处理多维度输出（如NER）：\n",
    "   predictions = np.argmax(logits, axis=-1)  # shape=(batch_size, seq_len)\n",
    "\n",
    "3. 概率校准：\n",
    "   probs = softmax(logits, axis=-1)\n",
    "   return {\"roc_auc\": roc_auc_score(labels, probs[:,1])}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2feba67-9ca9-4793-9a15-3eaa426df2a1",
   "metadata": {},
   "source": [
    "#### 训练过程指标监控\n",
    "\n",
    "通常，为了监控训练过程中的评估指标变化，我们可以在`TrainingArguments`指定`evaluation_strategy`参数，以便在 epoch 结束时报告评估指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afaaee18-4986-4e39-8ad9-b8d413ab4cd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 初始化训练器\\ntrainer = Trainer(\\n    model=model,\\n    args=training_args,\\n    train_dataset=train_dataset,\\n    eval_dataset=val_dataset,\\n    compute_metrics=compute_metrics  # 需要自定义评估函数\\n)\\n\\n# 启动训练\\ntrainer.train() \\n\\n# 保存最终模型\\ntrainer.save_model(\"final_model\")\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "# 🎛️ 训练参数配置\n",
    "training_args = TrainingArguments(\n",
    "    # 必须参数\n",
    "    output_dir=model_dir,  # 模型保存路径（⚠️重要！会自动创建以下内容）\n",
    "    # 📂 目录结构示例：\n",
    "    #   ├── config.json\n",
    "    #   ├── runs/ (TensorBoard日志)\n",
    "    #   └── checkpoint-100/ (自动保存的检查点)\n",
    "    \n",
    "    # 🔄 验证策略\n",
    "    evaluation_strategy=\"epoch\",  # 每个epoch后验证（可选：\"steps\"按步验证/\"no\"不验证）\n",
    "    # eval_steps=500,           # 当evaluation_strategy=\"steps\"时，每500步验证一次\n",
    "    \n",
    "    # 🚀 训练效率参数\n",
    "    per_device_train_batch_size=16,  # 每个GPU的批次大小（调整依据显存）\n",
    "    # 总批次大小 = 16 * GPU数量 * gradient_accumulation_steps\n",
    "    \n",
    "    # ⏱️ 训练时长控制\n",
    "    num_train_epochs=3,           # 训练总轮次（推荐3-5轮用于微调）\n",
    "    \n",
    "    # 📊 日志与监控\n",
    "    logging_steps=30,             # 每30训练步记录一次日志（默认500）\n",
    "    # report_to=\"wandb\",         # 集成可视化工具（可选：\"tensorboard\"/\"wandb\"）\n",
    "    \n",
    "    # 💡 推荐添加的优化参数\n",
    "    # learning_rate=5e-5,        # BERT常用学习率（默认5e-5）\n",
    "    # fp16=True,                # 混合精度训练（节省30%显存）\n",
    "    # gradient_accumulation_steps=2, # 梯度累积（模拟更大批次）\n",
    "    # warmup_steps=500,         # 学习率预热步数\n",
    "    # load_best_model_at_end=True, # 训练完成时加载最佳模型\n",
    ")\n",
    "\n",
    "# 典型应用场景示例\n",
    "\"\"\"\n",
    "# 初始化训练器\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics  # 需要自定义评估函数\n",
    ")\n",
    "\n",
    "# 启动训练\n",
    "trainer.train() \n",
    "\n",
    "# 保存最终模型\n",
    "trainer.save_model(\"final_model\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d6981-e444-4c0f-a7cb-dd7f2ba8df12",
   "metadata": {},
   "source": [
    "## 开始训练\n",
    "\n",
    "### 实例化训练器（Trainer）\n",
    "\n",
    "`kernel version` 版本问题：暂不影响本示例代码运行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca1d12ac-89dc-4c30-8282-f859724c0062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n| 参数                | 作用                                 | 典型值示例                     |\\n|---------------------|--------------------------------------|-------------------------------|\\n| model               | 定义模型架构                         | BertForSequenceClassification |\\n| args                | 控制训练超参数                       | TrainingArguments实例         |\\n| train_dataset       | 模型学习的训练数据                   | Dataset对象（1w条样本）       |\\n| eval_dataset        | 监控模型性能的验证数据               | Dataset对象（2k条样本）       |\\n| compute_metrics     | 定义评估指标计算方法                 | 返回{accuracy: 0.85}的函数    |\\n| data_collator       | 自定义数据打包逻辑（如动态填充）      | DataCollatorWithPadding       |\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 初始化训练器 (核心训练引擎)\n",
    "trainer = Trainer(\n",
    "    # 🧠 模型配置\n",
    "    model=model,  # 要训练的模型实例（需继承自PreTrainedModel）\n",
    "    # 示例：AutoModelForSequenceClassification.from_pretrained(...)\n",
    "    \n",
    "    # ⚙️ 训练参数配置\n",
    "    args=training_args,  # 训练参数对象（包含batch_size/epochs等超参数）\n",
    "    # 通过TrainingArguments类创建，控制训练全过程\n",
    "    \n",
    "    # 📦 数据配置\n",
    "    train_dataset=small_train_dataset,  # 训练集（Dataset对象）\n",
    "    # 建议格式：datasets.Dataset或torch.utils.data.Dataset\n",
    "    eval_dataset=small_eval_dataset,    # 验证集（用于评估模型性能）\n",
    "    # 数据预处理应在加载dataset前完成\n",
    "    \n",
    "    # 📊 评估指标配置\n",
    "    compute_metrics=compute_metrics,  # 自定义评估指标计算函数\n",
    "    # 该函数接收eval_pred(logits, labels)，返回指标字典\n",
    "    # 示例：计算准确率/召回率等\n",
    "    \n",
    "    # 🔄 可选高级配置（按需添加）\n",
    "    # data_collator=collate_fn,        # 自定义批次数据打包逻辑\n",
    "    # callbacks=[EarlyStoppingCallback(...)], # 训练回调（如早停）\n",
    "    # tokenizer=tokenizer,            # 用于数据预处理记录\n",
    ")\n",
    "\n",
    "# 典型训练流程\n",
    "\"\"\"\n",
    "1. 启动训练：\n",
    "   trainer.train() \n",
    "   \n",
    "2. 评估模型：\n",
    "   eval_results = trainer.evaluate()\n",
    "   \n",
    "3. 保存最佳模型：\n",
    "   trainer.save_model(\"best_model\")\n",
    "   \n",
    "4. 查看训练日志：\n",
    "   !tensorboard --logdir=models/bert-base-cased-finetune-yelp/runs/\n",
    "\"\"\"\n",
    "\n",
    "# 参数详解表\n",
    "\"\"\"\n",
    "| 参数                | 作用                                 | 典型值示例                     |\n",
    "|---------------------|--------------------------------------|-------------------------------|\n",
    "| model               | 定义模型架构                         | BertForSequenceClassification |\n",
    "| args                | 控制训练超参数                       | TrainingArguments实例         |\n",
    "| train_dataset       | 模型学习的训练数据                   | Dataset对象（1w条样本）       |\n",
    "| eval_dataset        | 监控模型性能的验证数据               | Dataset对象（2k条样本）       |\n",
    "| compute_metrics     | 定义评估指标计算方法                 | 返回{accuracy: 0.85}的函数    |\n",
    "| data_collator       | 自定义数据打包逻辑（如动态填充）      | DataCollatorWithPadding       |\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833e0db-1168-4a3c-8b75-bfdcef8c5157",
   "metadata": {},
   "source": [
    "## 使用 nvidia-smi 查看 GPU 使用\n",
    "\n",
    "为了实时查看GPU使用情况，可以使用 `watch` 指令实现轮询：`watch -n 1 nvidia-smi`:\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023\n",
    "\n",
    "Wed Dec 20 14:37:41 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accfe921-471d-481a-96da-c491cdebad0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [189/189 05:38, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.449800</td>\n",
       "      <td>1.203277</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.031600</td>\n",
       "      <td>0.998879</td>\n",
       "      <td>0.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.765300</td>\n",
       "      <td>0.973219</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=189, training_loss=1.1220053264072962, metrics={'train_runtime': 341.0837, 'train_samples_per_second': 8.795, 'train_steps_per_second': 0.554, 'total_flos': 789354427392000.0, 'train_loss': 1.1220053264072962, 'epoch': 3.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d581099-37a4-4470-b051-1ada38554089",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ffb47eab-1370-491e-8a84-6d5347a350b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.0240269899368286,\n",
       " 'eval_accuracy': 0.49,\n",
       " 'eval_runtime': 2.9923,\n",
       " 'eval_samples_per_second': 33.419,\n",
       " 'eval_steps_per_second': 4.344,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(small_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a55686-7c43-4ab8-a5cd-0e77f14c7c52",
   "metadata": {},
   "source": [
    "### 保存模型和训练状态\n",
    "\n",
    "- 使用 `trainer.save_model` 方法保存模型，后续可以通过 from_pretrained() 方法重新加载\n",
    "- 使用 `trainer.save_state` 方法保存训练状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad0cbc14-9ef7-450f-a1a3-4f92b6486f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e30510-0536-49d4-8e1b-43fc25272bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "badf5868-2847-439d-a73e-42d1cca67b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9441ad-f65a-42b7-9016-4809c78285e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fd92e35d-fed7-4ff2-aa84-27b5e29b917a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.model.save_pretrained(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61828934-01da-4fc3-9e75-8d754c25dfbc",
   "metadata": {},
   "source": [
    "## Homework: 使用完整的 YelpReviewFull 数据集训练，看 Acc 最高能到多少"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee2580a-7a5a-46ae-a28b-b41e9e838eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载完整数据集（已下载时自动使用缓存）\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5b736d2-53d4-4ba9-8286-207ffa07fa78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 650000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'text'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "878fa043-b4f7-4c1d-8fc7-1011fb6f069b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 3,\n",
       " 'text': \"All in favor of a deep dish pizza say I!.......IIIIIII,  ok now that i have that out of my system. This place is such a great hangout/eat-in spot. I hadn't been here and years and some friends invited us out for the evening. I was so glad they were paying cause  I was low on funds at the time.\\\\n\\\\nWe arrived on a friday night and of course it was busy there. We waited about 10 minutes to get a table which wasn't bad considering the crowd. We looked over the menu and they have so many great choices. Pizza, pasta, appetizers, seafood, burgers, salads and sandwiches. \\\\n\\\\nAfter ordering two mango lemonades that were wayyyyy over sweetened we ordered our food. We both are going gluten free which is tough but UNO's gave us a nice selection of dishes to choose from. Plus! They make a thin crust gluten free pizza which taste great. My hubby ordered the mediterrean thin crust because he loves kalamata olives and I ordered the Guac-alicious burger with a Caesar side salad. My salad came out pretty quick which was nice but it had a little too much dressing on it. I didn't complain, it still tasted great.\\\\n\\\\nI'm not into red meat so I tried to order a black bean burger or get chicken instead of beef, but the ran out of black bean and they couldn't get the chicken so i just ordered it anyway. The burger was piled really high with all the toppings including guacamole and it was very creamy but i couldn't get over the taste of the burger because it just didn't have any flavor. Very saddening. I ended up just eating the veggies and discarding the meat. I snacked on some of my hubbies pizza even though it was only a small amount. \\\\n\\\\nWe came here twice in one week. The second time we ordered the 9-grain deep dish with mushrooms, parmesan and a garlic white sauce. Was this pizza amazing or what?? I will probably always eat this pizza whenever I come. \\\\n\\\\nOnly down side is slow service. It took 20 minutes for our pizza to come out and my hubbies was a little over cooked. He got the numero uno which was ok but mine had way more flavor!!\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][333]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42376ce5-782c-409a-81f4-0eaad69fa9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b403788f5ab642598fb3a19c931bd620",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 预处理数据\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c56843d-f86c-467f-ae1c-bf502a7288c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 👈 必须放在所有import之前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72fb30a8-8df6-405b-b3e2-7e5810aa1de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## 微调训练配置\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0f4a7da-2aad-46a6-a953-263f4fea260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 训练超参数（TrainingArguments）\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model_dir = \"models/bert-base-cased-finetune-yelp-full\"\n",
    "\n",
    "# # logging_steps 默认值为500，根据我们的训练数据和步长，将其设置为100\n",
    "# training_args = TrainingArguments(output_dir=model_dir,\n",
    "#                                   per_device_train_batch_size=16,\n",
    "#                                   num_train_epochs=5,\n",
    "#                                   logging_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0c92158-53d2-4500-aa21-1367644252ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 训练过程中的指标评估（Evaluate)\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"./accuracy.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a4be496-cc67-4147-91bd-61e88343cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用 `compute` 函数来计算预测的准确率。\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c9ad2675-c929-46b6-b932-1bd512a4a963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 训练过程指标监控\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    # 输出目录：保存模型和日志的核心路径\n",
    "    output_dir=model_dir,  \n",
    "\n",
    "    # 维持工作进程\n",
    "    dataloader_persistent_workers=True,\n",
    "    \n",
    "    # 评估策略：每个epoch结束后在验证集评估（更耗时但结果准确）\n",
    "    evaluation_strategy=\"epoch\",  \n",
    "    \n",
    "    # 批次配置：物理batch_size=8，通过2次梯度累积等效于16\n",
    "    per_device_train_batch_size=12,     # 适合T4等中等显存GPU\n",
    "    gradient_accumulation_steps=2,     # 累计2个batch的梯度再更新参数\n",
    "    \n",
    "    # 训练轮次：3轮在650k数据下可能略少（推荐5-10轮）\n",
    "    num_train_epochs=5,  \n",
    "    \n",
    "    # 日志记录：每100步打印日志（约每100*8=800样本记录一次）\n",
    "    logging_steps=100,  \n",
    "    \n",
    "    # 混合精度：开启FP16训练（需GPU支持）\n",
    "    fp16=True,\n",
    "    learning_rate=3e-5,          # 重要！BERT微调黄金学习率\n",
    "    weight_decay=0.01,           # 防止过拟合\n",
    "    warmup_ratio=0.1,            # 前10%步数用于学习率预热\n",
    "    save_strategy=\"epoch\",       # 每个epoch保存检查点\n",
    "    load_best_model_at_end=True, # 训练结束加载最佳模型\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    # CPU并行优化配置（重点调整部分）\n",
    "    dataloader_num_workers=4        # ← 根据8核设置为4（最佳实践：核心数的50%）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f22f44f-4e97-4a57-8463-b88a249ea617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainingArguments(\n",
      "_n_gpu=1,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=True,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_steps=None,\n",
      "evaluation_strategy=epoch,\n",
      "fp16=True,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=2,\n",
      "gradient_checkpointing=False,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=True,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=3e-05,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=True,\n",
      "local_rank=0,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=models/bert-base-cased-finetune-yelp-full/runs/Mar07_01-44-51_deepseek-r1-t4-test,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=100,\n",
      "logging_strategy=steps,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=linear,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=accuracy,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=5,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "output_dir=models/bert-base-cased-finetune-yelp-full,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=8,\n",
      "per_device_train_batch_size=12,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=models/bert-base-cased-finetune-yelp-full,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=500,\n",
      "save_strategy=epoch,\n",
      "save_total_limit=None,\n",
      "seed=42,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=False,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.1,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.01,\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 完整的超参数配置\n",
    "print(training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12d3f311-be99-4ec9-a486-1ff020671a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "## 开始训练\n",
    "### 实例化训练器（Trainer）\n",
    "train_dataset = tokenized_datasets[\"train\"]\n",
    "eval_dataset = tokenized_datasets[\"test\"]\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d07b67a-6773-42b5-8857-d403832c754d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='65977' max='135415' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 65977/135415 15:01:56 < 15:49:16, 1.22 it/s, Epoch 2.44/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.740500</td>\n",
       "      <td>0.726306</td>\n",
       "      <td>0.682700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.704100</td>\n",
       "      <td>0.702620</td>\n",
       "      <td>0.690260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff50b32-cc78-4085-a4ac-9cee7ed06eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 创建小型测试集\n",
    "small_test_dataset = tokenized_datasets[\"test\"].shuffle(seed=64).select(range(100))\n",
    "# 2. 使用小型测试集评估\n",
    "small_results = trainer.evaluate(small_test_dataset)\n",
    "print(\"\\n📊 评估结果详情：\")\n",
    "for key, value in small_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key:25} → {value:.4f}\")  # 浮点数保留4位小数\n",
    "    else:\n",
    "        print(f\"{key:25} → {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bfa523-4c82-4401-bdf4-29f3b04dd30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用完整测试集评估\n",
    "full_results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"\\n📊 评估结果详情：\")\n",
    "for key, value in full_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key:25} → {value:.4f}\")  # 浮点数保留4位小数\n",
    "    else:\n",
    "        print(f\"{key:25} → {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c235f2b-f3b5-42a4-b4d3-6b39c8684f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 保存模型和训练状态\n",
    "trainer.save_model(model_dir)\n",
    "trainer.save_state()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
