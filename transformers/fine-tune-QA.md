# Hugging Face Transformers å¾®è°ƒè¯­è¨€æ¨¡å‹-é—®ç­”ä»»åŠ¡

æˆ‘ä»¬å·²ç»å­¦ä¼šä½¿ç”¨ Pipeline åŠ è½½æ”¯æŒé—®ç­”ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ¬æ•™ç¨‹ä»£ç å°†å±•ç¤ºå¦‚ä½•å¾®è°ƒè®­ç»ƒä¸€ä¸ªæ”¯æŒé—®ç­”ä»»åŠ¡çš„æ¨¡å‹ã€‚

**æ³¨æ„ï¼šå¾®è°ƒåçš„æ¨¡å‹ä»ç„¶æ˜¯é€šè¿‡æå–ä¸Šä¸‹æ–‡çš„å­ä¸²æ¥å›ç­”é—®é¢˜çš„ï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚**

### æ¨¡å‹æ‰§è¡Œé—®ç­”æ•ˆæœç¤ºä¾‹

![Widget inference representing the QA task](docs/images/question_answering.png)


```python
# æ ¹æ®ä½ ä½¿ç”¨çš„æ¨¡å‹å’ŒGPUèµ„æºæƒ…å†µï¼Œè°ƒæ•´ä»¥ä¸‹å…³é”®å‚æ•°
squad_v2 = False
model_checkpoint = "distilbert-base-uncased"
batch_size = 16
```

## ä¸‹è½½æ•°æ®é›†

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuADï¼‰](https://rajpurkar.github.io/SQuAD-explorer/)ã€‚

### SQuAD æ•°æ®é›†

**æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuAD)** æ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±ä¼—åŒ…å·¥ä½œè€…åœ¨ä¸€ç³»åˆ—ç»´åŸºç™¾ç§‘æ–‡ç« ä¸Šæå‡ºé—®é¢˜ç»„æˆã€‚æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ç›¸åº”é˜…è¯»æ®µè½ä¸­çš„æ–‡æœ¬ç‰‡æ®µæˆ–èŒƒå›´ï¼Œæˆ–è€…è¯¥é—®é¢˜å¯èƒ½æ— æ³•å›ç­”ã€‚

SQuAD2.0å°†SQuAD1.1ä¸­çš„10ä¸‡ä¸ªé—®é¢˜ä¸ç”±ä¼—åŒ…å·¥ä½œè€…å¯¹æŠ—æ€§åœ°æ’°å†™çš„5ä¸‡å¤šä¸ªæ— æ³•å›ç­”çš„é—®é¢˜ç›¸ç»“åˆï¼Œä½¿å…¶çœ‹èµ·æ¥ä¸å¯å›ç­”çš„é—®é¢˜ç±»ä¼¼ã€‚è¦åœ¨SQuAD2.0ä¸Šè¡¨ç°è‰¯å¥½ï¼Œç³»ç»Ÿä¸ä»…å¿…é¡»åœ¨å¯èƒ½æ—¶å›ç­”é—®é¢˜ï¼Œè¿˜å¿…é¡»ç¡®å®šæ®µè½ä¸­æ²¡æœ‰æ”¯æŒä»»ä½•ç­”æ¡ˆï¼Œå¹¶æ”¾å¼ƒå›ç­”ã€‚


```python
from datasets import load_dataset
```


```python
datasets = load_dataset("squad_v2" if squad_v2 else "squad")
```


    Downloading readme: 0.00B [00:00, ?B/s]



    Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]



    Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]



    Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]



    Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]


The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set.


```python
datasets
```




    DatasetDict({
        train: Dataset({
            features: ['id', 'title', 'context', 'question', 'answers'],
            num_rows: 87599
        })
        validation: Dataset({
            features: ['id', 'title', 'context', 'question', 'answers'],
            num_rows: 10570
        })
    })



#### å¯¹æ¯”æ•°æ®é›†

ç›¸æ¯”å¿«é€Ÿå…¥é—¨ä½¿ç”¨çš„ Yelp è¯„è®ºæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° SQuAD è®­ç»ƒå’Œæµ‹è¯•é›†éƒ½æ–°å¢äº†ç”¨äºä¸Šä¸‹æ–‡ã€é—®é¢˜ä»¥åŠé—®é¢˜ç­”æ¡ˆçš„åˆ—ï¼š

**YelpReviewFull Datasetï¼š**

```json

DatasetDict({
    train: Dataset({
        features: ['label', 'text'],
        num_rows: 650000
    })
    test: Dataset({
        features: ['label', 'text'],
        num_rows: 50000
    })
})
```


```python
datasets["train"][0]
```




    {'id': '5733be284776f41900661182',
     'title': 'University_of_Notre_Dame',
     'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',
     'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',
     'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}




```python
datasets["train"][333]
```




    {'id': '56d443ef2ccc5a1400d830db',
     'title': 'BeyoncÃ©',
     'context': 'BeyoncÃ© attended St. Mary\'s Elementary School in Fredericksburg, Texas, where she enrolled in dance classes. Her singing talent was discovered when dance instructor Darlette Johnson began humming a song and she finished it, able to hit the high-pitched notes. BeyoncÃ©\'s interest in music and performing continued after winning a school talent show at age seven, singing John Lennon\'s "Imagine" to beat 15/16-year-olds. In fall of 1990, BeyoncÃ© enrolled in Parker Elementary School, a music magnet school in Houston, where she would perform with the school\'s choir. She also attended the High School for the Performing and Visual Arts and later Alief Elsik High School. BeyoncÃ© was also a member of the choir at St. John\'s United Methodist Church as a soloist for two years.',
     'question': "What city was BeyoncÃ©'s elementary school located in?",
     'answers': {'text': ['Fredericksburg'], 'answer_start': [49]}}



#### ä»ä¸Šä¸‹æ–‡ä¸­ç»„ç»‡å›å¤å†…å®¹

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç­”æ¡ˆæ˜¯é€šè¿‡å®ƒä»¬åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®ï¼ˆè¿™é‡Œæ˜¯ç¬¬515ä¸ªå­—ç¬¦ï¼‰ä»¥åŠå®ƒä»¬çš„å®Œæ•´æ–‡æœ¬è¡¨ç¤ºçš„ï¼Œè¿™æ˜¯ä¸Šé¢æåˆ°çš„ä¸Šä¸‹æ–‡çš„å­å­—ç¬¦ä¸²ã€‚


```python
from datasets import ClassLabel, Sequence
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):
            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])
    display(HTML(df.to_html()))
```


```python
show_random_elements(datasets["train"])
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>title</th>
      <th>context</th>
      <th>question</th>
      <th>answers</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5731ab21b9d445190005e44f</td>
      <td>Religion_in_ancient_Rome</td>
      <td>The meaning and origin of many archaic festivals baffled even Rome's intellectual elite, but the more obscure they were, the greater the opportunity for reinvention and reinterpretation â€” a fact lost neither on Augustus in his program of religious reform, which often cloaked autocratic innovation, nor on his only rival as mythmaker of the era, Ovid. In his Fasti, a long-form poem covering Roman holidays from January to June, Ovid presents a unique look at Roman antiquarian lore, popular customs, and religious practice that is by turns imaginative, entertaining, high-minded, and scurrilous; not a priestly account, despite the speaker's pose as a vates or inspired poet-prophet, but a work of description, imagination and poetic etymology that reflects the broad humor and burlesque spirit of such venerable festivals as the Saturnalia, Consualia, and feast of Anna Perenna on the Ides of March, where Ovid treats the assassination of the newly deified Julius Caesar as utterly incidental to the festivities among the Roman people. But official calendars preserved from different times and places also show a flexibility in omitting or expanding events, indicating that there was no single static and authoritative calendar of required observances. In the later Empire under Christian rule, the new Christian festivals were incorporated into the existing framework of the Roman calendar, alongside at least some of the traditional festivals.</td>
      <td>What poet wrote a long poem describing Roman religious holidays?</td>
      <td>{'text': ['Ovid'], 'answer_start': [346]}</td>
    </tr>
    <tr>
      <th>1</th>
      <td>56e08b457aa994140058e5e3</td>
      <td>Hydrogen</td>
      <td>Hydrogen forms a vast array of compounds with carbon called the hydrocarbons, and an even vaster array with heteroatoms that, because of their general association with living things, are called organic compounds. The study of their properties is known as organic chemistry and their study in the context of living organisms is known as biochemistry. By some definitions, "organic" compounds are only required to contain carbon. However, most of them also contain hydrogen, and because it is the carbon-hydrogen bond which gives this class of compounds most of its particular chemical characteristics, carbon-hydrogen bonds are required in some definitions of the word "organic" in chemistry. Millions of hydrocarbons are known, and they are usually formed by complicated synthetic pathways, which seldom involve elementary hydrogen.</td>
      <td>What is the form of hydrogen and carbon called?</td>
      <td>{'text': ['hydrocarbons'], 'answer_start': [64]}</td>
    </tr>
    <tr>
      <th>2</th>
      <td>56cef65baab44d1400b88d36</td>
      <td>Spectre_(2015_film)</td>
      <td>Christopher Orr, writing in The Atlantic, also criticised the film, saying that Spectre "backslides on virtually every [aspect]". Lawrence Toppman of The Charlotte Observer called Craig's performance "Bored, James Bored." Alyssa Rosenberg, writing for The Washington Post, stated that the film turned into "a disappointingly conventional Bond film."</td>
      <td>What adjective did Lawrence Toppman use to describe Craig's portrayal of James Bond?</td>
      <td>{'text': ['Bored'], 'answer_start': [201]}</td>
    </tr>
    <tr>
      <th>3</th>
      <td>571a30bb10f8ca1400304f53</td>
      <td>Seattle</td>
      <td>King County Metro provides frequent stop bus service within the city and surrounding county, as well as a South Lake Union Streetcar line between the South Lake Union neighborhood and Westlake Center in downtown. Seattle is one of the few cities in North America whose bus fleet includes electric trolleybuses. Sound Transit currently provides an express bus service within the metropolitan area; two Sounder commuter rail lines between the suburbs and downtown; its Central Link light rail line, which opened in 2009, between downtown and Sea-Tac Airport gives the city its first rapid transit line that has intermediate stops within the city limits. Washington State Ferries, which manages the largest network of ferries in the United States and third largest in the world, connects Seattle to Bainbridge and Vashon Islands in Puget Sound and to Bremerton and Southworth on the Kitsap Peninsula.</td>
      <td>To what two islands does the ferry service connect?</td>
      <td>{'text': ['Bainbridge and Vashon'], 'answer_start': [796]}</td>
    </tr>
    <tr>
      <th>4</th>
      <td>570d2cb4fed7b91900d45cb5</td>
      <td>Macintosh</td>
      <td>In 1998, after the return of Steve Jobs, Apple consolidated its multiple consumer-level desktop models into the all-in-one iMac G3, which became a commercial success and revitalized the brand. Since their transition to Intel processors in 2006, the complete lineup is entirely based on said processors and associated systems. Its current lineup comprises three desktops (the all-in-one iMac, entry-level Mac mini, and the Mac Pro tower graphics workstation), and four laptops (the MacBook, MacBook Air, MacBook Pro, and MacBook Pro with Retina display). Its Xserve server was discontinued in 2011 in favor of the Mac Mini and Mac Pro.</td>
      <td>What took the place of Mac's Xserve server?</td>
      <td>{'text': ['Mac Mini and Mac Pro'], 'answer_start': [613]}</td>
    </tr>
    <tr>
      <th>5</th>
      <td>570af6876b8089140040f646</td>
      <td>Videoconferencing</td>
      <td>Technological developments by videoconferencing developers in the 2010s have extended the capabilities of video conferencing systems beyond the boardroom for use with hand-held mobile devices that combine the use of video, audio and on-screen drawing capabilities broadcasting in real-time over secure networks, independent of location. Mobile collaboration systems now allow multiple people in previously unreachable locations, such as workers on an off-shore oil rig, the ability to view and discuss issues with colleagues thousands of miles away. Traditional videoconferencing system manufacturers have begun providing mobile applications as well, such as those that allow for live and still image streaming.</td>
      <td>What is one example of an application that videoconferencing manufacturers have begun to offer?</td>
      <td>{'text': ['still image streaming'], 'answer_start': [689]}</td>
    </tr>
    <tr>
      <th>6</th>
      <td>56e82d0100c9c71400d775eb</td>
      <td>Dialect</td>
      <td>Italy is home to a vast array of native regional minority languages, most of which are Romance-based and have their own local variants. These regional languages are often referred to colloquially or in non-linguistic circles as Italian "dialects," or dialetti (standard Italian for "dialects"). However, the majority of the regional languages in Italy are in fact not actually "dialects" of standard Italian in the strict linguistic sense, as they are not derived from modern standard Italian but instead evolved locally from Vulgar Latin independent of standard Italian, with little to no influence from what is now known as "standard Italian." They are therefore better classified as individual languages rather than "dialects."</td>
      <td>What are Italian dialects termed in the Italian language?</td>
      <td>{'text': ['dialetti'], 'answer_start': [251]}</td>
    </tr>
    <tr>
      <th>7</th>
      <td>56e147e6cd28a01900c6772b</td>
      <td>Universal_Studios</td>
      <td>The Universal Film Manufacturing Company was incorporated in New York on April 30, 1912. Laemmle, who emerged as president in July 1912, was the primary figure in the partnership with Dintenfass, Baumann, Kessel, Powers, Swanson, Horsley, and Brulatour. Eventually all would be bought out by Laemmle. The new Universal studio was a vertically integrated company, with movie production, distribution and exhibition venues all linked in the same corporate entity, the central element of the Studio system era.</td>
      <td>Along with exhibition and distribution, what business did the Universal Film Manufacturing Company engage in?</td>
      <td>{'text': ['movie production'], 'answer_start': [368]}</td>
    </tr>
    <tr>
      <th>8</th>
      <td>5731933a05b4da19006bd2d0</td>
      <td>Steven_Spielberg</td>
      <td>Spielberg's next film, Schindler's List, was based on the true story of Oskar Schindler, a man who risked his life to save 1,100 Jews from the Holocaust. Schindler's List earned Spielberg his first Academy Award for Best Director (it also won Best Picture). With the film a huge success at the box office, Spielberg used the profits to set up the Shoah Foundation, a non-profit organization that archives filmed testimony of Holocaust survivors. In 1997, the American Film Institute listed it among the 10 Greatest American Films ever Made (#9) which moved up to (#8) when the list was remade in 2007.</td>
      <td>Whose life was 'Schindler's List' based on?</td>
      <td>{'text': ['Oskar Schindler'], 'answer_start': [72]}</td>
    </tr>
    <tr>
      <th>9</th>
      <td>56de93f94396321400ee2a36</td>
      <td>Arnold_Schwarzenegger</td>
      <td>In 1985, Schwarzenegger appeared in "Stop the Madness", an anti-drug music video sponsored by the Reagan administration. He first came to wide public notice as a Republican during the 1988 presidential election, accompanying then-Vice President George H.W. Bush at a campaign rally.</td>
      <td>In what presidential election year did Schwarzenegger make a name for himself as a prominent Republican?</td>
      <td>{'text': ['1988'], 'answer_start': [184]}</td>
    </tr>
  </tbody>
</table>


## é¢„å¤„ç†æ•°æ®

**Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰å°±åƒä¸€ä½ã€Œè¯­è¨€æ‹†è§£ä¸“å®¶ã€**ï¼Œä¸“é—¨å¸®è®¡ç®—æœºç†è§£äººç±»æ–‡å­—ã€‚å®ƒçš„æ ¸å¿ƒä½œç”¨å¯ä»¥ç”¨ä¸‰æ­¥è¯´æ¸…æ¥šï¼š

---

### 1ï¸âƒ£ **æ‹†è§£æ–‡æœ¬**  
æŠŠå¥å­æ‹†æˆ **æ¨¡å‹è®¤è¯†çš„ç‰‡æ®µ**ï¼ˆè¯æˆ–å­è¯ï¼‰ã€‚  
ä¾‹å¦‚ï¼š  
`"æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"` â†’ `["æˆ‘", "çˆ±", "è‡ªç„¶", "è¯­è¨€", "å¤„ç†"]`  
ï¼ˆè‹±æ–‡å¦‚ `"Hugging Face"` â†’ `["Hug", "##ging", "Face"]`ï¼‰

---

### 2ï¸âƒ£ **æ·»åŠ ã€Œæš—å·ã€**  
æ’å…¥æ¨¡å‹éœ€è¦çš„**ç‰¹æ®Šæ ‡è®°**ï¼Œæ¯”å¦‚ï¼š  
- **`[CLS]`**ï¼šå¼€å¤´æ ‡è®°ï¼ˆBERTç”¨ï¼‰  
- **`[SEP]`**ï¼šåˆ†éš”æ ‡è®°ï¼ˆåŒºåˆ†å¥å­ï¼‰  
```python
"ä½ å¥½å—ï¼Ÿ" â†’ ["[CLS]", "ä½ ", "å¥½", "å—", "ï¼Ÿ", "[SEP]"]
```

---

### 3ï¸âƒ£ **è½¬æˆå¯†ç æ•°å­—**  
æŠŠæ¯ä¸ªè¯æ¢æˆ**æ¨¡å‹è¯æ±‡è¡¨é‡Œçš„IDå·**ï¼Œç±»ä¼¼å¯†ç æœ¬ï¼š  
```python
["[CLS]", "ä½ ", "å¥½", "å—"] â†’ [101, 872, 1962, 3221, 102]
```

---

### ğŸŒ° **å®é™…æ•ˆæœç¤ºä¾‹**  
ä½ è¾“å…¥ï¼š`"ä»Šå¤©å¦é—¨å¤©æ°”å¦‚ä½•ï¼Ÿ"`  
Tokenizerå¤„ç†åè¾“å‡ºï¼š  
```python
{
  "input_ids": [101, 791, 1921, 1762, 1377, 1442, 3221, 102],
  "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1]  # æ ‡è®°å“ªäº›æ˜¯æœ‰æ•ˆå†…å®¹
}
```
æ¨¡å‹çœ‹åˆ°è¿™äº›æ•°å­—å°±èƒ½åˆ†æè¯­ä¹‰ï¼Œç”Ÿæˆå›ç­”å•¦ï¼

---

### ğŸ¤– **ä¸åŒæ¨¡å‹çš„å·®å¼‚**  
- **BERTç±»**ï¼šæ‹†è¯è¾ƒç»†ï¼ŒåŠ `[CLS]`/`[SEP]`  
- **GPTç±»**ï¼šæŒ‰å­—èŠ‚æ‹†åˆ†ï¼ŒåŠ `<|endoftext|>`  
- **å¤šè¯­è¨€æ¨¡å‹**ï¼šæ”¯æŒä¸­/è‹±/æ—¥ç­‰æ··åˆæ‹†åˆ†  

ä¸€å¥è¯æ€»ç»“ï¼š**Tokenizerå°±æ˜¯æŠŠäººç±»è¯­è¨€ã€Œç¿»è¯‘ã€æˆAIèƒ½æ‡‚çš„æ•°å­—å¯†ç ï¼** ğŸ˜Š


```python
from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

    /root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
      warnings.warn(


**AutoTokenizer å°±åƒã€Œä¸‡èƒ½é€‚é…å™¨ã€**  
â€”â€”ä½ åªéœ€è¦å‘Šè¯‰å®ƒç”¨å“ªä¸ªAIæ¨¡å‹ï¼ˆæ¯”å¦‚BERTã€GPT-3ï¼‰ï¼Œå®ƒå°±ä¼šè‡ªåŠ¨åŒ¹é…å¯¹åº”çš„æ–‡å­—ç¿»è¯‘è§„åˆ™ã€‚

ä¸¾ä¸ªæ —å­ğŸŒ°ï¼š  
- ä½ æƒ³ç”¨ **BERT** æ¨¡å‹ â†’ å®ƒè‡ªåŠ¨åŠ è½½BERTçš„åˆ†è¯è§„åˆ™  
  ```python
  tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
  ```
- ä½ æƒ³ç”¨ **GPT** æ¨¡å‹ â†’ å®ƒè‡ªåŠ¨åˆ‡æ¢æˆGPTçš„åˆ†è¯æ–¹å¼  
  ```python
  tokenizer = AutoTokenizer.from_pretrained("gpt2")
  ```

**å¥½å¤„**ï¼šä¸ç”¨è®°ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨åå­—ï¼ˆæ¯”å¦‚`BertTokenizer`ã€`GPT2Tokenizer`ï¼‰ï¼Œä¸€ä¸ª`AutoTokenizer`é€šåƒæ‰€æœ‰æ¨¡å‹ï¼Œå°±åƒä¸‡èƒ½å……ç”µå™¨ä¸€æ ·æ–¹ä¾¿ï¼

---

### å¯¹æ¯”ç¤ºä¾‹ï¼ˆæ‰‹åŠ¨ vs è‡ªåŠ¨ï¼‰
| æ–¹å¼          | æ‰‹åŠ¨é€‰æ‹©åˆ†è¯å™¨                   | AutoTokenizer                  |
|---------------|----------------------------------|---------------------------------|
| **BERTæ¨¡å‹**  | `from transformers import BertTokenizer`<br>`tokenizer = BertTokenizer.from_pretrained("bert-base")` | `AutoTokenizer.from_pretrained("bert-base")` |
| **GPTæ¨¡å‹**   | `from transformers import GPT2Tokenizer`<br>`tokenizer = GPT2Tokenizer.from_pretrained("gpt2")` | `AutoTokenizer.from_pretrained("gpt2")` |

---

âš ï¸ **æ³¨æ„**ï¼šåå­—è¦å¯¹ï¼ˆæ¯”å¦‚`bert-base-chinese`ä¸èƒ½å†™æˆ`bert-chinese`ï¼‰ï¼Œå¦åˆ™è¿™ä¸ªä¸‡èƒ½å……ç”µå™¨ä¹Ÿä¼šæ‰¾ä¸åˆ°æ’å£~

ä»¥ä¸‹æ–­è¨€ç¡®ä¿æˆ‘ä»¬çš„ Tokenizers ä½¿ç”¨çš„æ˜¯ FastTokenizerï¼ˆRust å®ç°ï¼Œé€Ÿåº¦å’ŒåŠŸèƒ½æ€§ä¸Šæœ‰ä¸€å®šä¼˜åŠ¿ï¼‰ã€‚


```python
import transformers
assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)
# ç›´æ¥æ‰“å°åˆ¤æ–­ç»“æœ
print("æ˜¯å¦æ˜¯å¿«é€Ÿç‰ˆåˆ†è¯å™¨:", isinstance(tokenizer, transformers.PreTrainedTokenizerFast))

# æˆ–æ›´è¯¦ç»†çš„è¾“å‡º
if isinstance(tokenizer, transformers.PreTrainedTokenizerFast):
    print("âœ… Tokenizer æ˜¯å¿«é€Ÿç‰ˆ (PreTrainedTokenizerFast)")
else:
    print("âŒ Tokenizer æ˜¯æ™®é€šç‰ˆ (PreTrainedTokenizer)")

```

    æ˜¯å¦æ˜¯å¿«é€Ÿç‰ˆåˆ†è¯å™¨: True
    âœ… Tokenizer æ˜¯å¿«é€Ÿç‰ˆ (PreTrainedTokenizerFast)


**PreTrainedTokenizer å°±åƒä¸ªã€Œæ–‡å­—ç¿»è¯‘å®˜ã€**ï¼Œä¸“é—¨å¸® AI æ¨¡å‹å’Œäººç±»æ–‡å­—æ‰“äº¤é“ã€‚  

ä¸¾ä¸ªæ —å­ğŸŒ°ï¼š  
ä½ æƒ³é—® AI "å¦é—¨ä»Šå¤©çƒ­å—ï¼Ÿ"  
â¡ï¸ **ç¿»è¯‘å®˜çš„å·¥ä½œ**ï¼š  
1. æŠŠè¿™å¥è¯åˆ‡æˆå°å—ï¼š`["å¦é—¨", "ä»Šå¤©", "çƒ­", "å—"]`  
2. å·å·åŠ æš—å·ï¼š`[å¼€å¤´æš—å·] å¦é—¨ ä»Šå¤© çƒ­ å— [ç»“å°¾æš—å·]`  
3. è½¬æˆå¯†ç æ•°å­—ï¼š`[101, 2345, 567, 8910, 102]`  

ç„¶å AI å°±èƒ½çœ‹æ‡‚è¿™äº›æ•°å­—å¯†ç ï¼Œç»™å‡ºå›ç­”å•¦ï¼  
ï¼ˆåè¿‡æ¥ä¹Ÿä¼šæŠŠ AI çš„æ•°å­—å¯†ç ç¿»è¯‘æˆäººç±»æ–‡å­—ç»™ä½ çœ‹ï¼‰

`assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)`

è¿™ä¸ªæ­¥éª¤æ˜¯**å¯é€‰çš„å®‰å…¨æ£€æŸ¥**ï¼Œä¸»è¦ä¸ºäº†ç¡®ä¿ä½ åŠ è½½çš„æ˜¯**å¿«é€Ÿç‰ˆåˆ†è¯å™¨ï¼ˆPreTrainedTokenizerFastï¼‰**ï¼Œè€Œä¸æ˜¯æ—§ç‰ˆçš„æ…¢é€Ÿåˆ†è¯å™¨ï¼ˆPreTrainedTokenizerï¼‰ã€‚ä¸æ£€æŸ¥ä¹Ÿèƒ½è¿è¡Œï¼Œä½†å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é—®é¢˜ï¼š

---

### ğŸ¤” **ä¸ºä»€ä¹ˆè¦åŒºåˆ† Fast å’Œæ™®é€šç‰ˆï¼Ÿ**
| ç‰¹æ€§                | PreTrainedTokenizerFastï¼ˆå¿«é€Ÿç‰ˆï¼‰          | PreTrainedTokenizerï¼ˆæ™®é€šç‰ˆï¼‰       |
|---------------------|--------------------------------------------|-------------------------------------|
| **åº•å±‚å®ç°**         | Rustè¯­è¨€ç¼–å†™ï¼ˆé€Ÿåº¦å¿«ï¼‰                     | Pythonå®ç°ï¼ˆé€Ÿåº¦æ…¢ï¼‰                 |
| **æ‰¹å¤„ç†æ”¯æŒ**       | âœ… åŸç”Ÿæ”¯æŒï¼ˆå¦‚`batch_encode_plus`ï¼‰        | âŒ éœ€æ‰‹åŠ¨å¾ªç¯å¤„ç†                    |
| **ç‰¹æ®Šæ ‡è®°å¤„ç†**     | è‡ªåŠ¨ç®¡ç†ï¼ˆå¦‚å¡«å……ã€æˆªæ–­ï¼‰                   | éœ€æ‰‹åŠ¨é…ç½®                          |
| **å…¸å‹åœºæ™¯**         | ç”Ÿäº§ç¯å¢ƒã€å¤§æ•°æ®å¤„ç†                        | æ•™å­¦æˆ–å…¼å®¹æ—§ä»£ç                      |

---

### ğŸ’¥ **ä¸æ£€æŸ¥å¯èƒ½å¸¦æ¥çš„é—®é¢˜**
1. **æ€§èƒ½ä¸‹é™**ï¼šå¤„ç†1000æ¡æ–‡æœ¬æ—¶ï¼Œå¿«é€Ÿç‰ˆå¯èƒ½æ¯”æ™®é€šç‰ˆå¿«**5-10å€**ã€‚
2. **åŠŸèƒ½ç¼ºå¤±**ï¼šæ™®é€šç‰ˆå¯èƒ½ç¼ºå°‘æŸäº›APIï¼ˆå¦‚`decode`çš„`skip_special_tokens`å‚æ•°ï¼‰ã€‚
3. **æ„å¤–é”™è¯¯**ï¼šæŸäº›åº“ï¼ˆå¦‚Datasetsï¼‰é»˜è®¤è¦æ±‚å¿«é€Ÿç‰ˆåˆ†è¯å™¨ã€‚

---

### ğŸŒ° **å®é™…æ¡ˆä¾‹**
å‡è®¾ä½ çš„`model_checkpoint`æ„å¤–æŒ‡å‘äº†ä¸€ä¸ªæ²¡æœ‰å¿«é€Ÿç‰ˆçš„æ¨¡å‹ï¼š
```python
model_checkpoint = "some-old-model"  # å‡è®¾è¯¥æ¨¡å‹åªæœ‰æ™®é€šç‰ˆåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
# æ­¤æ—¶ tokenizer æ˜¯ PreTrainedTokenizer è€Œé Fast ç‰ˆ
# åç»­è°ƒç”¨ batch_encode_plus å¯èƒ½æŠ¥é”™ï¼
```

é€šè¿‡`assert`æ£€æŸ¥ï¼Œå¯ä»¥**æå‰å‘ç°é—®é¢˜**ï¼Œé¿å…åç»­ä»£ç å´©æºƒã€‚

---

### ğŸ”§ **æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚æœä¸åšæ–­è¨€ï¼‰**
1. **ç›´æ¥ä½¿ç”¨**ï¼šå¦‚æœç¡®å®šæ¨¡å‹æœ‰å¿«é€Ÿç‰ˆï¼Œå¯ä»¥è·³è¿‡æ£€æŸ¥ã€‚
2. **é™çº§å¤„ç†**ï¼šæ•è·å¼‚å¸¸å¹¶æ”¹ç”¨æ™®é€šç‰ˆé€»è¾‘ï¼š
```python
if not isinstance(tokenizer, PreTrainedTokenizerFast):
    print("è­¦å‘Šï¼šä½¿ç”¨æ…¢é€Ÿåˆ†è¯å™¨ï¼Œæ€§èƒ½å¯èƒ½å—å½±å“ï¼")
    # æ‰‹åŠ¨å¤„ç†æ™®é€šç‰ˆçš„é™åˆ¶
```

---

æ€»ç»“ï¼šè¿™ä¸ªæ–­è¨€æ˜¯**é˜²å¾¡æ€§ç¼–ç¨‹**çš„ä½“ç°ï¼Œç¡®ä¿ä»£ç åœ¨æ€§èƒ½å’ŒåŠŸèƒ½ä¸ŠæŒ‰é¢„æœŸè¿è¡Œã€‚å¯¹äºå…³é”®é¡¹ç›®å»ºè®®ä¿ç•™ï¼Œä¸ªäººå®éªŒå¯è·³è¿‡ã€‚

æ‚¨å¯ä»¥åœ¨å¤§æ¨¡å‹è¡¨ä¸ŠæŸ¥çœ‹å“ªç§ç±»å‹çš„æ¨¡å‹å…·æœ‰å¯ç”¨çš„å¿«é€Ÿæ ‡è®°å™¨ï¼Œå“ªç§ç±»å‹æ²¡æœ‰ã€‚

æ‚¨å¯ä»¥ç›´æ¥åœ¨ä¸¤ä¸ªå¥å­ä¸Šè°ƒç”¨æ­¤æ ‡è®°å™¨ï¼ˆä¸€ä¸ªç”¨äºç­”æ¡ˆï¼Œä¸€ä¸ªç”¨äºä¸Šä¸‹æ–‡ï¼‰ï¼š


```python
tokenizer("What is your name?", "My name is Sylvain.")
```




    {'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}




```python
 tokenizer("How are you?")
```




    {'input_ids': [101, 2129, 2024, 2017, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}



### Tokenizer è¿›é˜¶æ“ä½œ

åœ¨é—®ç­”é¢„å¤„ç†ä¸­çš„ä¸€ä¸ªç‰¹å®šé—®é¢˜æ˜¯å¦‚ä½•å¤„ç†éå¸¸é•¿çš„æ–‡æ¡£ã€‚

åœ¨å…¶ä»–ä»»åŠ¡ä¸­ï¼Œå½“æ–‡æ¡£çš„é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€å¤§å¥å­é•¿åº¦æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæˆªæ–­å®ƒä»¬ï¼Œä½†åœ¨è¿™é‡Œï¼Œåˆ é™¤ä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†å¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬ä¸¢å¤±æ­£åœ¨å¯»æ‰¾çš„ç­”æ¡ˆã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å…è®¸æ•°æ®é›†ä¸­çš„ä¸€ä¸ªï¼ˆé•¿ï¼‰ç¤ºä¾‹ç”Ÿæˆå¤šä¸ªè¾“å…¥ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„é•¿åº¦éƒ½å°äºæ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆæˆ–æˆ‘ä»¬è®¾ç½®çš„è¶…å‚æ•°ï¼‰ã€‚


```python
# The maximum length of a feature (question and context)
max_length = 384 
# The authorized overlap between two part of the context when splitting it is needed.
doc_stride = 128 
```

---

### **ä¸ºä½•è®¾ç½® `max_length=384`ï¼Ÿ**
1. **æ¨¡å‹é™åˆ¶**  
   BERTç­‰æ¨¡å‹æœ€å¤§æ”¯æŒ **512 tokens**ï¼Œéœ€ä¸ºä»¥ä¸‹å†…å®¹ç•™ç©ºé—´ï¼š  
   - **é—®é¢˜æœ¬èº«**ï¼ˆçº¦20-30 tokensï¼‰  
   - **ç‰¹æ®Šæ ‡è®°**ï¼ˆå¦‚ `[CLS]`ã€`[SEP]`ï¼Œå 3-5 tokensï¼‰  
   - **ç­”æ¡ˆä½ç½®**ï¼ˆé¿å…è¢«æˆªæ–­ï¼‰

2. **ç»éªŒæ¯”ä¾‹**  
   å¯ç”¨ä¸Šä¸‹æ–‡é•¿åº¦ â‰ˆ æ€»é•¿çš„ **75%**ï¼ˆ512Ã—0.75â‰ˆ384ï¼‰ï¼Œå¹³è¡¡è¦†ç›–ç‡å’Œè®¡ç®—æ•ˆç‡ã€‚

3. **åˆ†å—ä¼˜åŒ–**  
   ç»“åˆ `doc_stride=128`ï¼ˆé‡å é‡ï¼‰ï¼Œç¡®ä¿ç­”æ¡ˆåœ¨è‡³å°‘ä¸€ä¸ªåˆ†å—ä¸­å®Œæ•´å‡ºç°ã€‚

---

### **å®é™…æ¡ˆä¾‹**  
- **è¾“å…¥**ï¼šé—®é¢˜ï¼ˆ20 tokensï¼‰+ ä¸Šä¸‹æ–‡ï¼ˆ500 tokensï¼‰  
- **å¤„ç†**ï¼š  
  1. åˆ†å—1ï¼šé—®é¢˜ + ä¸Šä¸‹æ–‡0-363  
  2. åˆ†å—2ï¼šé—®é¢˜ + ä¸Šä¸‹æ–‡236-500ï¼ˆä¸åˆ†å—1é‡å 128 tokensï¼‰  
- **ç»“æœ**ï¼šå³ä½¿ç­”æ¡ˆåœ¨360-400åŒºé—´ï¼Œä¹Ÿèƒ½è¢«åˆ†å—2è¦†ç›–ã€‚

---

### **è°ƒæ•´å»ºè®®**
- **çŸ­æ–‡æœ¬ä»»åŠ¡**ï¼šç›´æ¥è®¾ä¸º512  
- **è¶…é•¿æ–‡æ¡£**ï¼šå¯é™ä½åˆ°256ï¼ˆéœ€æ›´å¤šåˆ†å—ï¼‰  
- **æ”¯æŒæ›´é•¿æ¨¡å‹**ï¼šå¦‚æ”¯æŒ1024ï¼Œå¯è®¾ä¸º768  

ä¸€å¥è¯æ€»ç»“ï¼š**384æ˜¯å¹³è¡¡æ¨¡å‹é™åˆ¶ã€ç­”æ¡ˆå®Œæ•´æ€§å’Œè®¡ç®—æ•ˆç‡çš„ç»éªŒå€¼ã€‚**

`doc_stride=128` çš„åŸç†ä¸ `max_length=384` ç±»ä¼¼ï¼Œä½†å…³æ³¨ç‚¹ä¸åŒã€‚ä»¥ä¸‹æ˜¯ç®€æ´æ¸…æ™°çš„è§£é‡Šï¼š

---

### **ä¸ºä½•è®¾ç½® `doc_stride=128`ï¼Ÿ**
1. **æ ¸å¿ƒç›®çš„**  
   **é¿å…ç­”æ¡ˆè¢«åˆ‡å‰²åœ¨åˆ†å—è¾¹ç•Œ**ã€‚é€šè¿‡è®¾ç½®åˆ†å—é—´çš„é‡å åŒºåŸŸï¼Œç¡®ä¿å³ä½¿ç­”æ¡ˆä½äºåˆ†å—è¾¹ç¼˜ï¼Œä¹Ÿèƒ½è¢«è‡³å°‘ä¸€ä¸ªå®Œæ•´åˆ†å—è¦†ç›–ã€‚

2. **ç»éªŒå…¬å¼**  
   `doc_stride` â‰ˆ `max_length` çš„ **1/3~1/4**ï¼ˆå¦‚ `384/3â‰ˆ128`ï¼‰ï¼Œå¹³è¡¡ï¼š
   - **è®¡ç®—æ•ˆç‡**ï¼ˆåˆ†å—è¶Šå°‘è¶Šå¥½ï¼‰
   - **ç­”æ¡ˆè¦†ç›–ç‡**ï¼ˆé‡å è¶Šå¤šè¶Šå®‰å…¨ï¼‰

---

### **åˆ†å—é€»è¾‘ç¤ºä¾‹**
- **å‚æ•°**ï¼š
  - `max_length=384`ï¼ˆæ€»é•¿åº¦ï¼‰
  - é—®é¢˜é•¿åº¦ = 20 tokens
  - å¯ç”¨ä¸Šä¸‹æ–‡é•¿åº¦ = `384 - 20 - 3ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰â‰ˆ 361 tokens`
  - `doc_stride=128`
- **åˆ†å—æ­¥é•¿** = `361 - 128 = 233 tokens`

| åˆ†å— | èµ·å§‹ä½ç½® | ç»“æŸä½ç½® | è¦†ç›–çš„ä¸Šä¸‹æ–‡èŒƒå›´ |
|------|----------|----------|------------------|
| 1    | 0        | 360      | tokens 0-360     |
| 2    | 233      | 593      | tokens 233-593   |
| 3    | 466      | 826      | tokens 466-826   |

- **å‡è®¾ç­”æ¡ˆåœ¨ tokens 350-370**ï¼š
  - åˆ†å—1ï¼šè¦†ç›–åˆ°360 â†’ ç­”æ¡ˆéƒ¨åˆ†æˆªæ–­ï¼ˆ350-360ä¿ç•™ï¼‰
  - åˆ†å—2ï¼šä»233å¼€å§‹ â†’ å®Œæ•´è¦†ç›–ç­”æ¡ˆï¼ˆ350-370ï¼‰

---

### **å…³é”®å½±å“**
| `doc_stride` å€¼ | ä¼˜ç‚¹               | ç¼ºç‚¹                 |
|-----------------|--------------------|----------------------|
| **è¾ƒå°ï¼ˆå¦‚64ï¼‰** | ç­”æ¡ˆè¦†ç›–ç‡â†‘        | åˆ†å—æ•°é‡â†‘ï¼Œè®¡ç®—é‡â†‘   |
| **è¾ƒå¤§ï¼ˆå¦‚192ï¼‰**| åˆ†å—æ•°é‡â†“ï¼Œé€Ÿåº¦â†‘   | æ¼ç­”é£é™©â†‘            |

---

### **è°ƒæ•´å»ºè®®**
- **çŸ­ç­”æ¡ˆä»»åŠ¡**ï¼ˆå¦‚å®ä½“æŠ½å–ï¼‰ï¼š`doc_stride=64~128`
- **é•¿ç­”æ¡ˆä»»åŠ¡**ï¼ˆå¦‚æ®µè½æ€»ç»“ï¼‰ï¼š`doc_stride=128~256`

ä¸€å¥è¯æ€»ç»“ï¼š**`doc_stride=128` æ˜¯ç»éªŒæ€§å‚æ•°ï¼Œé€šè¿‡åˆ†å—é‡å å¹³è¡¡æ•ˆç‡ä¸ç­”æ¡ˆå®Œæ•´æ€§ã€‚**

å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹å‚æ•°ï¼š
- **`max_length = 10`**ï¼ˆæ¯ä¸ªç‰‡æ®µæœ€å¤šåŒ…å«10ä¸ªå­—ç¬¦ï¼‰
- **`doc_stride = 4`**ï¼ˆç›¸é‚»ç‰‡æ®µé‡å 4ä¸ªå­—ç¬¦ï¼‰

---

### **åˆ‡å‰²è¿‡ç¨‹**
åŸå§‹æ–‡æœ¬ï¼š`ABCDEFGHIJKLMN`ï¼ˆå‡è®¾æ¯ä¸ªå­—æ¯ä»£è¡¨ä¸€ä¸ªtokenï¼‰

1. **ç¬¬ä¸€ä¸ªç‰‡æ®µ**ï¼š  
   - å–å‰10ä¸ªå­—ç¬¦ â†’ `ABCDEFGHIJ`ï¼ˆAåˆ°Jï¼‰
   - ç»“æŸä½ç½®ï¼šç¬¬10ä¸ªå­—ç¬¦ï¼ˆJï¼‰

2. **ç¬¬äºŒä¸ªç‰‡æ®µ**ï¼š  
   - èµ·å§‹ä½ç½® = å‰ç‰‡æ®µçš„èµ·å§‹ä½ç½® + (`max_length - doc_stride`) = 0 + (10 - 4) = 6  
     ï¼ˆå³ä»ç¬¬7ä¸ªå­—ç¬¦å¼€å§‹ï¼Œå¯¹åº”å­—æ¯ `G`ï¼‰
   - å®é™…å­—ç¬¦ï¼š`GHIJKLMN`ï¼ˆGåˆ°Nï¼Œå…±8ä¸ªå­—ç¬¦ï¼Œä¸è¶³10ä¸ªåˆ™ä¿ç•™ï¼‰
   - é‡å éƒ¨åˆ†ï¼š`GHIJ`ï¼ˆä¸å‰ä¸€ç‰‡æ®µçš„å4ä¸ªå­—ç¬¦é‡å ï¼‰

---

### **å›¾ç¤ºåˆ‡å‰²æ•ˆæœ**
```
åŸå§‹æ–‡æœ¬ï¼š A B C D E F G H I J K L M N
ç‰‡æ®µ1ï¼š    [A B C D E F G H I J]          â†’ é•¿åº¦10
ç‰‡æ®µ2ï¼š            [G H I J K L M N]      â†’ èµ·å§‹ä½ç½®6ï¼Œé‡å 4ä¸ªå­—ç¬¦
```

---

### **ä¸ºä»€ä¹ˆéœ€è¦é‡å ï¼Ÿ**
å‡è®¾ç­”æ¡ˆåœ¨ `H I J K` åŒºåŸŸï¼š
- **æ— é‡å **ï¼šå¯èƒ½è¢«æˆªæ–­åœ¨ç‰‡æ®µ1æœ«å°¾æˆ–ç‰‡æ®µ2å¼€å¤´
- **æœ‰é‡å **ï¼šç¡®ä¿ç­”æ¡ˆå®Œæ•´åŒ…å«åœ¨è‡³å°‘ä¸€ä¸ªç‰‡æ®µä¸­

---

### **å®é™…é—®ç­”ä¸­çš„å‚æ•°**
å½“ `max_length=384` ä¸” `doc_stride=128` æ—¶ï¼Œé€»è¾‘å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯æ•°å€¼æ›´å¤§ã€‚è¿™ç§æ»‘åŠ¨çª—å£åˆ‡å‰²æ˜¯å¤„ç†é•¿æ–‡æœ¬é—®ç­”çš„å¸¸ç”¨ç­–ç•¥ï¼ ğŸ˜Š

#### è¶…å‡ºæœ€å¤§é•¿åº¦çš„æ–‡æœ¬æ•°æ®å¤„ç†

ä¸‹é¢ï¼Œæˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­æ‰¾å‡ºä¸€ä¸ªè¶…è¿‡æœ€å¤§é•¿åº¦ï¼ˆ384ï¼‰çš„æ–‡æœ¬ï¼š


```python
for i, example in enumerate(datasets["train"]):
    if len(tokenizer(example["question"], example["context"])["input_ids"]) > 384:
        break
# æŒ‘é€‰å‡ºæ¥è¶…è¿‡384ï¼ˆæœ€å¤§é•¿åº¦ï¼‰çš„æ•°æ®æ ·ä¾‹
example = datasets["train"][i]
```


```python
len(tokenizer(example["question"], example["context"])["input_ids"])
```




    396



#### æˆªæ–­ä¸Šä¸‹æ–‡ä¸ä¿ç•™è¶…å‡ºéƒ¨åˆ†


```python
len(tokenizer(example["question"],
              example["context"],
              max_length=max_length,
              truncation="only_second")["input_ids"])
```




    384



#### å…³äºæˆªæ–­çš„ç­–ç•¥

- ç›´æ¥æˆªæ–­è¶…å‡ºéƒ¨åˆ†: truncation=`only_second`
- ä»…æˆªæ–­ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ï¼Œä¿ç•™é—®é¢˜ï¼ˆquestionï¼‰ï¼š`return_overflowing_tokens=True` & è®¾ç½®`stride`



```python
tokenized_example = tokenizer(
    example["question"],
    example["context"],
    max_length=max_length,
    truncation="only_second",
    return_overflowing_tokens=True,
    stride=doc_stride
)
```

ä½¿ç”¨æ­¤ç­–ç•¥æˆªæ–­åï¼ŒTokenizer å°†è¿”å›å¤šä¸ª `input_ids` åˆ—è¡¨ã€‚


```python
[len(x) for x in tokenized_example["input_ids"]]
```




    [384, 157]



è§£ç ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ï¼Œå¯ä»¥çœ‹åˆ°é‡å çš„éƒ¨åˆ†ï¼š


```python
for x in tokenized_example["input_ids"][:2]:
    print(tokenizer.decode(x))
```

    [CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 â€“ 2010 season. the team is coached by mike brey, who, as of the 2014 â€“ 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]
    [CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]


#### ä½¿ç”¨ offsets_mapping è·å–åŸå§‹çš„ input_ids

è®¾ç½® `return_offsets_mapping=True`ï¼Œå°†ä½¿å¾—æˆªæ–­åˆ†å‰²ç”Ÿæˆçš„å¤šä¸ª input_ids åˆ—è¡¨ä¸­çš„ tokenï¼Œé€šè¿‡æ˜ å°„ä¿ç•™åŸå§‹æ–‡æœ¬çš„ input_idsã€‚

å¦‚ä¸‹æ‰€ç¤ºï¼šç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆ[CLS]ï¼‰çš„èµ·å§‹å’Œç»“æŸå­—ç¬¦éƒ½æ˜¯ï¼ˆ0, 0ï¼‰ï¼Œå› ä¸ºå®ƒä¸å¯¹åº”é—®é¢˜/ç­”æ¡ˆçš„ä»»ä½•éƒ¨åˆ†ï¼Œç„¶åç¬¬äºŒä¸ªæ ‡è®°ä¸é—®é¢˜(question)çš„å­—ç¬¦0åˆ°3ç›¸åŒ.


```python
tokenized_example = tokenizer(
    example["question"],            # ç¬¬ä¸€ä¸ªå‚æ•°ï¼šé—®é¢˜æ–‡æœ¬
    example["context"],             # ç¬¬äºŒä¸ªå‚æ•°ï¼šä¸Šä¸‹æ–‡æ–‡æœ¬
    max_length=max_length,          # æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚384ï¼‰
    truncation="only_second",       # å…³é”®å‚æ•°1ï¼šæˆªæ–­ç­–ç•¥
    return_overflowing_tokens=True, # å…³é”®å‚æ•°2ï¼šè¿”å›åˆ†å—ç»“æœ
    return_offsets_mapping=True,    # å…³é”®å‚æ•°3ï¼šè¿”å›å­—ç¬¦çº§ä½ç½®æ˜ å°„
    return_token_type_ids=True,     # æ˜¾å¼è¦æ±‚è¿”å› token_type_ids
    stride=doc_stride               # åˆ†å—æ»‘åŠ¨æ­¥é•¿ï¼ˆå¦‚128ï¼‰
)
print(tokenized_example["offset_mapping"][0][:100])
```

    [(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]


---

### **å‚æ•°è¯¦è§£**
#### 1. `truncation="only_second"`
- **ä½œç”¨**ï¼š**åªæˆªæ–­ç¬¬äºŒä¸ªå‚æ•°ï¼ˆä¸Šä¸‹æ–‡ï¼‰**ï¼Œä¿æŒç¬¬ä¸€ä¸ªå‚æ•°ï¼ˆé—®é¢˜ï¼‰å®Œæ•´
- **åœºæ™¯**ï¼šå½“ `é—®é¢˜+ä¸Šä¸‹æ–‡` æ€»é•¿åº¦è¶…è¿‡ `max_length` æ—¶ï¼Œä¼˜å…ˆä¿ç•™é—®é¢˜å®Œæ•´æ€§
- **ç¤ºä¾‹**ï¼š
  ```python
  # è¾“å…¥ï¼šé—®é¢˜é•¿åº¦20ï¼Œä¸Šä¸‹æ–‡é•¿åº¦400 â†’ æ€»é•¿åº¦420 > 384
  # å¤„ç†ï¼šæˆªæ–­ä¸Šä¸‹æ–‡ä¸º 384-20-3ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰= 361 tokens
  ```

#### 2. `return_overflowing_tokens=True`
- **ä½œç”¨**ï¼š**è¿”å›åˆ†å—åçš„å¤šä¸ªè¾“å…¥ç‰¹å¾**ï¼ˆå½“è¾“å…¥è¿‡é•¿æ—¶è‡ªåŠ¨åˆ†å‰²ï¼‰
- **è¾“å‡ºå­—æ®µ**ï¼š`overflow_to_sample_mapping`ï¼ˆåˆ†å—å¯¹åº”åŸå§‹æ ·æœ¬çš„ç´¢å¼•ï¼‰
- **åˆ†å—é€»è¾‘**ï¼š
  - å°†é•¿ä¸Šä¸‹æ–‡æŒ‰ `max_length - é—®é¢˜é•¿åº¦` åˆ‡å‰²
  - ç›¸é‚»åˆ†å—é‡å  `stride` tokensï¼ˆç¡®ä¿ç­”æ¡ˆä¸è¢«åˆ‡å‰²ï¼‰

#### 3. `return_offsets_mapping=True`
- **ä½œç”¨**ï¼š**è¿”å›æ¯ä¸ª token åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®**ï¼ˆèµ·å§‹å’Œç»“æŸç´¢å¼•ï¼‰
- **è¾“å‡ºå­—æ®µ**ï¼š`offset_mapping`ï¼ˆåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ `(start, end)` å…ƒç»„ï¼‰
- **å…³é”®ç”¨é€”**ï¼šå°†æ¨¡å‹é¢„æµ‹çš„ token ä½ç½®æ˜ å°„å›åŸå§‹æ–‡æœ¬ï¼ˆå¦‚å®šä½ç­”æ¡ˆï¼‰

---

### **`offset_mapping` ç¤ºä¾‹è§£æ**
```python
# å‡è®¾æ‰“å°ç»“æœå‰5ä¸ªå…ƒç´ ï¼š
[(0, 0), (0, 3), (4, 7), (8, 11), (12, 15), ...]

# å¯¹åº”å«ä¹‰ï¼š
# [CLS]  What    is    your   name?  [SEP] ...
# (0,0) (0,3) (4,7) (8,11) (12,15)   ...
```
- **ç‰¹æ®Šæ ‡è®°**ï¼š`[CLS]`ã€`[SEP]` ç­‰æ— å¯¹åº”æ–‡æœ¬ â†’ `(0, 0)`
- **é—®é¢˜éƒ¨åˆ†**ï¼šå­—ç¬¦ç´¢å¼•ä»é—®é¢˜æ–‡æœ¬çš„èµ·å§‹ä½ç½®è®¡ç®—
- **ä¸Šä¸‹æ–‡éƒ¨åˆ†**ï¼šå­—ç¬¦ç´¢å¼•ä»ä¸Šä¸‹æ–‡æ–‡æœ¬çš„èµ·å§‹ä½ç½®è®¡ç®—ï¼ˆéœ€æ³¨æ„é—®é¢˜æ–‡æœ¬é•¿åº¦ï¼‰

---

### **å‚æ•°ååŒä½œç”¨**
| å‚æ•°ç»„åˆ                        | å®é™…æ•ˆæœ                                                                 |
|---------------------------------|------------------------------------------------------------------------|
| `truncation="only_second"` + `return_overflowing_tokens=True` | å°†é•¿ä¸Šä¸‹æ–‡åˆ‡å‰²ä¸ºå¤šä¸ªåˆ†å—ï¼Œæ¯ä¸ªåˆ†å—åŒ…å«å®Œæ•´é—®é¢˜å’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡ |
| `return_offsets_mapping=True`   | æä¾›åˆ†å—ä¸­æ¯ä¸ª token åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„ä½ç½®ï¼Œç”¨äºç­”æ¡ˆä½ç½®æ˜ å°„               |

---

### **åº”ç”¨åœºæ™¯**
1. **è®­ç»ƒé˜¶æ®µ**ï¼šå°†ç­”æ¡ˆçš„å­—ç¬¦ä½ç½®è½¬æ¢ä¸ºåˆ†å—å†…çš„ token ä½ç½®
2. **æ¨ç†é˜¶æ®µ**ï¼šå°†æ¨¡å‹é¢„æµ‹çš„ token ä½ç½®åå‘æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡
3. **æ•°æ®éªŒè¯**ï¼šæ£€æŸ¥åˆ†å—æ˜¯å¦è¦†ç›–æ­£ç¡®ç­”æ¡ˆçš„åŸå§‹ä½ç½®

---

é€šè¿‡è¿™ä¸‰ä¸ªå‚æ•°ï¼Œå®ç°äº†é•¿æ–‡æœ¬é—®ç­”ä»»åŠ¡ä¸­ **è¾“å…¥åˆ†å—å¤„ç†** å’Œ **ä½ç½®ç²¾ç¡®æ˜ å°„** çš„æ ¸å¿ƒéœ€æ±‚ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ˜ å°„æ¥æ‰¾åˆ°ç­”æ¡ˆåœ¨ç»™å®šç‰¹å¾ä¸­çš„èµ·å§‹å’Œç»“æŸæ ‡è®°çš„ä½ç½®ã€‚

æˆ‘ä»¬åªéœ€åŒºåˆ†åç§»çš„å“ªäº›éƒ¨åˆ†å¯¹åº”äºé—®é¢˜ï¼Œå“ªäº›éƒ¨åˆ†å¯¹åº”äºä¸Šä¸‹æ–‡ã€‚


```python
first_token_id = tokenized_example["input_ids"][0][1]
offsets = tokenized_example["offset_mapping"][0][1]
print(first_token_id)
print(offsets)
print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example["question"][offsets[0]:offsets[1]])
```

    2129
    (0, 3)
    how How



```python
second_token_id = tokenized_example["input_ids"][0][2]
offsets = tokenized_example["offset_mapping"][0][2]
print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example["question"][offsets[0]:offsets[1]])
```

    many many



```python
first_token_id = tokenized_example["input_ids"][0][7]
offsets = tokenized_example["offset_mapping"][0][7]
print(first_token_id)
print(offsets)
print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example["question"][offsets[0]:offsets[1]])
```

    8214
    (29, 33)
    dame Dame



```python
first_token_id = tokenized_example["input_ids"][0][10]
offsets = tokenized_example["offset_mapping"][0][10]
print(first_token_id)
print(offsets)
print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example["context"][offsets[0]:offsets[1]])
```

    1055
    (38, 39)
    s 0



```python
# éå†æ¯ä¸ªåˆ†å—
for chunk_idx in range(len(tokenized_example["input_ids"])):
    print(f"\n=== åˆ†å— {chunk_idx} ===")
    
    # è·å–å½“å‰åˆ†å—çš„æ•°æ®
    input_ids = tokenized_example["input_ids"][chunk_idx]
    offset_mapping = tokenized_example["offset_mapping"][chunk_idx]
    token_type_ids = tokenized_example["token_type_ids"][chunk_idx]

    # éå†åˆ†å—å†…çš„æ¯ä¸ª token
    for token_idx, (token_id, offset, token_type) in enumerate(zip(input_ids, offset_mapping, token_type_ids)):
        # æ ¹æ® token_type é€‰æ‹©æ¥æºæ–‡æœ¬
        if token_type == 0:
            source_text = example["question"]
        else:
            source_text = example["context"]

        # å…³é”®ä¿®å¤ç‚¹ï¼šåˆ†è§£ offset å…ƒç»„ä¸º start å’Œ end
        start = offset[0]  # èµ·å§‹å­—ç¬¦ä½ç½®
        end = offset[1]    # ç»“æŸå­—ç¬¦ä½ç½®
        print(start, end)
        original_text = source_text[start:end]
        
        # è½¬æ¢ token_id ä¸ºå¯è¯»æ–‡æœ¬
        token_str = tokenizer.convert_ids_to_tokens([token_id])[0]  # å–åˆ—è¡¨ç¬¬ä¸€ä¸ªå…ƒç´ 
        
        # æ‰“å°ç»“æœ
        print(f"Token {token_idx}: {token_str} â†’ {original_text}")
```

    
    === åˆ†å— 0 ===
    0 0
    Token 0: [CLS] â†’ 
    0 3
    Token 1: how â†’ How
    4 8
    Token 2: many â†’ many
    9 13
    Token 3: wins â†’ wins
    14 18
    Token 4: does â†’ does
    19 22
    Token 5: the â†’ the
    23 28
    Token 6: notre â†’ Notre
    29 33
    Token 7: dame â†’ Dame
    34 37
    Token 8: men â†’ men
    37 38
    Token 9: ' â†’ '
    38 39
    Token 10: s â†’ s
    40 50
    Token 11: basketball â†’ basketball
    51 55
    Token 12: team â†’ team
    56 60
    Token 13: have â†’ have
    60 61
    Token 14: ? â†’ ?
    0 0
    Token 15: [SEP] â†’ 
    0 3
    Token 16: the â†’ The
    4 7
    Token 17: men â†’ men
    7 8
    Token 18: ' â†’ '
    8 9
    Token 19: s â†’ s
    10 20
    Token 20: basketball â†’ basketball
    21 25
    Token 21: team â†’ team
    26 29
    Token 22: has â†’ has
    30 34
    Token 23: over â†’ over
    35 36
    Token 24: 1 â†’ 1
    36 37
    Token 25: , â†’ ,
    37 40
    Token 26: 600 â†’ 600
    41 45
    Token 27: wins â†’ wins
    45 46
    Token 28: , â†’ ,
    47 50
    Token 29: one â†’ one
    51 53
    Token 30: of â†’ of
    54 58
    Token 31: only â†’ only
    59 61
    Token 32: 12 â†’ 12
    62 69
    Token 33: schools â†’ schools
    70 73
    Token 34: who â†’ who
    74 78
    Token 35: have â†’ have
    79 86
    Token 36: reached â†’ reached
    87 91
    Token 37: that â†’ that
    92 96
    Token 38: mark â†’ mark
    96 97
    Token 39: , â†’ ,
    98 101
    Token 40: and â†’ and
    102 106
    Token 41: have â†’ have
    107 115
    Token 42: appeared â†’ appeared
    116 118
    Token 43: in â†’ in
    119 121
    Token 44: 28 â†’ 28
    122 126
    Token 45: ncaa â†’ NCAA
    127 138
    Token 46: tournaments â†’ tournaments
    138 139
    Token 47: . â†’ .
    140 146
    Token 48: former â†’ Former
    147 153
    Token 49: player â†’ player
    154 160
    Token 50: austin â†’ Austin
    161 165
    Token 51: carr â†’ Carr
    166 171
    Token 52: holds â†’ holds
    172 175
    Token 53: the â†’ the
    176 182
    Token 54: record â†’ record
    183 186
    Token 55: for â†’ for
    187 191
    Token 56: most â†’ most
    192 198
    Token 57: points â†’ points
    199 205
    Token 58: scored â†’ scored
    206 208
    Token 59: in â†’ in
    209 210
    Token 60: a â†’ a
    211 217
    Token 61: single â†’ single
    218 222
    Token 62: game â†’ game
    223 225
    Token 63: of â†’ of
    226 229
    Token 64: the â†’ the
    230 240
    Token 65: tournament â†’ tournament
    241 245
    Token 66: with â†’ with
    246 248
    Token 67: 61 â†’ 61
    248 249
    Token 68: . â†’ .
    250 258
    Token 69: although â†’ Although
    259 262
    Token 70: the â†’ the
    263 267
    Token 71: team â†’ team
    268 271
    Token 72: has â†’ has
    272 277
    Token 73: never â†’ never
    278 281
    Token 74: won â†’ won
    282 285
    Token 75: the â†’ the
    286 290
    Token 76: ncaa â†’ NCAA
    291 301
    Token 77: tournament â†’ Tournament
    301 302
    Token 78: , â†’ ,
    303 307
    Token 79: they â†’ they
    308 312
    Token 80: were â†’ were
    313 318
    Token 81: named â†’ named
    319 321
    Token 82: by â†’ by
    322 325
    Token 83: the â†’ the
    326 330
    Token 84: helm â†’ Helm
    330 331
    Token 85: ##s â†’ s
    332 340
    Token 86: athletic â†’ Athletic
    341 351
    Token 87: foundation â†’ Foundation
    352 354
    Token 88: as â†’ as
    355 363
    Token 89: national â†’ national
    364 373
    Token 90: champions â†’ champions
    374 379
    Token 91: twice â†’ twice
    379 380
    Token 92: . â†’ .
    381 384
    Token 93: the â†’ The
    385 389
    Token 94: team â†’ team
    390 393
    Token 95: has â†’ has
    394 406
    Token 96: orchestrated â†’ orchestrated
    407 408
    Token 97: a â†’ a
    409 415
    Token 98: number â†’ number
    416 418
    Token 99: of â†’ of
    419 424
    Token 100: upset â†’ upset
    424 425
    Token 101: ##s â†’ s
    426 428
    Token 102: of â†’ of
    429 435
    Token 103: number â†’ number
    436 439
    Token 104: one â†’ one
    440 446
    Token 105: ranked â†’ ranked
    447 452
    Token 106: teams â†’ teams
    452 453
    Token 107: , â†’ ,
    454 457
    Token 108: the â†’ the
    458 462
    Token 109: most â†’ most
    463 470
    Token 110: notable â†’ notable
    471 473
    Token 111: of â†’ of
    474 479
    Token 112: which â†’ which
    480 483
    Token 113: was â†’ was
    484 490
    Token 114: ending â†’ ending
    491 495
    Token 115: ucla â†’ UCLA
    495 496
    Token 116: ' â†’ '
    496 497
    Token 117: s â†’ s
    498 504
    Token 118: record â†’ record
    505 507
    Token 119: 88 â†’ 88
    507 508
    Token 120: - â†’ -
    508 512
    Token 121: game â†’ game
    513 520
    Token 122: winning â†’ winning
    521 527
    Token 123: streak â†’ streak
    528 530
    Token 124: in â†’ in
    531 535
    Token 125: 1974 â†’ 1974
    535 536
    Token 126: . â†’ .
    537 540
    Token 127: the â†’ The
    541 545
    Token 128: team â†’ team
    546 549
    Token 129: has â†’ has
    550 556
    Token 130: beaten â†’ beaten
    557 559
    Token 131: an â†’ an
    560 570
    Token 132: additional â†’ additional
    571 576
    Token 133: eight â†’ eight
    577 583
    Token 134: number â†’ number
    583 584
    Token 135: - â†’ -
    584 587
    Token 136: one â†’ one
    588 593
    Token 137: teams â†’ teams
    593 594
    Token 138: , â†’ ,
    595 598
    Token 139: and â†’ and
    599 604
    Token 140: those â†’ those
    605 609
    Token 141: nine â†’ nine
    610 614
    Token 142: wins â†’ wins
    615 619
    Token 143: rank â†’ rank
    620 626
    Token 144: second â†’ second
    626 627
    Token 145: , â†’ ,
    628 630
    Token 146: to â†’ to
    631 635
    Token 147: ucla â†’ UCLA
    635 636
    Token 148: ' â†’ '
    636 637
    Token 149: s â†’ s
    638 640
    Token 150: 10 â†’ 10
    640 641
    Token 151: , â†’ ,
    642 645
    Token 152: all â†’ all
    645 646
    Token 153: - â†’ -
    646 650
    Token 154: time â†’ time
    651 653
    Token 155: in â†’ in
    654 658
    Token 156: wins â†’ wins
    659 666
    Token 157: against â†’ against
    667 670
    Token 158: the â†’ the
    671 674
    Token 159: top â†’ top
    675 679
    Token 160: team â†’ team
    679 680
    Token 161: . â†’ .
    681 684
    Token 162: the â†’ The
    685 689
    Token 163: team â†’ team
    690 695
    Token 164: plays â†’ plays
    696 698
    Token 165: in â†’ in
    699 704
    Token 166: newly â†’ newly
    705 714
    Token 167: renovated â†’ renovated
    715 722
    Token 168: purcell â†’ Purcell
    723 731
    Token 169: pavilion â†’ Pavilion
    732 733
    Token 170: ( â†’ (
    733 739
    Token 171: within â†’ within
    740 743
    Token 172: the â†’ the
    744 750
    Token 173: edmund â†’ Edmund
    751 752
    Token 174: p â†’ P
    752 753
    Token 175: . â†’ .
    754 759
    Token 176: joyce â†’ Joyce
    760 766
    Token 177: center â†’ Center
    766 767
    Token 178: ) â†’ )
    767 768
    Token 179: , â†’ ,
    769 774
    Token 180: which â†’ which
    775 783
    Token 181: reopened â†’ reopened
    784 787
    Token 182: for â†’ for
    788 791
    Token 183: the â†’ the
    792 801
    Token 184: beginning â†’ beginning
    802 804
    Token 185: of â†’ of
    805 808
    Token 186: the â†’ the
    809 813
    Token 187: 2009 â†’ 2009
    813 814
    Token 188: â€“ â†’ â€“
    814 818
    Token 189: 2010 â†’ 2010
    819 825
    Token 190: season â†’ season
    825 826
    Token 191: . â†’ .
    827 830
    Token 192: the â†’ The
    831 835
    Token 193: team â†’ team
    836 838
    Token 194: is â†’ is
    839 846
    Token 195: coached â†’ coached
    847 849
    Token 196: by â†’ by
    850 854
    Token 197: mike â†’ Mike
    855 857
    Token 198: br â†’ Br
    857 859
    Token 199: ##ey â†’ ey
    859 860
    Token 200: , â†’ ,
    861 864
    Token 201: who â†’ who
    864 865
    Token 202: , â†’ ,
    866 868
    Token 203: as â†’ as
    869 871
    Token 204: of â†’ of
    872 875
    Token 205: the â†’ the
    876 880
    Token 206: 2014 â†’ 2014
    880 881
    Token 207: â€“ â†’ â€“
    881 883
    Token 208: 15 â†’ 15
    884 890
    Token 209: season â†’ season
    890 891
    Token 210: , â†’ ,
    892 895
    Token 211: his â†’ his
    896 905
    Token 212: fifteenth â†’ fifteenth
    906 908
    Token 213: at â†’ at
    909 914
    Token 214: notre â†’ Notre
    915 919
    Token 215: dame â†’ Dame
    919 920
    Token 216: , â†’ ,
    921 924
    Token 217: has â†’ has
    925 933
    Token 218: achieved â†’ achieved
    934 935
    Token 219: a â†’ a
    936 939
    Token 220: 332 â†’ 332
    939 940
    Token 221: - â†’ -
    940 943
    Token 222: 165 â†’ 165
    944 950
    Token 223: record â†’ record
    950 951
    Token 224: . â†’ .
    952 954
    Token 225: in â†’ In
    955 959
    Token 226: 2009 â†’ 2009
    960 964
    Token 227: they â†’ they
    965 969
    Token 228: were â†’ were
    970 977
    Token 229: invited â†’ invited
    978 980
    Token 230: to â†’ to
    981 984
    Token 231: the â†’ the
    985 987
    Token 232: ni â†’ NI
    987 988
    Token 233: ##t â†’ T
    988 989
    Token 234: , â†’ ,
    990 995
    Token 235: where â†’ where
    996 1000
    Token 236: they â†’ they
    1001 1009
    Token 237: advanced â†’ advanced
    1010 1012
    Token 238: to â†’ to
    1013 1016
    Token 239: the â†’ the
    1017 1027
    Token 240: semifinals â†’ semifinals
    1028 1031
    Token 241: but â†’ but
    1032 1036
    Token 242: were â†’ were
    1037 1043
    Token 243: beaten â†’ beaten
    1044 1046
    Token 244: by â†’ by
    1047 1051
    Token 245: penn â†’ Penn
    1052 1057
    Token 246: state â†’ State
    1058 1061
    Token 247: who â†’ who
    1062 1066
    Token 248: went â†’ went
    1067 1069
    Token 249: on â†’ on
    1070 1073
    Token 250: and â†’ and
    1074 1078
    Token 251: beat â†’ beat
    1079 1085
    Token 252: baylor â†’ Baylor
    1086 1088
    Token 253: in â†’ in
    1089 1092
    Token 254: the â†’ the
    1093 1105
    Token 255: championship â†’ championship
    1105 1106
    Token 256: . â†’ .
    1107 1110
    Token 257: the â†’ The
    1111 1115
    Token 258: 2010 â†’ 2010
    1115 1116
    Token 259: â€“ â†’ â€“
    1116 1118
    Token 260: 11 â†’ 11
    1119 1123
    Token 261: team â†’ team
    1124 1133
    Token 262: concluded â†’ concluded
    1134 1137
    Token 263: its â†’ its
    1138 1145
    Token 264: regular â†’ regular
    1146 1152
    Token 265: season â†’ season
    1153 1159
    Token 266: ranked â†’ ranked
    1160 1166
    Token 267: number â†’ number
    1167 1172
    Token 268: seven â†’ seven
    1173 1175
    Token 269: in â†’ in
    1176 1179
    Token 270: the â†’ the
    1180 1187
    Token 271: country â†’ country
    1187 1188
    Token 272: , â†’ ,
    1189 1193
    Token 273: with â†’ with
    1194 1195
    Token 274: a â†’ a
    1196 1202
    Token 275: record â†’ record
    1203 1205
    Token 276: of â†’ of
    1206 1208
    Token 277: 25 â†’ 25
    1208 1209
    Token 278: â€“ â†’ â€“
    1209 1210
    Token 279: 5 â†’ 5
    1210 1211
    Token 280: , â†’ ,
    1212 1214
    Token 281: br â†’ Br
    1214 1216
    Token 282: ##ey â†’ ey
    1216 1217
    Token 283: ' â†’ '
    1217 1218
    Token 284: s â†’ s
    1219 1224
    Token 285: fifth â†’ fifth
    1225 1233
    Token 286: straight â†’ straight
    1234 1236
    Token 287: 20 â†’ 20
    1236 1237
    Token 288: - â†’ -
    1237 1240
    Token 289: win â†’ win
    1241 1247
    Token 290: season â†’ season
    1247 1248
    Token 291: , â†’ ,
    1249 1252
    Token 292: and â†’ and
    1253 1254
    Token 293: a â†’ a
    1255 1261
    Token 294: second â†’ second
    1261 1262
    Token 295: - â†’ -
    1262 1267
    Token 296: place â†’ place
    1268 1274
    Token 297: finish â†’ finish
    1275 1277
    Token 298: in â†’ in
    1278 1281
    Token 299: the â†’ the
    1282 1285
    Token 300: big â†’ Big
    1286 1290
    Token 301: east â†’ East
    1290 1291
    Token 302: . â†’ .
    1292 1298
    Token 303: during â†’ During
    1299 1302
    Token 304: the â†’ the
    1303 1307
    Token 305: 2014 â†’ 2014
    1307 1308
    Token 306: - â†’ -
    1308 1310
    Token 307: 15 â†’ 15
    1311 1317
    Token 308: season â†’ season
    1317 1318
    Token 309: , â†’ ,
    1319 1322
    Token 310: the â†’ the
    1323 1327
    Token 311: team â†’ team
    1328 1332
    Token 312: went â†’ went
    1333 1335
    Token 313: 32 â†’ 32
    1335 1336
    Token 314: - â†’ -
    1336 1337
    Token 315: 6 â†’ 6
    1338 1341
    Token 316: and â†’ and
    1342 1345
    Token 317: won â†’ won
    1346 1349
    Token 318: the â†’ the
    1350 1353
    Token 319: acc â†’ ACC
    1354 1364
    Token 320: conference â†’ conference
    1365 1375
    Token 321: tournament â†’ tournament
    1375 1376
    Token 322: , â†’ ,
    1377 1382
    Token 323: later â†’ later
    1383 1392
    Token 324: advancing â†’ advancing
    1393 1395
    Token 325: to â†’ to
    1396 1399
    Token 326: the â†’ the
    1400 1405
    Token 327: elite â†’ Elite
    1406 1407
    Token 328: 8 â†’ 8
    1407 1408
    Token 329: , â†’ ,
    1409 1414
    Token 330: where â†’ where
    1415 1418
    Token 331: the â†’ the
    1419 1427
    Token 332: fighting â†’ Fighting
    1428 1433
    Token 333: irish â†’ Irish
    1434 1438
    Token 334: lost â†’ lost
    1439 1441
    Token 335: on â†’ on
    1442 1443
    Token 336: a â†’ a
    1444 1450
    Token 337: missed â†’ missed
    1451 1455
    Token 338: buzz â†’ buzz
    1455 1457
    Token 339: ##er â†’ er
    1457 1458
    Token 340: - â†’ -
    1458 1462
    Token 341: beat â†’ beat
    1462 1464
    Token 342: ##er â†’ er
    1465 1472
    Token 343: against â†’ against
    1473 1477
    Token 344: then â†’ then
    1478 1488
    Token 345: undefeated â†’ undefeated
    1489 1497
    Token 346: kentucky â†’ Kentucky
    1497 1498
    Token 347: . â†’ .
    1499 1502
    Token 348: led â†’ Led
    1503 1505
    Token 349: by â†’ by
    1506 1509
    Token 350: nba â†’ NBA
    1510 1515
    Token 351: draft â†’ draft
    1516 1521
    Token 352: picks â†’ picks
    1522 1524
    Token 353: je â†’ Je
    1524 1528
    Token 354: ##rian â†’ rian
    1529 1534
    Token 355: grant â†’ Grant
    1535 1538
    Token 356: and â†’ and
    1539 1542
    Token 357: pat â†’ Pat
    1543 1546
    Token 358: con â†’ Con
    1546 1548
    Token 359: ##na â†’ na
    1548 1552
    Token 360: ##ught â†’ ught
    1552 1554
    Token 361: ##on â†’ on
    1554 1555
    Token 362: , â†’ ,
    1556 1559
    Token 363: the â†’ the
    1560 1568
    Token 364: fighting â†’ Fighting
    1569 1574
    Token 365: irish â†’ Irish
    1575 1579
    Token 366: beat â†’ beat
    1580 1583
    Token 367: the â†’ the
    1584 1592
    Token 368: eventual â†’ eventual
    1593 1601
    Token 369: national â†’ national
    1602 1610
    Token 370: champion â†’ champion
    1611 1615
    Token 371: duke â†’ Duke
    1616 1620
    Token 372: blue â†’ Blue
    1621 1627
    Token 373: devils â†’ Devils
    1628 1633
    Token 374: twice â†’ twice
    1634 1640
    Token 375: during â†’ during
    1641 1644
    Token 376: the â†’ the
    1645 1651
    Token 377: season â†’ season
    1651 1652
    Token 378: . â†’ .
    1653 1656
    Token 379: the â†’ The
    1657 1659
    Token 380: 32 â†’ 32
    1660 1664
    Token 381: wins â†’ wins
    1665 1669
    Token 382: were â†’ were
    0 0
    Token 383: [SEP] â†’ 
    
    === åˆ†å— 1 ===
    0 0
    Token 0: [CLS] â†’ 
    0 3
    Token 1: how â†’ How
    4 8
    Token 2: many â†’ many
    9 13
    Token 3: wins â†’ wins
    14 18
    Token 4: does â†’ does
    19 22
    Token 5: the â†’ the
    23 28
    Token 6: notre â†’ Notre
    29 33
    Token 7: dame â†’ Dame
    34 37
    Token 8: men â†’ men
    37 38
    Token 9: ' â†’ '
    38 39
    Token 10: s â†’ s
    40 50
    Token 11: basketball â†’ basketball
    51 55
    Token 12: team â†’ team
    56 60
    Token 13: have â†’ have
    60 61
    Token 14: ? â†’ ?
    0 0
    Token 15: [SEP] â†’ 
    1093 1105
    Token 16: championship â†’ championship
    1105 1106
    Token 17: . â†’ .
    1107 1110
    Token 18: the â†’ The
    1111 1115
    Token 19: 2010 â†’ 2010
    1115 1116
    Token 20: â€“ â†’ â€“
    1116 1118
    Token 21: 11 â†’ 11
    1119 1123
    Token 22: team â†’ team
    1124 1133
    Token 23: concluded â†’ concluded
    1134 1137
    Token 24: its â†’ its
    1138 1145
    Token 25: regular â†’ regular
    1146 1152
    Token 26: season â†’ season
    1153 1159
    Token 27: ranked â†’ ranked
    1160 1166
    Token 28: number â†’ number
    1167 1172
    Token 29: seven â†’ seven
    1173 1175
    Token 30: in â†’ in
    1176 1179
    Token 31: the â†’ the
    1180 1187
    Token 32: country â†’ country
    1187 1188
    Token 33: , â†’ ,
    1189 1193
    Token 34: with â†’ with
    1194 1195
    Token 35: a â†’ a
    1196 1202
    Token 36: record â†’ record
    1203 1205
    Token 37: of â†’ of
    1206 1208
    Token 38: 25 â†’ 25
    1208 1209
    Token 39: â€“ â†’ â€“
    1209 1210
    Token 40: 5 â†’ 5
    1210 1211
    Token 41: , â†’ ,
    1212 1214
    Token 42: br â†’ Br
    1214 1216
    Token 43: ##ey â†’ ey
    1216 1217
    Token 44: ' â†’ '
    1217 1218
    Token 45: s â†’ s
    1219 1224
    Token 46: fifth â†’ fifth
    1225 1233
    Token 47: straight â†’ straight
    1234 1236
    Token 48: 20 â†’ 20
    1236 1237
    Token 49: - â†’ -
    1237 1240
    Token 50: win â†’ win
    1241 1247
    Token 51: season â†’ season
    1247 1248
    Token 52: , â†’ ,
    1249 1252
    Token 53: and â†’ and
    1253 1254
    Token 54: a â†’ a
    1255 1261
    Token 55: second â†’ second
    1261 1262
    Token 56: - â†’ -
    1262 1267
    Token 57: place â†’ place
    1268 1274
    Token 58: finish â†’ finish
    1275 1277
    Token 59: in â†’ in
    1278 1281
    Token 60: the â†’ the
    1282 1285
    Token 61: big â†’ Big
    1286 1290
    Token 62: east â†’ East
    1290 1291
    Token 63: . â†’ .
    1292 1298
    Token 64: during â†’ During
    1299 1302
    Token 65: the â†’ the
    1303 1307
    Token 66: 2014 â†’ 2014
    1307 1308
    Token 67: - â†’ -
    1308 1310
    Token 68: 15 â†’ 15
    1311 1317
    Token 69: season â†’ season
    1317 1318
    Token 70: , â†’ ,
    1319 1322
    Token 71: the â†’ the
    1323 1327
    Token 72: team â†’ team
    1328 1332
    Token 73: went â†’ went
    1333 1335
    Token 74: 32 â†’ 32
    1335 1336
    Token 75: - â†’ -
    1336 1337
    Token 76: 6 â†’ 6
    1338 1341
    Token 77: and â†’ and
    1342 1345
    Token 78: won â†’ won
    1346 1349
    Token 79: the â†’ the
    1350 1353
    Token 80: acc â†’ ACC
    1354 1364
    Token 81: conference â†’ conference
    1365 1375
    Token 82: tournament â†’ tournament
    1375 1376
    Token 83: , â†’ ,
    1377 1382
    Token 84: later â†’ later
    1383 1392
    Token 85: advancing â†’ advancing
    1393 1395
    Token 86: to â†’ to
    1396 1399
    Token 87: the â†’ the
    1400 1405
    Token 88: elite â†’ Elite
    1406 1407
    Token 89: 8 â†’ 8
    1407 1408
    Token 90: , â†’ ,
    1409 1414
    Token 91: where â†’ where
    1415 1418
    Token 92: the â†’ the
    1419 1427
    Token 93: fighting â†’ Fighting
    1428 1433
    Token 94: irish â†’ Irish
    1434 1438
    Token 95: lost â†’ lost
    1439 1441
    Token 96: on â†’ on
    1442 1443
    Token 97: a â†’ a
    1444 1450
    Token 98: missed â†’ missed
    1451 1455
    Token 99: buzz â†’ buzz
    1455 1457
    Token 100: ##er â†’ er
    1457 1458
    Token 101: - â†’ -
    1458 1462
    Token 102: beat â†’ beat
    1462 1464
    Token 103: ##er â†’ er
    1465 1472
    Token 104: against â†’ against
    1473 1477
    Token 105: then â†’ then
    1478 1488
    Token 106: undefeated â†’ undefeated
    1489 1497
    Token 107: kentucky â†’ Kentucky
    1497 1498
    Token 108: . â†’ .
    1499 1502
    Token 109: led â†’ Led
    1503 1505
    Token 110: by â†’ by
    1506 1509
    Token 111: nba â†’ NBA
    1510 1515
    Token 112: draft â†’ draft
    1516 1521
    Token 113: picks â†’ picks
    1522 1524
    Token 114: je â†’ Je
    1524 1528
    Token 115: ##rian â†’ rian
    1529 1534
    Token 116: grant â†’ Grant
    1535 1538
    Token 117: and â†’ and
    1539 1542
    Token 118: pat â†’ Pat
    1543 1546
    Token 119: con â†’ Con
    1546 1548
    Token 120: ##na â†’ na
    1548 1552
    Token 121: ##ught â†’ ught
    1552 1554
    Token 122: ##on â†’ on
    1554 1555
    Token 123: , â†’ ,
    1556 1559
    Token 124: the â†’ the
    1560 1568
    Token 125: fighting â†’ Fighting
    1569 1574
    Token 126: irish â†’ Irish
    1575 1579
    Token 127: beat â†’ beat
    1580 1583
    Token 128: the â†’ the
    1584 1592
    Token 129: eventual â†’ eventual
    1593 1601
    Token 130: national â†’ national
    1602 1610
    Token 131: champion â†’ champion
    1611 1615
    Token 132: duke â†’ Duke
    1616 1620
    Token 133: blue â†’ Blue
    1621 1627
    Token 134: devils â†’ Devils
    1628 1633
    Token 135: twice â†’ twice
    1634 1640
    Token 136: during â†’ during
    1641 1644
    Token 137: the â†’ the
    1645 1651
    Token 138: season â†’ season
    1651 1652
    Token 139: . â†’ .
    1653 1656
    Token 140: the â†’ The
    1657 1659
    Token 141: 32 â†’ 32
    1660 1664
    Token 142: wins â†’ wins
    1665 1669
    Token 143: were â†’ were
    1670 1673
    Token 144: the â†’ the
    1674 1678
    Token 145: most â†’ most
    1679 1681
    Token 146: by â†’ by
    1682 1685
    Token 147: the â†’ the
    1686 1694
    Token 148: fighting â†’ Fighting
    1695 1700
    Token 149: irish â†’ Irish
    1701 1705
    Token 150: team â†’ team
    1706 1711
    Token 151: since â†’ since
    1712 1716
    Token 152: 1908 â†’ 1908
    1716 1717
    Token 153: - â†’ -
    1717 1719
    Token 154: 09 â†’ 09
    1719 1720
    Token 155: . â†’ .
    0 0
    Token 156: [SEP] â†’ 


ç”¨æœ€ç®€å•çš„æ¯”å–»è§£é‡Šè¿™æ®µä»£ç ï¼š

**1. åˆ†å—ï¼ˆåˆ‡ä¹¦ï¼‰**  
- å°±åƒä¸€æœ¬åšä¹¦æ‹†æˆå‡ æœ¬å°å†Œå­ï¼Œæ¯æœ¬æœ€å¤š512é¡µï¼ˆæ¨¡å‹ä¸€æ¬¡è¯»ä¸å®Œé•¿æ–‡æœ¬ï¼‰

**2. æ–‡å­—å˜æ•°å­—ï¼ˆåŠ å¯†ï¼‰**  
- æŠŠæ¯ä¸ªå­—å˜æˆæ•°å­—å¯†ç ï¼Œæ¯”å¦‚ "è´"â†’100ï¼Œ"çˆ·"â†’101  
- `input_ids` å°±æ˜¯è¿™äº›å¯†ç ç»„æˆçš„åˆ—è¡¨ï¼š[100, 101, ...]

**3. è®°ä½ç½®ï¼ˆä¹¦ç­¾ï¼‰**  
- `offset_mapping` è®°å½•æ¯ä¸ªå¯†ç åœ¨åŸæ–‡çš„ä½ç½®ï¼Œæ¯”å¦‚ (0,2) è¡¨ç¤ºå‰ä¸¤ä¸ªå­—

**4. åŒºåˆ†é—®é¢˜å’Œç­”æ¡ˆï¼ˆè´´æ ‡ç­¾ï¼‰**  
- `token_type_ids=0` è¡¨ç¤ºæ–‡å­—æ¥è‡ªé—®é¢˜ï¼ˆå¦‚ "è´çˆ·å“ªå¹´ç»“å©šï¼Ÿ"ï¼‰  
- `token_type_ids=1` è¡¨ç¤ºæ–‡å­—æ¥è‡ªç­”æ¡ˆï¼ˆå¦‚ "2000å¹´..."ï¼‰

**5. æ‰¾å¯¹åº”æ–‡å­—ï¼ˆè§£å¯†ï¼‰**  
- ç”¨å¯†ç æœ¬æŠŠæ•°å­—è½¬å›æ–‡å­—  
- æ ¹æ®ä½ç½®æ ‡ç­¾ï¼Œä»é—®é¢˜æˆ–ç­”æ¡ˆæ–‡æœ¬æˆªå–å¯¹åº”æ–‡å­—

**å°±åƒè¿™æ ·ï¼š**  
å¯†ç  `100` â†’ æŸ¥å¯†ç æœ¬ â†’ æ˜¯"è´" â†’ åœ¨é—®é¢˜ç¬¬0-2ä¸ªä½ç½® â†’ æˆªå–"è´çˆ·"

æ•´ä¸ªè¿‡ç¨‹è®©è®¡ç®—æœºåƒäººç±»ä¸€æ ·ï¼šå…ˆçœ‹é—®é¢˜ï¼Œå†å¿«é€Ÿç¿»ä¹¦æ‰¾ç­”æ¡ˆä½ç½®ã€‚


```python
example["question"]
```




    "How many wins does the Notre Dame men's basketball team have?"




```python
example["context"]
```




    "The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009â€“2010 season. The team is coached by Mike Brey, who, as of the 2014â€“15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010â€“11 team concluded its regular season ranked number seven in the country, with a record of 25â€“5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09."



å€ŸåŠ©`tokenized_example`çš„`sequence_ids`æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿çš„åŒºåˆ†tokençš„æ¥æºç¼–å·ï¼š

- å¯¹äºç‰¹æ®Šæ ‡è®°ï¼šè¿”å›Noneï¼Œ
- å¯¹äºæ­£æ–‡Tokenï¼šè¿”å›å¥å­ç¼–å·ï¼ˆä»0å¼€å§‹ç¼–å·ï¼‰ã€‚

ç»¼ä¸Šï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿çš„åœ¨ä¸€ä¸ªè¾“å…¥ç‰¹å¾ä¸­æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸ Tokenã€‚


```python
sequence_ids = tokenized_example.sequence_ids()
print(sequence_ids)
```

    [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]


 **`sequence_ids`**ï¼š

---

### **ç±»æ¯”åœºæ™¯**
æƒ³è±¡ä½ åœ¨ç©ä¸€ä¸ª**åŒè‰²è§å…‰ç¬”æ ‡è®°**çš„æ¸¸æˆï¼š
- **é»„è‰²**ï¼šæ ‡è®°é—®é¢˜ï¼ˆæ¯”å¦‚ï¼š"è´çˆ·å“ªå¹´ç»“å©šï¼Ÿ"ï¼‰
- **è“è‰²**ï¼šæ ‡è®°ä¹¦ä¸­çš„ç­”æ¡ˆæ®µè½ï¼ˆæ¯”å¦‚ä¹¦é‡Œå†™ï¼š"è´çˆ·2008å¹´ç»“å©š..."ï¼‰
- **çº¢è‰²**ï¼šæ ‡è®°ç‰¹æ®Šç¬¦å·ï¼ˆæ¯”å¦‚ä¹¦çš„å°é¢ã€ç« èŠ‚åˆ†éš”é¡µï¼‰

`sequence_ids` å°±æ˜¯ä¸€ä¸ª**é¢œè‰²ç¼–å·åˆ—è¡¨**ï¼Œå‘Šè¯‰ä½ æ¯ä¸ªå­—å±äºå“ªéƒ¨åˆ†ã€‚

---

### **ä¸‰ç§æ ‡è®°è§„åˆ™**
1. **`None` â†’ çº¢è‰²æ ‡è®°**  
   - å¯¹åº”ç‰¹æ®Šç¬¦å·ï¼š`[CLS]`ï¼ˆå¼€å¤´æ ‡å¿—ï¼‰ã€`[SEP]`ï¼ˆåˆ†éš”ç¬¦ï¼‰
   - ä¾‹ï¼š`[CLS]` â†’ `None`

2. **`0` â†’ é»„è‰²æ ‡è®°**  
   - æ‰€æœ‰æ¥è‡ª**é—®é¢˜**çš„æ–‡å­—  
   - ä¾‹ï¼š"è´çˆ·"ã€"å“ªå¹´" â†’ `0`

3. **`1` â†’ è“è‰²æ ‡è®°**  
   - æ‰€æœ‰æ¥è‡ª**ä¹¦æœ¬æ–‡æ¡£**çš„æ–‡å­—  
   - ä¾‹ï¼š"2008å¹´"ã€"ç»“å©š" â†’ `1`

---

### **å®é™…æ•ˆæœç¤ºä¾‹**
å‡è®¾é—®é¢˜å’Œæ–‡æ¡£ç»„åˆåï¼š
```
[CLS] è´çˆ·å“ªå¹´ç»“å©šï¼Ÿ [SEP] è´çˆ·2008å¹´ä¸Jay-Zç»“å©š... [SEP]
```

å¯¹åº”çš„ `sequence_ids` å°±åƒè¿™æ ·ï¼š
```
[ None, 0,0,0,0, None, 1,1,1,1,1, None ]
```
å¯è§†åŒ–æ ‡è®°ï¼š
```
çº¢è‰² [CLS] â†’ é»„é»„é»„é»„ â†’ çº¢è‰² [SEP] â†’ è“è“è“è“è“ â†’ çº¢è‰² [SEP]
```

---

### **æ ¸å¿ƒç”¨é€”**
1. **å¿«é€Ÿå®šä½ç­”æ¡ˆèŒƒå›´**  
   ```python
   # æ‰¾åˆ°æ–‡æ¡£éƒ¨åˆ†çš„èµ·æ­¢ä½ç½®
   start = sequence_ids.index(1)                 # ç¬¬ä¸€ä¸ªè“è‰²æ ‡è®°çš„ä½ç½®
   end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1  # æœ€åä¸€ä¸ªè“è‰²æ ‡è®°
   ```

2. **è¿‡æ»¤æ— æ•ˆå†…å®¹**  
   ```python
   # åªå¤„ç†æ–‡æ¡£éƒ¨åˆ†çš„æ–‡å­—
   if sequence_ids[i] == 1:
       print("è¿™æ˜¯ä¹¦é‡Œçš„å†…å®¹ï¼")
   ```

3. **å¤„ç†é•¿æ–‡æœ¬åˆ†å—**  
   - å½“æ–‡æ¡£å¤ªé•¿æ—¶ï¼Œè‡ªåŠ¨åˆ†æˆå¤šå—ï¼Œæ¯å—éƒ½æœ‰è‡ªå·±çš„ `sequence_ids`
   - ä¾‹ï¼šåˆ†å—1çš„è“è‰²æ ‡è®°å¯¹åº”æ–‡æ¡£å‰åŠéƒ¨åˆ†ï¼Œåˆ†å—2å¯¹åº”ååŠéƒ¨åˆ†

---

### **ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ**
å°±åƒè¯»ä¹¦æ—¶ç”¨è§å…‰ç¬”åˆ’é‡ç‚¹ï¼š
- **é»„è‰²**ï¼šæ˜ç¡®é—®é¢˜ï¼ˆçŸ¥é“è¦æ‰¾ä»€ä¹ˆï¼‰
- **è“è‰²**ï¼šå¿«é€Ÿé”å®šç­”æ¡ˆåŒºåŸŸï¼ˆä¸ç”¨è¯»å®Œæ•´æœ¬ä¹¦ï¼‰
- **çº¢è‰²**ï¼šå¿½ç•¥æ— å…³çš„å°é¢/åˆ†éš”é¡µ

è¿™è®©æ¨¡å‹åƒäººç±»ä¸€æ ·ï¼šå…ˆçœ‹é—®é¢˜ï¼Œå†å¿«é€Ÿç¿»ä¹¦æ‰¾ç­”æ¡ˆä½ç½®ï¼Œè€Œä¸æ˜¯å‚»å‚»é€šè¯»å…¨æ–‡ã€‚


```python
# æ£€æŸ¥åˆ†å—æ•°é‡
num_chunks = len(tokenized_example["input_ids"])
print(f"ç”Ÿæˆåˆ†å—æ•°: {num_chunks}")

# éå†æ¯ä¸ªåˆ†å—
for chunk_idx in range(num_chunks):
    print(f"\n=== åˆ†å— {chunk_idx} ===")
    
    # æ­£ç¡®è·å–å½“å‰åˆ†å—çš„æ•°æ®
    chunk_input_ids = tokenized_example["input_ids"][chunk_idx]
    chunk_sequence_ids = tokenized_example.sequence_ids(chunk_idx)  # å…³é”®ä¿®å¤ç‚¹
    
    # æ‰“å°å…³é”®ä¿¡æ¯
    print(f"Tokenæ•°é‡: {len(chunk_input_ids)}")
    print(f"sequence_idsç»“æ„: {chunk_sequence_ids[:20]}...")  # æ‰“å°å‰20ä¸ªå…ƒç´ 
    
    # æ£€æŸ¥é—®é¢˜éƒ¨åˆ†æ˜¯å¦å®Œæ•´
    question_segment = [i for i, sid in enumerate(chunk_sequence_ids) if sid == 0]
    print(f"é—®é¢˜éƒ¨åˆ†è¦†ç›–çš„tokenä½ç½®: {question_segment[:5]}...")

```

    ç”Ÿæˆåˆ†å—æ•°: 2
    
    === åˆ†å— 0 ===
    Tokenæ•°é‡: 384
    sequence_idsç»“æ„: [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1]...
    é—®é¢˜éƒ¨åˆ†è¦†ç›–çš„tokenä½ç½®: [1, 2, 3, 4, 5]...
    
    === åˆ†å— 1 ===
    Tokenæ•°é‡: 157
    sequence_idsç»“æ„: [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1]...
    é—®é¢˜éƒ¨åˆ†è¦†ç›–çš„tokenä½ç½®: [1, 2, 3, 4, 5]...



```python
answers = example["answers"]
start_char = answers["answer_start"][0]
end_char = start_char + len(answers["text"][0])

print(answers)
print(start_char)
print(end_char)
# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹æ ‡è®°ç´¢å¼•ã€‚
token_start_index = 0
while sequence_ids[token_start_index] != 1:
    token_start_index += 1

# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„ç»“æŸæ ‡è®°ç´¢å¼•ã€‚
token_end_index = len(tokenized_example["input_ids"][0]) - 1
while sequence_ids[token_end_index] != 1:
    token_end_index -= 1

# æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºspanèŒƒå›´ï¼ˆå¦‚æœè¶…å‡ºèŒƒå›´ï¼Œè¯¥ç‰¹å¾å°†ä»¥CLSæ ‡è®°ç´¢å¼•æ ‡è®°ï¼‰ã€‚
offsets = tokenized_example["offset_mapping"][0]
if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
    # å°†token_start_indexå’Œtoken_end_indexç§»åŠ¨åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚
    # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç§»åˆ°æœ€åä¸€ä¸ªæ ‡è®°ä¹‹åï¼ˆè¾¹ç•Œæƒ…å†µï¼‰ã€‚
    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
        token_start_index += 1
    start_position = token_start_index - 1
    while offsets[token_end_index][1] >= end_char:
        token_end_index -= 1
    end_position = token_end_index + 1
    print(start_position, end_position)
else:
    print("ç­”æ¡ˆä¸åœ¨æ­¤ç‰¹å¾ä¸­ã€‚")

```

    {'text': ['over 1,600'], 'answer_start': [30]}
    30
    40
    23 26


æ‰“å°æ£€æŸ¥æ˜¯å¦å‡†ç¡®æ‰¾åˆ°äº†èµ·å§‹ä½ç½®ï¼š


```python
# é€šè¿‡æŸ¥æ‰¾ offset mapping ä½ç½®ï¼Œè§£ç  context ä¸­çš„ç­”æ¡ˆ 
print(tokenizer.decode(tokenized_example["input_ids"][0][start_position: end_position+1]))
# ç›´æ¥æ‰“å° æ•°æ®é›†ä¸­çš„æ ‡å‡†ç­”æ¡ˆï¼ˆanswer["text"])
print(answers["text"][0])
```

    over 1, 600
    over 1,600


#### å…³äºå¡«å……çš„ç­–ç•¥

- å¯¹äºæ²¡æœ‰è¶…è¿‡æœ€å¤§é•¿åº¦çš„æ–‡æœ¬ï¼Œå¡«å……è¡¥é½é•¿åº¦ã€‚
- å¯¹äºéœ€è¦å·¦ä¾§å¡«å……çš„æ¨¡å‹ï¼Œäº¤æ¢ question å’Œ context é¡ºåº


```python
pad_on_right = tokenizer.padding_side == "right"
```

### æ•´åˆä»¥ä¸Šæ‰€æœ‰é¢„å¤„ç†æ­¥éª¤

è®©æˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹æ•´åˆåˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°è®­ç»ƒé›†ã€‚

é’ˆå¯¹ä¸å¯å›ç­”çš„æƒ…å†µï¼ˆä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œç­”æ¡ˆåœ¨å¦ä¸€ä¸ªç‰¹å¾ä¸­ï¼‰ï¼Œæˆ‘ä»¬ä¸ºå¼€å§‹å’Œç»“æŸä½ç½®éƒ½è®¾ç½®äº†clsç´¢å¼•ã€‚

å¦‚æœallow_impossible_answersæ ‡å¿—ä¸ºFalseï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç®€å•åœ°ä»è®­ç»ƒé›†ä¸­ä¸¢å¼ƒè¿™äº›ç¤ºä¾‹ã€‚


```python
def prepare_train_features(examples):
    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§å¯èƒ½æœ‰å¾ˆå¤šç©ºç™½å­—ç¬¦ï¼Œè¿™å¯¹æˆ‘ä»¬æ²¡æœ‰ç”¨ï¼Œè€Œä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡çš„æˆªæ–­å¤±è´¥
    # ï¼ˆæ ‡è®°åŒ–çš„é—®é¢˜å°†å ç”¨å¤§é‡ç©ºé—´ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ é™¤å·¦ä¾§çš„ç©ºç™½å­—ç¬¦ã€‚
    examples["question"] = [q.lstrip() for q in examples["question"]]

    # ä½¿ç”¨æˆªæ–­å’Œå¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†ä¿ç•™æº¢å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨æ­¥å¹…ï¼ˆstrideï¼‰ã€‚
    # å½“ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªç¤ºä¾‹å¯èƒ½æä¾›å¤šä¸ªç‰¹å¾ï¼Œå…¶ä¸­æ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡æœ‰ä¸€äº›é‡å ã€‚
    tokenized_examples = tokenizer(
        examples["question" if pad_on_right else "context"],
        examples["context" if pad_on_right else "question"],
        truncation="only_second" if pad_on_right else "only_first",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç»™æˆ‘ä»¬æä¾›å¤šä¸ªç‰¹å¾ï¼ˆå¦‚æœå®ƒå…·æœ‰å¾ˆé•¿çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æä¾›äº†è¿™ä¸ªæ˜ å°„å…³ç³»ã€‚
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    # åç§»æ˜ å°„å°†ä¸ºæˆ‘ä»¬æä¾›ä»ä»¤ç‰Œåˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬è®¡ç®—å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ã€‚
    offset_mapping = tokenized_examples.pop("offset_mapping")

    # è®©æˆ‘ä»¬ä¸ºè¿™äº›ç¤ºä¾‹è¿›è¡Œæ ‡è®°ï¼
    tokenized_examples["start_positions"] = []
    tokenized_examples["end_positions"] = []

    for i, offsets in enumerate(offset_mapping):
        # æˆ‘ä»¬å°†ä½¿ç”¨ CLS ç‰¹æ®Š token çš„ç´¢å¼•æ¥æ ‡è®°ä¸å¯èƒ½çš„ç­”æ¡ˆã€‚
        input_ids = tokenized_examples["input_ids"][i]
        cls_index = input_ids.index(tokenizer.cls_token_id)

        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£ä¸Šä¸‹æ–‡å’Œé—®é¢˜æ˜¯ä»€ä¹ˆï¼‰ã€‚
        sequence_ids = tokenized_examples.sequence_ids(i)

        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥æä¾›å¤šä¸ªè·¨åº¦ï¼Œè¿™æ˜¯åŒ…å«æ­¤æ–‡æœ¬è·¨åº¦çš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]
        # å¦‚æœæ²¡æœ‰ç»™å‡ºç­”æ¡ˆï¼Œåˆ™å°†cls_indexè®¾ç½®ä¸ºç­”æ¡ˆã€‚
        if len(answers["answer_start"]) == 0:
            tokenized_examples["start_positions"].append(cls_index)
            tokenized_examples["end_positions"].append(cls_index)
        else:
            # ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹å’Œç»“æŸå­—ç¬¦ç´¢å¼•ã€‚
            start_char = answers["answer_start"][0]
            end_char = start_char + len(answers["text"][0])

            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹ä»¤ç‰Œç´¢å¼•ã€‚
            token_start_index = 0
            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
                token_start_index += 1

            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„ç»“æŸä»¤ç‰Œç´¢å¼•ã€‚
            token_end_index = len(input_ids) - 1
            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
                token_end_index -= 1

            # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºè·¨åº¦ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥ç‰¹å¾çš„æ ‡ç­¾å°†ä½¿ç”¨CLSç´¢å¼•ï¼‰ã€‚
            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                # å¦åˆ™ï¼Œå°†token_start_indexå’Œtoken_end_indexç§»åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚
                # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼ˆè¾¹ç¼˜æƒ…å†µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€åä¸€ä¸ªåç§»ä¹‹åç»§ç»­ã€‚
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
                    token_start_index += 1
                tokenized_examples["start_positions"].append(token_start_index - 1)
                while offsets[token_end_index][1] >= end_char:
                    token_end_index -= 1
                tokenized_examples["end_positions"].append(token_end_index + 1)

    return tokenized_examples
```

---

### **åŠŸèƒ½ç›®æ ‡**
è¿™ä¸ªå‡½æ•°å°±åƒä¸€ä½ **æ•°æ®åŠ å·¥å‚çš„æµæ°´çº¿å·¥äºº**ï¼Œè´Ÿè´£æŠŠåŸå§‹é—®ç­”æ•°æ®æ”¹é€ æˆé€‚åˆæ¨¡å‹ç†è§£çš„æ ¼å¼ã€‚ä¸»è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼š
1. **é•¿æ–‡æœ¬åˆ‡å‰²**ï¼šå½“ç­”æ¡ˆæ–‡ç« å¤ªé•¿æ—¶ï¼Œåˆ‡æˆå¤šä¸ªçŸ­å—ï¼ˆç±»ä¼¼å°†é•¿è§†é¢‘åˆ†æ®µï¼‰
2. **ç­”æ¡ˆå®šä½**ï¼šåœ¨æ¯ä¸ªçŸ­å—ä¸­æ ‡æ³¨ç­”æ¡ˆçš„ä½ç½®ï¼ˆç±»ä¼¼è§†é¢‘å‰ªè¾‘æ—¶æ ‡è®°ç²¾å½©ç‰‡æ®µçš„èµ·æ­¢æ—¶é—´ï¼‰

---

### **æ ¸å¿ƒå¤„ç†æ­¥éª¤**

#### **1. æ¸…ç†é—®é¢˜æ–‡å­—ï¼ˆå»å·¦ç©ºæ ¼ï¼‰**
- **é—®é¢˜**ï¼šç”¨æˆ·æé—®å¯èƒ½åŒ…å«å¤šä½™ç©ºæ ¼ï¼Œä¾‹å¦‚ `"Â Â  Beyonceå“ªå¹´ç»“å©šï¼Ÿ"`
- **å¤„ç†**ï¼šå»æ‰å·¦è¾¹çš„ç©ºæ ¼ â†’ `"Beyonceå“ªå¹´ç»“å©šï¼Ÿ"`
- **åŸå› **ï¼šé˜²æ­¢ç©ºæ ¼å ç”¨åˆ†è¯åé¢ï¼Œå¯¼è‡´æ­£æ–‡è¢«è¿‡åº¦æˆªæ–­

#### **2. æ–‡æœ¬åˆ†å—å¤„ç†**
- **æ“ä½œ**ï¼šå°†é•¿æ–‡ç« åˆ‡æˆå¤šä¸ªå°å—ï¼ˆæ¯å—æœ€é•¿ `max_length`ï¼Œå—é—´é‡å  `stride`ï¼‰
- **ç¤ºä¾‹**ï¼š
  ```
  åŸæ–‡ç« ï¼šæ®µè½1...æ®µè½2...æ®µè½3...ï¼ˆæ€»é•¿è¶…è¿‡max_lengthï¼‰
  åˆ†å—1ï¼šæ®µè½1...æ®µè½2ï¼ˆå‰åŠï¼‰
  åˆ†å—2ï¼šæ®µè½2ï¼ˆååŠï¼‰...æ®µè½3
  ```

#### **3. è®°å½•åˆ†å—å…³ç³»**
- **overflow_to_sample_mapping**ï¼šè®°å½•æ¯ä¸ªåˆ†å—å±äºå“ªä¸ªåŸå§‹æ ·æœ¬  
  ï¼ˆç±»ä¼¼å¿«é€’åˆ†ç®±æ—¶åœ¨æ¯ç®±è´´åŸè®¢å•å·ï¼‰
- **offset_mapping**ï¼šè®°å½•æ¯ä¸ªåˆ†è¯å¯¹åº”çš„åŸå§‹å­—ç¬¦ä½ç½®  
  ï¼ˆç±»ä¼¼æ¯å—ç§¯æœ¨å¯¹åº”åŸå›¾çº¸çš„ä½ç½®ï¼‰

#### **4. å¤„ç†æ— ç­”æ¡ˆæƒ…å†µ**
- **åœºæ™¯**ï¼šå½“ç­”æ¡ˆä¸åœ¨å½“å‰åˆ†å—ä¸­ï¼ˆä¾‹å¦‚ç­”æ¡ˆåœ¨å¦ä¸€ä¸ªåˆ†å—é‡Œï¼‰
- **æ ‡è®°**ï¼šå°†ç­”æ¡ˆä½ç½®è®¾ä¸º `[CLS]` çš„ä½ç½®ï¼ˆæ¨¡å‹çœ‹åˆ°è¿™ä¸ªå°±çŸ¥é“å½“å‰å—æ— ç­”æ¡ˆï¼‰

#### **5. ç²¾ç¡®å®šä½ç­”æ¡ˆ**
- **æ­¥éª¤**ï¼š
  1. **ç¡®å®šç­”æ¡ˆå­—ç¬¦èŒƒå›´**ï¼š`start_char` åˆ° `end_char`
  2. **æ‰¾åˆ°åˆ†å—çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†**ï¼ˆè·³è¿‡é—®é¢˜å’Œç‰¹æ®Šæ ‡è®°ï¼‰
  3. **æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨æœ¬åˆ†å—**ï¼š
     - æ˜¯ â†’ è°ƒæ•´åˆ°ç²¾ç¡®çš„åˆ†è¯ä½ç½®
     - å¦ â†’ æ ‡è®°ä¸º `[CLS]`

---

### **å®é™…æ¡ˆä¾‹æ¼”ç¤º**
**è¾“å…¥æ•°æ®**ï¼š
```python
{
    "question": "Beyonceå“ªå¹´ç»“å©šï¼Ÿ",
    "context": "Beyonceäº2008å¹´ä¸Jay-Zç»“å©š...ï¼ˆé•¿æ–‡æœ¬ï¼‰",
    "answers": {"text": ["2008å¹´"], "answer_start": }
}
```

**å¤„ç†è¿‡ç¨‹**ï¼š
1. **åˆ†å—**ï¼šå°†é•¿ `context` åˆ†æˆä¸¤ä¸ªå—
2. **å—1å¤„ç†**ï¼š
   - å‘ç°ç­”æ¡ˆ `2008å¹´` åœ¨å—1ä¸­
   - æ ‡æ³¨èµ·å§‹ä½ç½®ä¸º `token 6`ï¼Œç»“æŸä½ç½®ä¸º `token 7`
3. **å—2å¤„ç†**ï¼š
   - å—2ä¸åŒ…å«ç­”æ¡ˆ â†’ æ ‡æ³¨ä¸º `[CLS]`

**è¾“å‡ºç‰¹å¾**ï¼š
```python
{
    "input_ids": [101, 2345, 3456, ..., 102],  # åˆ†å—åçš„token
    "start_positions": 6, 
    "end_positions": 7
}
```

---

### **å‚æ•°æ§åˆ¶è¡Œä¸º**
| å‚æ•° | ä½œç”¨ | ç±»æ¯”è§£é‡Š |
|------|------|----------|
| `max_length=384` | æ¯å—æœ€å¤§é•¿åº¦ | æ¯æ®µè§†é¢‘æœ€é•¿5åˆ†é’Ÿ |
| `stride=128` | åˆ†å—é—´é‡å é•¿åº¦ | ä¸¤æ®µè§†é¢‘é—´é‡å 30ç§’é˜²æ­¢æ¼å†…å®¹ |
| `pad_on_right=True` | é—®é¢˜åœ¨å³/å·¦å¡«å…… | å­—å¹•åœ¨è§†é¢‘å·¦ä¸‹æ–¹è¿˜æ˜¯å³ä¸‹æ–¹ |

---

### **æ€»ç»“**
è¿™ä¸ªå‡½æ•°å°±åƒä¸€ä½æ™ºèƒ½å‰ªè¾‘å¸ˆï¼š
1. **åˆ‡åˆ†é•¿è§†é¢‘**ï¼ˆåˆ†å—å¤„ç†ï¼‰
2. **æ ‡è®°å…³é”®ç‰‡æ®µ**ï¼ˆç­”æ¡ˆå®šä½ï¼‰
3. **å¤„ç†ç‰¹æ®Šæƒ…å†µ**ï¼ˆæ— ç­”æ¡ˆæ—¶æ‰“æ ‡è®°ï¼‰

æœ€ç»ˆè¾“å‡ºæ¨¡å‹å¯ä»¥ç›´æ¥å­¦ä¹ çš„æ ‡å‡†åŒ–æ•°æ®æ ¼å¼ï¼Œæ˜¯è®­ç»ƒé«˜è´¨é‡é—®ç­”æ¨¡å‹çš„å…³é”®é¢„å¤„ç†æ­¥éª¤ï¼ ğŸš€

#### datasets.map çš„è¿›é˜¶ä½¿ç”¨

ä½¿ç”¨ `datasets.map` æ–¹æ³•å°† `prepare_train_features` åº”ç”¨äºæ‰€æœ‰è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®ï¼š

- batched: æ‰¹é‡å¤„ç†æ•°æ®ã€‚
- remove_columns: å› ä¸ºé¢„å¤„ç†æ›´æ”¹äº†æ ·æœ¬çš„æ•°é‡ï¼Œæ‰€ä»¥åœ¨åº”ç”¨å®ƒæ—¶éœ€è¦åˆ é™¤æ—§åˆ—ã€‚
- load_from_cache_fileï¼šæ˜¯å¦ä½¿ç”¨datasetsåº“çš„è‡ªåŠ¨ç¼“å­˜

datasets åº“é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®ï¼Œå®ç°äº†é«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹ä¼ é€’ç»™ map çš„å‡½æ•°æ˜¯å¦å·²æ›´æ”¹ï¼ˆå› æ­¤éœ€è¦ä¸ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰ã€‚å¦‚æœåœ¨è°ƒç”¨ map æ—¶è®¾ç½® `load_from_cache_file=False`ï¼Œå¯ä»¥å¼ºåˆ¶é‡æ–°åº”ç”¨é¢„å¤„ç†ã€‚

---

### **æ ¸å¿ƒæµç¨‹ç±»æ¯”**
æƒ³è±¡ä½ ç»è¥ä¸€ä¸ª **å¤§å‹å¿«é€’åˆ†æ‹£ä¸­å¿ƒ**ï¼Œéœ€è¦å¤„ç†ä¸‰ç§åŒ…è£¹ï¼ˆè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼‰ã€‚`datasets.map` å°±æ˜¯ä½ çš„ **è‡ªåŠ¨åŒ–åˆ†æ‹£æµæ°´çº¿**ï¼Œ`prepare_train_features` æ˜¯ä½ å®šåˆ¶çš„ **æ™ºèƒ½åˆ†æ‹£è§„åˆ™**ã€‚

---

### **åˆ†æ‹£çº¿å‚æ•°è§£æ**
```python
tokenized_datasets = datasets.map(
    prepare_train_features,  # ä½ çš„æ™ºèƒ½åˆ†æ‹£è§„åˆ™
    batched=True,            # æ•´ç®±å¤„ç†ï¼ˆè€Œä¸æ˜¯å•ä»¶ï¼‰
    remove_columns=åŸå§‹åŒ…è£¹æ ‡ç­¾  # æ’•æ‰æ—§æ ‡ç­¾
)
```

#### 1. **`batched=True` â†’ æ•´ç®±å¤„ç†æ¨¡å¼**
- **ä¼ ç»Ÿæ–¹å¼**ï¼šå·¥äººé€ä¸ªæ£€æŸ¥åŒ…è£¹ï¼ˆå•æ¡æ•°æ®å¤„ç†ï¼‰
- **é«˜æ•ˆæ¨¡å¼**ï¼šæ•´ç®±å€’è¿›æœºå™¨ï¼ŒåŒæ—¶å¤„ç†æ•°ç™¾ä¸ªåŒ…è£¹ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
- **ä¼˜åŠ¿**ï¼šé€Ÿåº¦æå‡ 10-100 å€ï¼Œç‰¹åˆ«é€‚åˆ GPU å¹¶è¡Œè®¡ç®—

#### 2. **`remove_columns` â†’ æ¸…é™¤æ—§æ ‡ç­¾**
- **åŸå› **ï¼šç»è¿‡åˆ†æ‹£åï¼ŒåŒ…è£¹å½¢çŠ¶æ”¹å˜ï¼ˆæ•°æ®åˆ—å˜åŒ–ï¼‰
- **æ“ä½œ**ï¼š
  - åŸå§‹æ ‡ç­¾ï¼šå‘ä»¶äººã€æ”¶ä»¶äººï¼ˆ`question`, `context` ç­‰ï¼‰
  - æ–°æ ‡ç­¾ï¼šç›®çš„åœ°ä»£ç ã€é‡é‡åˆ†çº§ï¼ˆ`input_ids`, `attention_mask` ç­‰ï¼‰
- **ç¤ºä¾‹**ï¼šå°±åƒå¿«é€’é‡æ–°åŒ…è£…åï¼Œéœ€è¦å»æ‰æ—§é¢å•

#### 3. **ç¼“å­˜æœºåˆ¶ â†’ æ™ºèƒ½æš‚å­˜åŒº**
- **è‡ªåŠ¨æ£€æµ‹**ï¼šå¦‚æœåˆ†æ‹£è§„åˆ™æ²¡å˜ï¼Œç›´æ¥ä½¿ç”¨æš‚å­˜åŒºå¤„ç†å¥½çš„åŒ…è£¹
- **å¼ºåˆ¶åˆ·æ–°**ï¼š`load_from_cache_file=False` å°±åƒè¦æ±‚ã€Œä¸ç®¡æœ‰æ²¡æœ‰æ—§åŒ…è£¹ï¼Œå…¨éƒ¨é‡æ–°åˆ†æ‹£ã€
- **ä¼˜åŠ¿**ï¼šèŠ‚çœ 70% ä»¥ä¸Šæ—¶é—´ï¼Œé¿å…é‡å¤åŠ³åŠ¨

---

### **å®Œæ•´å·¥ä½œæµç¨‹**
1. **æ”¶åŒ…è£¹**ï¼šä¸‰ç§ç±»å‹åŒ…è£¹è¿›å…¥æµæ°´çº¿ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼‰
2. **è§„åˆ™åº”ç”¨**ï¼š
   - æ™ºèƒ½åˆ‡å‰²å¤§åŒ…è£¹ï¼ˆé•¿æ–‡æœ¬åˆ†å—ï¼‰
   - è´´ä¸Šç²¾å‡†ç›®çš„åœ°æ ‡ç­¾ï¼ˆç­”æ¡ˆä½ç½®æ ‡è®°ï¼‰
   - ä¸¢å¼ƒç ´æŸåŒ…è£¹ï¼ˆæ— æ•ˆæ ·æœ¬ï¼‰
3. **è¾“å‡ºç»“æœ**ï¼š
   - æ ‡å‡†åŒ–å¿«é€’ç®±ï¼ˆæ¨¡å‹å¯è¯»çš„ `input_ids` ç­‰ï¼‰
   - ç²¾å‡†ç‰©æµæ ‡ç­¾ï¼ˆ`start_positions`, `end_positions`ï¼‰

---

### **æŠ€æœ¯ç»†èŠ‚å¯¹åº”**
| å¿«é€’åœºæ™¯ | æ•°æ®å¤„ç† |
|---------|----------|
| åŒ…è£¹ç±»å‹åŒºåˆ† | ä¿æŒè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ç»“æ„ |
| åˆ†æ‹£æœºå™¨äºº | `prepare_train_features` å‡½æ•° |
| æ•´ç®±å¤„ç† | æ‰¹é‡çŸ©é˜µè¿ç®— |
| æš‚å­˜åŒº | Hugging Face çš„ç¼“å­˜æ–‡ä»¶ï¼ˆé€šå¸¸å­˜äº ~/.cache/huggingface/datasetsï¼‰|

---

### **ä¸ºä»€ä¹ˆéœ€è¦è¿™æ ·è®¾è®¡ï¼Ÿ**
1. **æ•ˆç‡ä¼˜å…ˆ**ï¼šå¦‚åŒå¿«é€’è¡Œä¸šè¿½æ±‚æ¯æ—¥ç™¾ä¸‡ä»¶å¤„ç†é‡ï¼Œæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒå°±æ˜¯ **å¤§è§„æ¨¡æ•°æ®åå**
2. **èµ„æºç®¡ç†**ï¼šç¼“å­˜æœºåˆ¶åƒåŒåä¸€çš„é¢„å”®åŒ…è£…ï¼Œæå‰å®Œæˆéƒ¨åˆ†å·¥ä½œå‡è½»é«˜å³°å‹åŠ›
3. **è´¨é‡ç®¡æ§**ï¼š`remove_columns` ç¡®ä¿ä¸ä¼šæŠŠç”Ÿé²œå’Œæ™®é€šåŒ…è£¹æ··æ·†ï¼ˆé˜²æ­¢æ•°æ®æ±¡æŸ“ï¼‰

é€šè¿‡è¿™å¥—ç³»ç»Ÿï¼Œä½ çš„æ¨¡å‹å°±åƒé«˜æ•ˆçš„ç‰©æµç½‘ç»œï¼Œèƒ½å¿«é€Ÿå‡†ç¡®åœ°å°†ã€Œé—®é¢˜åŒ…è£¹ã€é€è¾¾ã€Œç­”æ¡ˆç›®çš„åœ°ã€ï¼ğŸššâœ¨


```python
tokenized_datasets = datasets.map(prepare_train_features,
                                  batched=True,
                                  remove_columns=datasets["train"].column_names)
```


    Map:   0%|          | 0/87599 [00:00<?, ? examples/s]



    Map:   0%|          | 0/10570 [00:00<?, ? examples/s]


## å¾®è°ƒæ¨¡å‹

ç°åœ¨æˆ‘ä»¬çš„æ•°æ®å·²ç»å‡†å¤‡å¥½ç”¨äºè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒã€‚

ç”±äºæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯é—®ç­”ï¼Œæˆ‘ä»¬ä½¿ç”¨ `AutoModelForQuestionAnswering` ç±»ã€‚(å¯¹æ¯” Yelp è¯„è®ºæ‰“åˆ†ä½¿ç”¨çš„æ˜¯ `AutoModelForSequenceClassification` ç±»ï¼‰

è­¦å‘Šé€šçŸ¥æˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆ`vocab_transform` å’Œ `vocab_layer_norm` å±‚ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡ï¼ˆ`pre_classifier` å’Œ `classifier` å±‚ï¼‰ã€‚åœ¨å¾®è°ƒæ¨¡å‹æƒ…å†µä¸‹æ˜¯ç»å¯¹æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ é™¤ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å¤´éƒ¨ï¼Œå¹¶ç”¨ä¸€ä¸ªæ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒï¼Œå¯¹äºè¿™ä¸ªæ–°å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“ä¼šè­¦å‘Šæˆ‘ä»¬åœ¨ç”¨å®ƒè¿›è¡Œæ¨ç†ä¹‹å‰åº”è¯¥å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ã€‚

---

### ä¸€ã€ä»»åŠ¡ä¸æ¨¡å‹ç±»çš„å¯¹åº”å…³ç³»
Hugging Face Transformers åº“ä¸ºä¸åŒä»»åŠ¡æä¾›äº†ä¸“ç”¨ç±»ï¼Œå°±åƒé€‰æ‹©ä¸åŒçš„å·¥å…·ï¼š

| ä»»åŠ¡ç±»å‹                  | å¯¹åº”æ¨¡å‹ç±»                          | ç¤ºä¾‹åœºæ™¯                     |
|--------------------------|-----------------------------------|----------------------------|
| æ–‡æœ¬åˆ†ç±»                  | `AutoModelForSequenceClassification` | æƒ…æ„Ÿåˆ†æã€è¯„åˆ†é¢„æµ‹          |
| é—®ç­”ä»»åŠ¡                  | `AutoModelForQuestionAnswering`     | SQuAD é—®ç­”ã€é˜…è¯»ç†è§£        |
| æ–‡æœ¬ç”Ÿæˆ                  | `AutoModelForCausalLM`              | æ•…äº‹ç»­å†™ã€å¯¹è¯ç”Ÿæˆ          |
| æ©ç è¯­è¨€å»ºæ¨¡              | `AutoModelForMaskedLM`              | BERT å¼å¡«ç©ºä»»åŠ¡             |
| åºåˆ—åˆ°åºåˆ—                | `AutoModelForSeq2SeqLM`             | ç¿»è¯‘ã€æ‘˜è¦ç”Ÿæˆ              |
| æ ‡è®°åˆ†ç±»                  | `AutoModelForTokenClassification`   | å‘½åå®ä½“è¯†åˆ«ã€è¯æ€§æ ‡æ³¨      |
| å¤šé€‰ä»»åŠ¡                  | `AutoModelForMultipleChoice`        | å¤šé€‰é¢˜å›ç­”                  |

---

### äºŒã€Transformers åº“çš„ä¸»è¦æ¨¡å‹ç±»
ä»¥ä¸‹æ˜¯å¸¸ç”¨çš„æ¨¡å‹ç±»ï¼ˆä»¥ **BERT** æ¶æ„ä¸ºä¾‹ï¼Œå…¶ä»–æ¨¡å‹ç±»ä¼¼ï¼‰ï¼š

#### 1. åŸºç¡€æ¨¡å‹
```python
from transformers import AutoModel
model = AutoModel.from_pretrained("bert-base-uncased")  # é€šç”¨ç‰¹å¾æå–
```

#### 2. ä»»åŠ¡ä¸“ç”¨æ¨¡å‹
```python
# æ–‡æœ¬åˆ†ç±»ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰
AutoModelForSequenceClassification.from_pretrained(...)

# é—®ç­”ä»»åŠ¡ï¼ˆå¦‚SQuADï¼‰
AutoModelForQuestionAnswering.from_pretrained(...)

# æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚GPTé£æ ¼ï¼‰
AutoModelForCausalLM.from_pretrained(...)

# åºåˆ—åˆ°åºåˆ—ï¼ˆå¦‚BART/T5ï¼‰
AutoModelForSeq2SeqLM.from_pretrained(...)

# æ ‡è®°çº§åˆ†ç±»ï¼ˆå¦‚NERï¼‰
AutoModelForTokenClassification.from_pretrained(...)
```

---

### ä¸‰ã€å¤„ç†è‡ªå®šä¹‰ä»»åŠ¡çš„ä¸‰ç§æ–¹æ¡ˆ
å¦‚æœä½ çš„ä»»åŠ¡æ²¡æœ‰ç°æˆç±»ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•è§£å†³ï¼š

#### æ–¹æ¡ˆ 1ï¼šæ”¹é€ ç°æœ‰æ¨¡å‹ï¼ˆæ¨èï¼‰
```python
from transformers import AutoModel

# åŠ è½½åŸºç¡€æ¨¡å‹
model = AutoModel.from_pretrained("bert-base-uncased")

# æ·»åŠ è‡ªå®šä¹‰å¤´éƒ¨
class CustomModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = model
        self.custom_head = nn.Linear(768, 3)  # å‡è®¾ä½ çš„ä»»åŠ¡éœ€è¦3ç±»è¾“å‡º
        
    def forward(self, inputs):
        outputs = self.bert(**inputs)
        pooled = outputs.last_hidden_state[:,0]  # å–CLSæ ‡è®°
        return self.custom_head(pooled)
```

#### æ–¹æ¡ˆ 2ï¼šç»§æ‰¿å¹¶æ‰©å±•
```python
from transformers import BertPreTrainedModel, BertModel

class MyCustomModel(BertPreTrainedModel):
    def __init__(self, config):
        super().__init__(config)
        self.bert = BertModel(config)
        self.my_layer = nn.Linear(config.hidden_size, 5)  # è‡ªå®šä¹‰è¾“å‡ºç»´åº¦

    def forward(self, input_ids, attention_mask=None):
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state
        return self.my_layer(sequence_output[:,0])  # ä½¿ç”¨CLSæ ‡è®°
```

#### æ–¹æ¡ˆ 3ï¼šä½¿ç”¨ `AutoModelWithHeads`
ï¼ˆéœ€å®‰è£… `adapters` åº“ï¼‰
```python
from transformers.adapters import AutoAdapterModel

model = AutoAdapterModel.from_pretrained("bert-base-uncased")
model.add_classification_head("my_task", num_labels=3)  # æ·»åŠ åˆ†ç±»å¤´
```

---

### å››ã€å…³äºè­¦å‘Šä¿¡æ¯çš„è§£é‡Š
å½“ä½ çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è­¦å‘Šï¼š
```
Some weights were not used... (vocab_transform, vocab_layer_norm)
You should probably TRAIN this model...
```
è¿™æ˜¯ **æ­£å¸¸ç°è±¡**ï¼å› ä¸ºï¼š
1. é¢„è®­ç»ƒæ¨¡å‹çš„åŸå§‹å¤´éƒ¨ï¼ˆå¦‚MLMå¤´éƒ¨ï¼‰è¢«ç§»é™¤
2. æ–°çš„ä»»åŠ¡å¤´éƒ¨ï¼ˆå¦‚åˆ†ç±»å™¨ï¼‰éœ€è¦é‡æ–°è®­ç»ƒ
3. åº“åœ¨æé†’ä½ éœ€è¦å¾®è°ƒåæ‰èƒ½ç”¨äºæ¨ç†

---

### äº”ã€å­¦ä¹ èµ„æºæ¨è
1. [å®˜æ–¹ä»»åŠ¡æŒ‡å—](https://huggingface.co/docs/transformers/task_summary)
2. [è‡ªå®šä¹‰æ¨¡å‹æ•™ç¨‹](https://huggingface.co/docs/transformers/custom_models)
3. [ç¤¾åŒºè®ºå›](https://discuss.huggingface.co/)ï¼ˆé‡åˆ°é—®é¢˜æ—¶ä¼˜å…ˆæœç´¢ï¼‰

é€šè¿‡çµæ´»ç»„åˆè¿™äº›æ–¹æ³•ï¼Œä½ å¯ä»¥åº”å¯¹ä»»ä½•è‡ªå®šä¹‰ä»»åŠ¡éœ€æ±‚ï¼ğŸš€


```python
from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer

model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

    Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


#### TensorBoard


```python
from tensorboard import version
print("TensorBoard ç‰ˆæœ¬:", version.VERSION)
```

    TensorBoard ç‰ˆæœ¬: 2.19.0



```python
%load_ext tensorboard

# æŒ‡å®šæ—¥å¿—ç›®å½•å’Œç«¯å£ï¼ˆæ³¨æ„è¿™é‡Œçš„ç«¯å£è¦ä¸æ£€æµ‹çš„8001ä¸€è‡´ï¼‰
log_dir = "your_logs_directory"  # æ›¿æ¢ä¸ºå®é™…çš„æ—¥å¿—ç›®å½•è·¯å¾„
%tensorboard --logdir $log_dir --port 8001 --bind_all
```

    The tensorboard extension is already loaded. To reload it, use:
      %reload_ext tensorboard



    Reusing TensorBoard on port 8001 (pid 1319472), started 17:47:30 ago. (Use '!kill 1319472' to kill it.)




<iframe id="tensorboard-frame-6c031199972a8469" width="100%" height="800" frameborder="0">
</iframe>
<script>
  (function() {
    const frame = document.getElementById("tensorboard-frame-6c031199972a8469");
    const url = new URL("/", window.location);
    const port = 8001;
    if (port) {
      url.port = port;
    }
    frame.src = url;
  })();
</script>




```python
import socket
# åˆ›å»ºTCPå¥—æ¥å­—
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# å°è¯•è¿æ¥æœ¬åœ°8001ç«¯å£ï¼ˆéé˜»å¡æ–¹å¼ï¼‰
result = sock.connect_ex(('localhost', 8001))
# æ–­è¨€éªŒè¯ï¼ˆ0è¡¨ç¤ºç«¯å£å¼€æ”¾ï¼‰
assert result == 0, "TensorBoard ç«¯å£ 8001 æœªå¼€å¯ï¼"
```


```python
# æœ‰é—®é¢˜ï¼Œæš‚æ—¶ä¸ç”¨ï¼Œå…ˆè®­ç»ƒ

# import evaluate

# # åŠ è½½F1æŒ‡æ ‡ï¼ˆæ”¯æŒåˆ†ç±»ä»»åŠ¡çš„micro/macro/weightedï¼‰
# squad_metric = evaluate.load("/root/projects/LLM-learning/evaluate/squad_v2.py" if squad_v2 else "/root/projects/LLM-learning/evaluate/squad.py")

# # å®šä¹‰è®¡ç®—å‡½æ•°ï¼ˆå¤„ç†æ¨¡å‹è¾“å‡ºï¼‰
# def compute_metrics(eval_pred):
#     predictions, labels = eval_pred
#     predictions = np.argmax(predictions, axis=1)  # åˆ†ç±»ä»»åŠ¡å–æœ€å¤§æ¦‚ç‡ç±»åˆ«
#     return squad_metric.compute(
#         predictions=predictions, 
#         references=labels,
#         average="macro"  # "micro"ï¼ˆå…¨å±€ç»Ÿè®¡ï¼‰ã€"macro"ï¼ˆç±»åˆ«å¹³å‡ï¼‰ã€"weighted"ï¼ˆåŠ æƒå¹³å‡ï¼‰
#     )

```

#### è®­ç»ƒè¶…å‚æ•°ï¼ˆTrainingArgumentsï¼‰


```python
batch_size=64
model_dir = f"models/{model_checkpoint}-finetuned-squad"

args = TrainingArguments(
    output_dir=model_dir,  # æ¨¡å‹/æ—¥å¿—ä¿å­˜è·¯å¾„
    evaluation_strategy = "epoch",  # æ¯ä¸ªepochåè¯„ä¼°ï¼ˆå¯é€‰"steps"æŒ‰æ­¥è¯„ä¼°ï¼‰
    learning_rate=2e-5,  # ç»å…¸å¾®è°ƒå­¦ä¹ ç‡ï¼ˆé¢„è®­ç»ƒæ¨¡å‹çš„å…¸å‹å­¦ä¹ ç‡èŒƒå›´ï¼š1e-5~5e-5ï¼‰
    per_device_train_batch_size=batch_size,  # æ¯ä¸ªGPUçš„è®­ç»ƒæ‰¹æ¬¡ï¼ˆæ€»batch_size = è¯¥å€¼ * GPUæ•°é‡ï¼‰
    per_device_eval_batch_size=batch_size,   # æ¯ä¸ªGPUçš„è¯„ä¼°æ‰¹æ¬¡ï¼ˆå¯å¤§äºè®­ç»ƒbatch_sizeï¼‰
    num_train_epochs=3,  # è®­ç»ƒè½®æ¬¡ï¼ˆSQuADç­‰ä¸­å‹æ•°æ®é›†å¸¸ç”¨2-5è½®ï¼‰
    weight_decay=0.01,  # L2æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¸¸ç”¨0.01-0.1ï¼‰
    fp16=True,  # å¯ç”¨FP16æ··åˆç²¾åº¦
    # save_strategy="epoch",       # æ¯ä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
    # load_best_model_at_end=True, # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹
)
```

#### Data Collatorï¼ˆæ•°æ®æ•´ç†å™¨ï¼‰

æ•°æ®æ•´ç†å™¨å°†è®­ç»ƒæ•°æ®æ•´ç†ä¸ºæ‰¹æ¬¡æ•°æ®ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒæ—¶çš„æ‰¹æ¬¡å¤„ç†ã€‚æœ¬æ•™ç¨‹ä½¿ç”¨é»˜è®¤çš„ `default_data_collator`ã€‚



```python
from transformers import default_data_collator

data_collator = default_data_collator
```

### å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼ˆTrainerï¼‰

ä¸ºäº†å‡å°‘è®­ç»ƒæ—¶é—´ï¼ˆéœ€è¦å¤§é‡ç®—åŠ›æ”¯æŒï¼‰ï¼Œæˆ‘ä»¬ä¸åœ¨æœ¬æ•™ç¨‹çš„è®­ç»ƒæ¨¡å‹è¿‡ç¨‹ä¸­è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€‚

è€Œæ˜¯è®­ç»ƒå®Œæˆåï¼Œå†ç‹¬ç«‹è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚


```python
trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

    Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.


#### GPU ä½¿ç”¨æƒ…å†µ

è®­ç»ƒæ•°æ®ä¸æ¨¡å‹é…ç½®ï¼š

- SQUAD v1.1
- model_checkpoint = "distilbert-base-uncased"
- batch_size = 64

NVIDIA GPU ä½¿ç”¨æƒ…å†µï¼š

```shell
Every 5.0s: nvidia-smi                                                                                                                                 deepseek-r1-t4-test: Wed Mar 12 17:52:30 2025

Wed Mar 12 17:52:30 2025
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       On  | 00000000:00:07.0 Off |                    0 |
| N/A   53C    P0              63W /  70W |  10945MiB / 15360MiB |    100%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A   1271234      C   /root/miniconda3/envs/peft/bin/python     10942MiB |
+---------------------------------------------------------------------------------------+
```


```python
trainer.train()
```



    <div>

      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [4152/4152 49:18, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1.498200</td>
      <td>1.273643</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.121800</td>
      <td>1.185791</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.985200</td>
      <td>1.167942</td>
    </tr>
  </tbody>
</table><p>





    TrainOutput(global_step=4152, training_loss=1.3124258081807336, metrics={'train_runtime': 2959.0362, 'train_samples_per_second': 89.749, 'train_steps_per_second': 1.403, 'total_flos': 2.602335381127373e+16, 'train_loss': 1.3124258081807336, 'epoch': 3.0})



### è®­ç»ƒå®Œæˆåï¼Œç¬¬ä¸€æ—¶é—´ä¿å­˜æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚


```python
model_to_save = trainer.save_model(model_dir)
```

## æ¨¡å‹è¯„ä¼°

**è¯„ä¼°æ¨¡å‹è¾“å‡ºéœ€è¦ä¸€äº›é¢å¤–çš„å¤„ç†ï¼šå°†æ¨¡å‹çš„é¢„æµ‹æ˜ å°„å›ä¸Šä¸‹æ–‡çš„éƒ¨åˆ†ã€‚**

æ¨¡å‹ç›´æ¥è¾“å‡ºçš„æ˜¯é¢„æµ‹ç­”æ¡ˆçš„`èµ·å§‹ä½ç½®`å’Œ`ç»“æŸä½ç½®`çš„**logits**


```python
import torch

for batch in trainer.get_eval_dataloader():
    break
batch = {k: v.to(trainer.args.device) for k, v in batch.items()}
with torch.no_grad():
    output = trainer.model(**batch)
output.keys()
```




    odict_keys(['loss', 'start_logits', 'end_logits'])



æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç±»ä¼¼å­—å…¸çš„å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æŸå¤±ï¼ˆå› ä¸ºæˆ‘ä»¬æä¾›äº†æ ‡ç­¾ï¼‰ï¼Œä»¥åŠèµ·å§‹å’Œç»“æŸlogitsã€‚æˆ‘ä»¬ä¸éœ€è¦æŸå¤±æ¥è¿›è¡Œé¢„æµ‹ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹logitsï¼š


```python
output.start_logits.shape, output.end_logits.shape
```




    (torch.Size([64, 384]), torch.Size([64, 384]))




```python
output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)
```




    (tensor([ 46,  57,  78,  43, 118, 108,  72,  35, 108,  34,  73,  41,  80,  91,
             156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  41,  35,  42,  77,
              11,  44,  27, 133,  66,  40,  87,  44,  43,  83, 127,  26,  28,  33,
              87, 127,  95,  25,  43, 132,  42,  29,  44,  46,  24,  44,  65,  58,
              81,  14,  59,  72,  25,  36,  55,  43], device='cuda:0'),
     tensor([ 47,  58,  81,  44, 118, 109,  75,  37, 109,  36,  76,  42,  83,  94,
             158,  35,  83,  94,  83,  60,  80,  31,  43,  54,  42,  35,  43,  80,
              13,  45,  28, 133,  66,  41,  89,  45,  44,  85, 127,  27,  30,  34,
              89, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,
              81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))



#### å¦‚ä½•ä»æ¨¡å‹è¾“å‡ºçš„ä½ç½® logit ç»„åˆæˆç­”æ¡ˆ

æˆ‘ä»¬æœ‰æ¯ä¸ªç‰¹å¾å’Œæ¯ä¸ªæ ‡è®°çš„logitã€‚åœ¨æ¯ä¸ªç‰¹å¾ä¸­ä¸ºæ¯ä¸ªæ ‡è®°é¢„æµ‹ç­”æ¡ˆæœ€æ˜æ˜¾çš„æ–¹æ³•æ˜¯ï¼Œå°†èµ·å§‹logitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºèµ·å§‹ä½ç½®ï¼Œå°†ç»“æŸlogitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºç»“æŸä½ç½®ã€‚

åœ¨è®¸å¤šæƒ…å†µä¸‹è¿™ç§æ–¹å¼æ•ˆæœå¾ˆå¥½ï¼Œä½†æ˜¯å¦‚æœæ­¤é¢„æµ‹ç»™å‡ºäº†ä¸å¯èƒ½çš„ç»“æœè¯¥æ€ä¹ˆåŠï¼Ÿæ¯”å¦‚ï¼šèµ·å§‹ä½ç½®å¯èƒ½å¤§äºç»“æŸä½ç½®ï¼Œæˆ–è€…æŒ‡å‘é—®é¢˜ä¸­çš„æ–‡æœ¬ç‰‡æ®µè€Œä¸æ˜¯ç­”æ¡ˆã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æŸ¥çœ‹ç¬¬äºŒå¥½çš„é¢„æµ‹ï¼Œçœ‹å®ƒæ˜¯å¦ç»™å‡ºäº†ä¸€ä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼Œå¹¶é€‰æ‹©å®ƒã€‚

é€‰æ‹©ç¬¬äºŒå¥½çš„ç­”æ¡ˆå¹¶ä¸åƒé€‰æ‹©æœ€ä½³ç­”æ¡ˆé‚£ä¹ˆå®¹æ˜“ï¼š
- å®ƒæ˜¯èµ·å§‹logitsä¸­ç¬¬äºŒä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­æœ€ä½³ç´¢å¼•å—ï¼Ÿ
- è¿˜æ˜¯èµ·å§‹logitsä¸­æœ€ä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­ç¬¬äºŒä½³ç´¢å¼•ï¼Ÿ
- å¦‚æœç¬¬äºŒå¥½çš„ç­”æ¡ˆä¹Ÿä¸å¯èƒ½ï¼Œé‚£ä¹ˆå¯¹äºç¬¬ä¸‰å¥½çš„ç­”æ¡ˆï¼Œæƒ…å†µä¼šæ›´åŠ æ£˜æ‰‹ã€‚

ä¸ºäº†å¯¹ç­”æ¡ˆè¿›è¡Œåˆ†ç±»ï¼Œ
1. å°†ä½¿ç”¨é€šè¿‡æ·»åŠ èµ·å§‹å’Œç»“æŸlogitsè·å¾—çš„åˆ†æ•°
1. è®¾è®¡ä¸€ä¸ªåä¸º`n_best_size`çš„è¶…å‚æ•°ï¼Œé™åˆ¶ä¸å¯¹æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆè¿›è¡Œæ’åºã€‚
1. æˆ‘ä»¬å°†é€‰æ‹©èµ·å§‹å’Œç»“æŸlogitsä¸­çš„æœ€ä½³ç´¢å¼•ï¼Œå¹¶æ”¶é›†è¿™äº›é¢„æµ‹çš„æ‰€æœ‰ç­”æ¡ˆã€‚
1. åœ¨æ£€æŸ¥æ¯ä¸€ä¸ªæ˜¯å¦æœ‰æ•ˆåï¼Œæˆ‘ä»¬å°†æŒ‰ç…§å…¶åˆ†æ•°å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œå¹¶ä¿ç•™æœ€ä½³çš„ç­”æ¡ˆã€‚

ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šæ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ï¼š


```python
n_best_size = 20
```


```python
import numpy as np

start_logits = output.start_logits[0].cpu().numpy()
end_logits = output.end_logits[0].cpu().numpy()

# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š
start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()

valid_answers = []

# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ
for start_index in start_indexes:
    for end_index in end_indexes:
        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
            valid_answers.append(
                {
                    "score": start_logits[start_index] + end_logits[end_index],
                    "text": ""  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²
                }
            )

```


ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å®ƒä»¬çš„å¾—åˆ†å¯¹`valid_answers`è¿›è¡Œæ’åºï¼Œå¹¶ä»…ä¿ç•™æœ€ä½³ç­”æ¡ˆã€‚å”¯ä¸€å‰©ä¸‹çš„é—®é¢˜æ˜¯å¦‚ä½•æ£€æŸ¥ç»™å®šçš„è·¨åº¦æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼ˆè€Œä¸æ˜¯é—®é¢˜ä¸­ï¼‰ï¼Œä»¥åŠå¦‚ä½•è·å–å…¶ä¸­çš„æ–‡æœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‘æˆ‘ä»¬çš„éªŒè¯ç‰¹å¾æ·»åŠ ä¸¤ä¸ªå†…å®¹ï¼š

- ç”Ÿæˆè¯¥ç‰¹å¾çš„ç¤ºä¾‹çš„IDï¼ˆå› ä¸ºæ¯ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œå¦‚å‰æ‰€ç¤ºï¼‰ï¼›
- åç§»æ˜ å°„ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›ä»æ ‡è®°ç´¢å¼•åˆ°ä¸Šä¸‹æ–‡ä¸­å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ç¨å¾®ä¸åŒäº`prepare_train_features`æ¥é‡æ–°å¤„ç†éªŒè¯é›†ï¼š


```python
def prepare_validation_features(examples):
    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚
    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½
    examples["question"] = [q.lstrip() for q in examples["question"]]

    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ
    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚
    tokenized_examples = tokenizer(
        examples["question" if pad_on_right else "context"],
        examples["context" if pad_on_right else "question"],
        truncation="only_second" if pad_on_right else "only_first",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚
    tokenized_examples["example_id"] = []

    for i in range(len(tokenized_examples["input_ids"])):
        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚
        sequence_ids = tokenized_examples.sequence_ids(i)
        context_index = 1 if pad_on_right else 0

        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚
        sample_index = sample_mapping[i]
        tokenized_examples["example_id"].append(examples["id"][sample_index])

        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚
        tokenized_examples["offset_mapping"][i] = [
            (o if sequence_ids[k] == context_index else None)
            for k, o in enumerate(tokenized_examples["offset_mapping"][i])
        ]

    return tokenized_examples

```

å°†`prepare_validation_features`åº”ç”¨åˆ°æ•´ä¸ªéªŒè¯é›†ï¼š


```python
validation_features = datasets["validation"].map(
    prepare_validation_features,
    batched=True,
    remove_columns=datasets["validation"].column_names
)
```


    Map:   0%|          | 0/10570 [00:00<?, ? examples/s]


Now we can grab the predictions for all features by using the `Trainer.predict` method:


```python
raw_predictions = trainer.predict(validation_features)
```





`Trainer`ä¼šéšè—æ¨¡å‹ä¸ä½¿ç”¨çš„åˆ—ï¼ˆåœ¨è¿™é‡Œæ˜¯`example_id`å’Œ`offset_mapping`ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬è¿›è¡Œåå¤„ç†ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬é‡æ–°è®¾ç½®å›æ¥ï¼š


```python
validation_features.set_format(type=validation_features.format["type"], columns=list(validation_features.features.keys()))
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹è¿›ä¹‹å‰çš„æµ‹è¯•ï¼š

ç”±äºåœ¨åç§»æ˜ å°„ä¸­ï¼Œå½“å®ƒå¯¹åº”äºé—®é¢˜çš„ä¸€éƒ¨åˆ†æ—¶ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºNoneï¼Œå› æ­¤å¯ä»¥è½»æ¾æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å®Œå…¨åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä»è€ƒè™‘ä¸­æ’é™¤éå¸¸é•¿çš„ç­”æ¡ˆï¼ˆå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ï¼‰ã€‚

å±•å¼€è¯´ä¸‹å…·ä½“å®ç°ï¼š
- é¦–å…ˆä»æ¨¡å‹è¾“å‡ºä¸­è·å–èµ·å§‹å’Œç»“æŸçš„é€»è¾‘å€¼ï¼ˆlogitsï¼‰ï¼Œè¿™äº›å€¼è¡¨æ˜ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­å¯èƒ½å¼€å§‹å’Œç»“æŸçš„ä½ç½®ã€‚
- ç„¶åï¼Œå®ƒä½¿ç”¨åç§»æ˜ å°„ï¼ˆoffset_mappingï¼‰æ¥æ‰¾åˆ°è¿™äº›é€»è¾‘å€¼åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å…·ä½“ä½ç½®ã€‚
- æ¥ä¸‹æ¥ï¼Œä»£ç éå†å¯èƒ½çš„å¼€å§‹å’Œç»“æŸç´¢å¼•ç»„åˆï¼Œæ’é™¤é‚£äº›ä¸åœ¨ä¸Šä¸‹æ–‡èŒƒå›´å†…æˆ–é•¿åº¦ä¸åˆé€‚çš„ç­”æ¡ˆã€‚
- å¯¹äºæœ‰æ•ˆçš„ç­”æ¡ˆï¼Œå®ƒè®¡ç®—å‡ºä¸€ä¸ªåˆ†æ•°ï¼ˆåŸºäºå¼€å§‹å’Œç»“æŸé€»è¾‘å€¼çš„å’Œï¼‰ï¼Œå¹¶å°†ç­”æ¡ˆåŠå…¶åˆ†æ•°å­˜å‚¨èµ·æ¥ã€‚
- æœ€åï¼Œå®ƒæ ¹æ®åˆ†æ•°å¯¹ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œå¹¶è¿”å›å¾—åˆ†æœ€é«˜çš„å‡ ä¸ªç­”æ¡ˆã€‚


```python
max_answer_length = 30
```


```python
start_logits = output.start_logits[0].cpu().numpy()
end_logits = output.end_logits[0].cpu().numpy()
offset_mapping = validation_features[0]["offset_mapping"]

# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•
context = datasets["validation"][0]["context"]

# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š
start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
valid_answers = []
for start_index in start_indexes:
    for end_index in end_indexes:
        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚
        if (
            start_index >= len(offset_mapping)
            or end_index >= len(offset_mapping)
            or offset_mapping[start_index] is None
            or offset_mapping[end_index] is None
        ):
            continue
        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚
        if end_index < start_index or end_index - start_index + 1 > max_answer_length:
            continue
        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
            start_char = offset_mapping[start_index][0]
            end_char = offset_mapping[end_index][1]
            valid_answers.append(
                {
                    "score": start_logits[start_index] + end_logits[end_index],
                    "text": context[start_char: end_char]
                }
            )

valid_answers = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[:n_best_size]
valid_answers

```




    [{'score': 15.2265625, 'text': 'Denver Broncos'},
     {'score': 13.082031,
      'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 11.640625, 'text': 'Carolina Panthers'},
     {'score': 11.4296875, 'text': 'Broncos'},
     {'score': 11.277344,
      'text': 'American Football Conference (AFC) champion Denver Broncos'},
     {'score': 10.154297,
      'text': 'The American Football Conference (AFC) champion Denver Broncos'},
     {'score': 10.123047, 'text': 'Denver'},
     {'score': 9.285156,
      'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 9.1328125,
      'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 8.009766,
      'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 8.008057,
      'text': 'Denver Broncos defeated the National Football Conference'},
     {'score': 7.694336,
      'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},
     {'score': 7.317871,
      'text': 'Denver Broncos defeated the National Football Conference (NFC'},
     {'score': 7.251465,
      'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title.'},
     {'score': 7.1833496,
      'text': 'Denver Broncos defeated the National Football Conference (NFC)'},
     {'score': 6.987793, 'text': 'AFC) champion Denver Broncos'},
     {'score': 6.864746, 'text': 'champion Denver Broncos'},
     {'score': 6.614258,
      'text': 'National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 6.426758, 'text': 'Panthers'},
     {'score': 6.2529297, 'text': 'Carolina'}]



æ‰“å°æ¯”è¾ƒæ¨¡å‹è¾“å‡ºå’Œæ ‡å‡†ç­”æ¡ˆï¼ˆGround-truthï¼‰æ˜¯å¦ä¸€è‡´:


```python
datasets["validation"][0]["answers"]
```




    {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],
     'answer_start': [177, 177, 177]}



**æ¨¡å‹æœ€é«˜æ¦‚ç‡çš„è¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´**

æ­£å¦‚ä¸Šé¢çš„ä»£ç æ‰€ç¤ºï¼Œè¿™åœ¨ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šå¾ˆå®¹æ˜“ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å®ƒæ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚

å¯¹äºå…¶ä»–ç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªç¤ºä¾‹ä¸å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„å…³ç³»ã€‚

æ­¤å¤–ï¼Œç”±äºä¸€ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å°†ç”±ç»™å®šç¤ºä¾‹ç”Ÿæˆçš„æ‰€æœ‰ç‰¹å¾ä¸­çš„æ‰€æœ‰ç­”æ¡ˆæ±‡é›†åœ¨ä¸€èµ·ï¼Œç„¶åé€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚

ä¸‹é¢çš„ä»£ç æ„å»ºäº†ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•åˆ°å…¶å¯¹åº”ç‰¹å¾ç´¢å¼•çš„æ˜ å°„å…³ç³»ï¼š


```python
import collections

examples = datasets["validation"]
features = validation_features

example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
features_per_example = collections.defaultdict(list)
for i, feature in enumerate(features):
    features_per_example[example_id_to_index[feature["example_id"]]].append(i)
```

å½“`squad_v2 = True`æ—¶ï¼Œæœ‰ä¸€å®šæ¦‚ç‡å‡ºç°ä¸å¯èƒ½çš„ç­”æ¡ˆï¼ˆimpossible answer)ã€‚

ä¸Šé¢çš„ä»£ç ä»…ä¿ç•™åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬è¿˜éœ€è¦è·å–ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°ï¼ˆå…¶èµ·å§‹å’Œç»“æŸç´¢å¼•å¯¹åº”äºCLSæ ‡è®°çš„ç´¢å¼•ï¼‰ã€‚

å½“ä¸€ä¸ªç¤ºä¾‹ç”Ÿæˆå¤šä¸ªç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ‰€æœ‰ç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆéƒ½é¢„æµ‹å‡ºç°ä¸å¯èƒ½ç­”æ¡ˆæ—¶ï¼ˆå› ä¸ºä¸€ä¸ªç‰¹å¾å¯èƒ½ä¹‹æ‰€ä»¥èƒ½å¤Ÿé¢„æµ‹å‡ºä¸å¯èƒ½ç­”æ¡ˆï¼Œæ˜¯å› ä¸ºç­”æ¡ˆä¸åœ¨å®ƒå¯ä»¥è®¿é—®çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼‰ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸€ä¸ªç¤ºä¾‹ä¸­ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°æ˜¯è¯¥ç¤ºä¾‹ç”Ÿæˆçš„æ¯ä¸ªç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°çš„æœ€å°å€¼ã€‚


```python
from tqdm.auto import tqdm

def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):
    all_start_logits, all_end_logits = raw_predictions
    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚
    example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
    features_per_example = collections.defaultdict(list)
    for i, feature in enumerate(features):
        features_per_example[example_id_to_index[feature["example_id"]]].append(i)

    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚
    predictions = collections.OrderedDict()

    # æ—¥å¿—è®°å½•ã€‚
    print(f"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚")

    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼
    for example_index, example in enumerate(tqdm(examples)):
        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚
        feature_indices = features_per_example[example_index]

        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚
        valid_answers = []
        
        context = example["context"]
        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚
        for feature_index in feature_indices:
            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚
            start_logits = all_start_logits[feature_index]
            end_logits = all_end_logits[feature_index]
            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚
            offset_mapping = features[feature_index]["offset_mapping"]

            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚
            cls_index = features[feature_index]["input_ids"].index(tokenizer.cls_token_id)
            feature_null_score = start_logits[cls_index] + end_logits[cls_index]
            if min_null_score is None or min_null_score < feature_null_score:
                min_null_score = feature_null_score

            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚
            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚
                    if (
                        start_index >= len(offset_mapping)
                        or end_index >= len(offset_mapping)
                        or offset_mapping[start_index] is None
                        or offset_mapping[end_index] is None
                    ):
                        continue
                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚
                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:
                        continue

                    start_char = offset_mapping[start_index][0]
                    end_char = offset_mapping[end_index][1]
                    valid_answers.append(
                        {
                            "score": start_logits[start_index] + end_logits[end_index],
                            "text": context[start_char: end_char]
                        }
                    )
        
        if len(valid_answers) > 0:
            best_answer = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[0]
        else:
            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚
            best_answer = {"text": "", "score": 0.0}
        
        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰
        if not squad_v2:
            predictions[example["id"]] = best_answer["text"]
        else:
            answer = best_answer["text"] if best_answer["score"] > min_null_score else ""
            predictions[example["id"]] = answer

    return predictions

```

åœ¨åŸå§‹ç»“æœä¸Šåº”ç”¨åå¤„ç†é—®ç­”ç»“æœï¼š


```python
final_predictions = postprocess_qa_predictions(datasets["validation"], validation_features, raw_predictions.predictions)
```

    æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚



      0%|          | 0/10570 [00:00<?, ?it/s]


ä½¿ç”¨ `datasets.load_metric` ä¸­åŠ è½½ `SQuAD v2` çš„è¯„ä¼°æŒ‡æ ‡


```python
from datasets import load_metric

metric = load_metric("squad_v2" if squad_v2 else "squad", trust_remote_code=True)
```


    Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]



    Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]


æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°è¿›è¡Œè¯„ä¼°ã€‚

åªéœ€ç¨å¾®è°ƒæ•´ä¸€ä¸‹é¢„æµ‹å’Œæ ‡ç­¾çš„æ ¼å¼ï¼Œå› ä¸ºå®ƒæœŸæœ›çš„æ˜¯ä¸€ç³»åˆ—å­—å…¸è€Œä¸æ˜¯ä¸€ä¸ªå¤§å­—å…¸ã€‚

åœ¨ä½¿ç”¨`squad_v2`æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¾ç½®`no_answer_probability`å‚æ•°ï¼ˆæˆ‘ä»¬åœ¨è¿™é‡Œå°†å…¶è®¾ç½®ä¸º0.0ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬é€‰æ‹©äº†ç­”æ¡ˆï¼Œæˆ‘ä»¬å·²ç»å°†ç­”æ¡ˆè®¾ç½®ä¸ºç©ºï¼‰ã€‚


```python
if squad_v2:
    formatted_predictions = [{"id": k, "prediction_text": v, "no_answer_probability": 0.0} for k, v in final_predictions.items()]
else:
    formatted_predictions = [{"id": k, "prediction_text": v} for k, v in final_predictions.items()]
references = [{"id": ex["id"], "answers": ex["answers"]} for ex in datasets["validation"]]
metric.compute(predictions=formatted_predictions, references=references)
```




    {'exact_match': 74.33301797540209, 'f1': 83.26051790761488}




```python

```

### åŠ è½½æœ¬åœ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¿›è¡Œè¯„ä¼°å’Œå†è®­ç»ƒæ›´é«˜çš„ F1 Score


```python
trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)
```


```python
trained_trainer = Trainer(
    trained_model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

    Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.



```python
trained_trainer.train()
```



    <div>

      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [4152/4152 49:39, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.846900</td>
      <td>1.212233</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.713500</td>
      <td>1.232366</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.686100</td>
      <td>1.247295</td>
    </tr>
  </tbody>
</table><p>


    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3500 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-4000 already exists and is non-empty.Saving will proceed but saved results may be invalid.





    TrainOutput(global_step=4152, training_loss=0.7533242697890324, metrics={'train_runtime': 2980.1413, 'train_samples_per_second': 89.114, 'train_steps_per_second': 1.393, 'total_flos': 2.602335381127373e+16, 'train_loss': 0.7533242697890324, 'epoch': 3.0})




```python
model_to_save_2 = trained_trainer.save_model(f"{model_dir}-2")
```


```python
import torch

for batch in trained_trainer.get_eval_dataloader():
    break
batch = {k: v.to(trained_trainer.args.device) for k, v in batch.items()}
with torch.no_grad():
    output = trained_trainer.model(**batch)
output.keys()
```




    odict_keys(['loss', 'start_logits', 'end_logits'])




```python
output.start_logits.shape, output.end_logits.shape
```




    (torch.Size([64, 384]), torch.Size([64, 384]))




```python
output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)
```




    (tensor([ 46,  57,  78,  54, 118, 107,  72,  35, 107,  34,  73,  52,  80,  91,
             156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  52,  35,  53,  77,
              11,  44,  27, 133,  66,  40,  87,  44,  43,  41, 127,  26,  28,  33,
              87, 127,  95,  25,  43, 132,  42,  29,  44,  46,  24,  44,  65,  58,
              81,  14,  59,  72,  25,  36,  57,  43], device='cuda:0'),
     tensor([ 47,  58,  81,  44, 118, 110,  75,  37, 110,  36,  76,  42,  83,  94,
             158,  35,  83,  94,  83,  60,  80,  31,  43,  54,  42,  35,  43,  91,
              13,  45,  28, 133,  66,  41,  89,  45,  44,  42, 127,  27,  30,  34,
              90, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,
              81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))




```python
n_best_size = 20
```


```python
import numpy as np

start_logits = output.start_logits[0].cpu().numpy()
end_logits = output.end_logits[0].cpu().numpy()

# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š
start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()

valid_answers = []

# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ
for start_index in start_indexes:
    for end_index in end_indexes:
        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
            valid_answers.append(
                {
                    "score": start_logits[start_index] + end_logits[end_index],
                    "text": ""  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²
                }
            )

```


```python
validation_features = datasets["validation"].map(
    prepare_validation_features,
    batched=True,
    remove_columns=datasets["validation"].column_names
)
```


```python
raw_predictions = trained_trainer.predict(validation_features)
```






```python
validation_features.set_format(type=validation_features.format["type"], columns=list(validation_features.features.keys()))
```


```python
max_answer_length = 30
```


```python
start_logits = output.start_logits[0].cpu().numpy()
end_logits = output.end_logits[0].cpu().numpy()
offset_mapping = validation_features[0]["offset_mapping"]

# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•
context = datasets["validation"][0]["context"]

# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š
start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
valid_answers = []
for start_index in start_indexes:
    for end_index in end_indexes:
        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚
        if (
            start_index >= len(offset_mapping)
            or end_index >= len(offset_mapping)
            or offset_mapping[start_index] is None
            or offset_mapping[end_index] is None
        ):
            continue
        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚
        if end_index < start_index or end_index - start_index + 1 > max_answer_length:
            continue
        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
            start_char = offset_mapping[start_index][0]
            end_char = offset_mapping[end_index][1]
            valid_answers.append(
                {
                    "score": start_logits[start_index] + end_logits[end_index],
                    "text": context[start_char: end_char]
                }
            )

valid_answers = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[:n_best_size]
valid_answers

```




    [{'score': 18.515625, 'text': 'Denver Broncos'},
     {'score': 15.769531,
      'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 14.910156, 'text': 'Broncos'},
     {'score': 13.9296875, 'text': 'Carolina Panthers'},
     {'score': 13.019531,
      'text': 'The American Football Conference (AFC) champion Denver Broncos'},
     {'score': 12.859375,
      'text': 'American Football Conference (AFC) champion Denver Broncos'},
     {'score': 12.537109, 'text': 'Denver'},
     {'score': 12.1640625,
      'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 10.9296875,
      'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title.'},
     {'score': 10.2734375,
      'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 10.113281,
      'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 9.332031,
      'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},
     {'score': 9.089844,
      'text': 'Carolina Panthers 24â€“10 to earn their third Super Bowl title.'},
     {'score': 8.664551, 'text': 'AFC) champion Denver Broncos'},
     {'score': 8.66333,
      'text': 'Denver Broncos defeated the National Football Conference'},
     {'score': 8.26416,
      'text': 'Denver Broncos defeated the National Football Conference (NFC)'},
     {'score': 7.742676, 'text': 'Panthers'},
     {'score': 7.600586,
      'text': 'Denver Broncos defeated the National Football Conference (NFC'},
     {'score': 7.4921875, 'text': 'Carolina'},
     {'score': 7.3242188,
      'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title.'}]




```python
datasets["validation"][0]["answers"]
```




    {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],
     'answer_start': [177, 177, 177]}




```python
import collections

examples = datasets["validation"]
features = validation_features

example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
features_per_example = collections.defaultdict(list)
for i, feature in enumerate(features):
    features_per_example[example_id_to_index[feature["example_id"]]].append(i)
```


```python
final_predictions = postprocess_qa_predictions(datasets["validation"], validation_features, raw_predictions.predictions)
```

    æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚



      0%|          | 0/10570 [00:00<?, ?it/s]



```python
from datasets import load_metric

metric = load_metric("squad_v2" if squad_v2 else "squad", trust_remote_code=True)
```


```python
if squad_v2:
    formatted_predictions = [{"id": k, "prediction_text": v, "no_answer_probability": 0.0} for k, v in final_predictions.items()]
else:
    formatted_predictions = [{"id": k, "prediction_text": v} for k, v in final_predictions.items()]
references = [{"id": ex["id"], "answers": ex["answers"]} for ex in datasets["validation"]]
metric.compute(predictions=formatted_predictions, references=references)
```




    {'exact_match': 74.91958372753075, 'f1': 83.83155050300782}




```python

```
