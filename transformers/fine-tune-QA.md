# Hugging Face Transformers å¾®è°ƒè¯­è¨€æ¨¡å‹-é—®ç­”ä»»åŠ¡

æˆ‘ä»¬å·²ç»å­¦ä¼šä½¿ç”¨ Pipeline åŠ è½½æ”¯æŒé—®ç­”ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ¬æ•™ç¨‹ä»£ç å°†å±•ç¤ºå¦‚ä½•å¾®è°ƒè®­ç»ƒä¸€ä¸ªæ”¯æŒé—®ç­”ä»»åŠ¡çš„æ¨¡å‹ã€‚

**æ³¨æ„ï¼šå¾®è°ƒåçš„æ¨¡å‹ä»ç„¶æ˜¯é€šè¿‡æå–ä¸Šä¸‹æ–‡çš„å­ä¸²æ¥å›ç­”é—®é¢˜çš„ï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚**

### æ¨¡å‹æ‰§è¡Œé—®ç­”æ•ˆæœç¤ºä¾‹

![Widget inference representing the QA task](docs/images/question_answering.png)


```python
# æ ¹æ®ä½ ä½¿ç”¨çš„æ¨¡å‹å’ŒGPUèµ„æºæƒ…å†µï¼Œè°ƒæ•´ä»¥ä¸‹å…³é”®å‚æ•°
squad_v2 = True
model_checkpoint = "distilbert-base-uncased"
batch_size = 16
```

## ä¸‹è½½æ•°æ®é›†

åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuADï¼‰](https://rajpurkar.github.io/SQuAD-explorer/)ã€‚

### SQuAD æ•°æ®é›†

**æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuAD)** æ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±ä¼—åŒ…å·¥ä½œè€…åœ¨ä¸€ç³»åˆ—ç»´åŸºç™¾ç§‘æ–‡ç« ä¸Šæå‡ºé—®é¢˜ç»„æˆã€‚æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ç›¸åº”é˜…è¯»æ®µè½ä¸­çš„æ–‡æœ¬ç‰‡æ®µæˆ–èŒƒå›´ï¼Œæˆ–è€…è¯¥é—®é¢˜å¯èƒ½æ— æ³•å›ç­”ã€‚

SQuAD2.0å°†SQuAD1.1ä¸­çš„10ä¸‡ä¸ªé—®é¢˜ä¸ç”±ä¼—åŒ…å·¥ä½œè€…å¯¹æŠ—æ€§åœ°æ’°å†™çš„5ä¸‡å¤šä¸ªæ— æ³•å›ç­”çš„é—®é¢˜ç›¸ç»“åˆï¼Œä½¿å…¶çœ‹èµ·æ¥ä¸å¯å›ç­”çš„é—®é¢˜ç±»ä¼¼ã€‚è¦åœ¨SQuAD2.0ä¸Šè¡¨ç°è‰¯å¥½ï¼Œç³»ç»Ÿä¸ä»…å¿…é¡»åœ¨å¯èƒ½æ—¶å›ç­”é—®é¢˜ï¼Œè¿˜å¿…é¡»ç¡®å®šæ®µè½ä¸­æ²¡æœ‰æ”¯æŒä»»ä½•ç­”æ¡ˆï¼Œå¹¶æ”¾å¼ƒå›ç­”ã€‚


```python
from datasets import load_dataset
```


```python
datasets = load_dataset("squad_v2" if squad_v2 else "squad")
```

The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set.


```python
datasets
```




    DatasetDict({
        train: Dataset({
            features: ['id', 'title', 'context', 'question', 'answers'],
            num_rows: 130319
        })
        validation: Dataset({
            features: ['id', 'title', 'context', 'question', 'answers'],
            num_rows: 11873
        })
    })



#### å¯¹æ¯”æ•°æ®é›†

ç›¸æ¯”å¿«é€Ÿå…¥é—¨ä½¿ç”¨çš„ Yelp è¯„è®ºæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° SQuAD è®­ç»ƒå’Œæµ‹è¯•é›†éƒ½æ–°å¢äº†ç”¨äºä¸Šä¸‹æ–‡ã€é—®é¢˜ä»¥åŠé—®é¢˜ç­”æ¡ˆçš„åˆ—ï¼š

**YelpReviewFull Datasetï¼š**

```json

DatasetDict({
    train: Dataset({
        features: ['label', 'text'],
        num_rows: 650000
    })
    test: Dataset({
        features: ['label', 'text'],
        num_rows: 50000
    })
})
```


```python
datasets["train"][0]
```




    {'id': '5733be284776f41900661182',
     'title': 'University_of_Notre_Dame',
     'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',
     'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',
     'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}




```python
datasets["train"][333]
```




    {'id': '56d4d0c32ccc5a1400d83250',
     'title': 'BeyoncÃ©',
     'context': 'BeyoncÃ© is believed to have first started a relationship with Jay Z after a collaboration on "\'03 Bonnie & Clyde", which appeared on his seventh album The Blueprint 2: The Gift & The Curse (2002). BeyoncÃ© appeared as Jay Z\'s girlfriend in the music video for the song, which would further fuel speculation of their relationship. On April 4, 2008, BeyoncÃ© and Jay Z were married without publicity. As of April 2014, the couple have sold a combined 300 million records together. The couple are known for their private relationship, although they have appeared to become more relaxed in recent years. BeyoncÃ© suffered a miscarriage in 2010 or 2011, describing it as "the saddest thing" she had ever endured. She returned to the studio and wrote music in order to cope with the loss. In April 2011, BeyoncÃ© and Jay Z traveled to Paris in order to shoot the album cover for her 4, and unexpectedly became pregnant in Paris.',
     'question': 'How many records combined have BeyoncÃ© and Jay Z sold?',
     'answers': {'text': ['300 million'], 'answer_start': [447]}}



#### ä»ä¸Šä¸‹æ–‡ä¸­ç»„ç»‡å›å¤å†…å®¹

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç­”æ¡ˆæ˜¯é€šè¿‡å®ƒä»¬åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®ï¼ˆè¿™é‡Œæ˜¯ç¬¬515ä¸ªå­—ç¬¦ï¼‰ä»¥åŠå®ƒä»¬çš„å®Œæ•´æ–‡æœ¬è¡¨ç¤ºçš„ï¼Œè¿™æ˜¯ä¸Šé¢æåˆ°çš„ä¸Šä¸‹æ–‡çš„å­å­—ç¬¦ä¸²ã€‚


```python
from datasets import ClassLabel, Sequence
import random
import pandas as pd
from IPython.display import display, HTML

def show_random_elements(dataset, num_examples=10):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):
            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])
    display(HTML(df.to_html()))
```


```python
show_random_elements(datasets["train"])
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>id</th>
      <th>title</th>
      <th>context</th>
      <th>question</th>
      <th>answers</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>57267a20708984140094c764</td>
      <td>Department_store</td>
      <td>All major cities have their distinctive local department stores, which anchored the downtown shopping district until the arrival of the malls in the 1960s. Washington, for example, after 1887 had Woodward &amp; Lothrop and Garfinckel's starting in 1905. Garfield's went bankrupt in 1990, as did Woodward &amp; Lothrop in 1994. Baltimore had four major department stores: Hutzler's was the prestige leader, followed by Hecht's, Hochschild's and Stewart's. They all operated branches in the suburbs, but all closed in the late twentieth century. By 2015, most locally owned department stores around the country had been consolidated into larger chains, or had closed down entirely.</td>
      <td>In what year did Garfield's go bankrupt?</td>
      <td>{'text': ['1990'], 'answer_start': [278]}</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5ad122d9645df0001a2d0eca</td>
      <td>Labour_Party_(UK)</td>
      <td>Support for the LRC was boosted by the 1901 Taff Vale Case, a dispute between strikers and a railway company that ended with the union being ordered to pay Â£23,000 damages for a strike. The judgement effectively made strikes illegal since employers could recoup the cost of lost business from the unions. The apparent acquiescence of the Conservative Government of Arthur Balfour to industrial and business interests (traditionally the allies of the Liberal Party in opposition to the Conservative's landed interests) intensified support for the LRC against a government that appeared to have little concern for the industrial proletariat and its problems.</td>
      <td>What hurt support for the LRC?</td>
      <td>{'text': [], 'answer_start': []}</td>
    </tr>
    <tr>
      <th>2</th>
      <td>5ace3e6532bba1001ae4a021</td>
      <td>Avicenna</td>
      <td>According to his autobiography, Avicenna had memorised the entire Quran by the age of 10. He learned Indian arithmetic from an Indian greengrocer,Ø¡Mahmoud Massahi and he began to learn more from a wandering scholar who gained a livelihood by curing the sick and teaching the young. He also studied Fiqh (Islamic jurisprudence) under the Sunni Hanafi scholar Ismail al-Zahid. Avicenna was taught some extent of philosophy books such as Introduction (Isagoge)'s Porphyry (philosopher), Euclid's Elements, Ptolemy's Almagest by an unpopular philosopher, Abu Abdullah Nateli, who claimed philosophizing.</td>
      <td>When did Avicenna start studying the Quran?</td>
      <td>{'text': [], 'answer_start': []}</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5725ce2e271a42140099d20d</td>
      <td>Hellenistic_period</td>
      <td>The Odrysian Kingdom was a union of Thracian tribes under the kings of the powerful Odrysian tribe centered around the region of Thrace. Various parts of Thrace were under Macedonian rule under Philip II of Macedon, Alexander the Great, Lysimachus, Ptolemy II, and Philip V but were also often ruled by their own kings. The Thracians and Agrianes were widely used by Alexander as peltasts and light cavalry, forming about one fifth of his army. The Diadochi also used Thracian mercenaries in their armies and they were also used as colonists. The Odrysians used Greek as the language of administration and of the nobility. The nobility also adopted Greek fashions in dress, ornament and military equipment, spreading it to the other tribes. Thracian kings were among the first to be Hellenized.</td>
      <td>Which kings wre among the first to be Hellenized?</td>
      <td>{'text': ['Thracian'], 'answer_start': [741]}</td>
    </tr>
    <tr>
      <th>4</th>
      <td>56d97744dc89441400fdb4cb</td>
      <td>2008_Summer_Olympics_torch_relay</td>
      <td>A Macau resident was arrested on April 26 for posting a message on cyberctm.com encouraging people to disrupt the relay. Both orchidbbs.com and cyberctm.com Internet forums were shut down from May 2 to 4. This fueled speculation that the shutdowns were targeting speeches against the relay. The head of the Bureau of Telecommunications Regulation has denied that the shutdowns of the websites were politically motivated. About 2,200 police were deployed on the streets, there were no interruptions.</td>
      <td>In addition to cyberctm.com, what other website was shut down for two days?</td>
      <td>{'text': ['orchidbbs.com'], 'answer_start': [126]}</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5a68bac68476ee001a58a7cc</td>
      <td>Genocide</td>
      <td>In 2007 the European Court of Human Rights (ECHR), noted in its judgement on Jorgic v. Germany case that in 1992 the majority of legal scholars took the narrow view that "intent to destroy" in the CPPCG meant the intended physical-biological destruction of the protected group and that this was still the majority opinion. But the ECHR also noted that a minority took a broader view and did not consider biological-physical destruction was necessary as the intent to destroy a national, racial, religious or ethnic group was enough to qualify as genocide.</td>
      <td>What former case did the European Court of Human Rights draw on in 2002 to further refine qualifiers of genocide?</td>
      <td>{'text': [], 'answer_start': []}</td>
    </tr>
    <tr>
      <th>6</th>
      <td>5a6fd6388abb0b001a675f9b</td>
      <td>Alsace</td>
      <td>This situation prevailed until 1639, when most of Alsace was conquered by France so as to keep it out of the hands of the Spanish Habsburgs, who wanted a clear road to their valuable and rebellious possessions in the Spanish Netherlands. Beset by enemies and seeking to gain a free hand in Hungary, the Habsburgs sold their Sundgau territory (mostly in Upper Alsace) to France in 1646, which had occupied it, for the sum of 1.2 million Thalers. When hostilities were concluded in 1648 with the Treaty of Westphalia, most of Alsace was recognized as part of France, although some towns remained independent. The treaty stipulations regarding Alsace were complex; although the French king gained sovereignty, existing rights and customs of the inhabitants were largely preserved. France continued to maintain its customs border along the Vosges mountains where it had been, leaving Alsace more economically oriented to neighbouring German-speaking lands. The German language remained in use in local administration, in schools, and at the (Lutheran) University of Strasbourg, which continued to draw students from other German-speaking lands. The 1685 Edict of Fontainebleau, by which the French king ordered the suppression of French Protestantism, was not applied in Alsace. France did endeavour to promote Catholicism; Strasbourg Cathedral, for example, which had been Lutheran from 1524 to 1681, was returned to the Catholic Church. However, compared to the rest of France, Alsace enjoyed a climate of religious tolerance.</td>
      <td>When was Strasbourg Cathedral built?</td>
      <td>{'text': [], 'answer_start': []}</td>
    </tr>
    <tr>
      <th>7</th>
      <td>56dfbd5e231d4119001abd5b</td>
      <td>Internet_service_provider</td>
      <td>ISPs provide Internet access, employing a range of technologies to connect users to their network. Available technologies have ranged from computer modems with acoustic couplers to telephone lines, to television cable (CATV), wireless Ethernet (wi-fi), and fiber optics.</td>
      <td>what was an earlier technology used to connect to the internet?</td>
      <td>{'text': ['telephone lines'], 'answer_start': [182]}</td>
    </tr>
    <tr>
      <th>8</th>
      <td>56d2726659d6e41400145ffa</td>
      <td>Buddhism</td>
      <td>During the period of Late Mahayana Buddhism, four major types of thought developed: Madhyamaka, Yogacara, Tathagatagarbha, and Buddhist Logic as the last and most recent. In India, the two main philosophical schools of the Mahayana were the Madhyamaka and the later Yogacara. According to Dan Lusthaus, Madhyamaka and Yogacara have a great deal in common, and the commonality stems from early Buddhism. There were no great Indian teachers associated with tathagatagarbha thought.</td>
      <td>In India the two main philosophical schools of the Mahayana were Madhyamaka and what else?</td>
      <td>{'text': ['Yogacara'], 'answer_start': [96]}</td>
    </tr>
    <tr>
      <th>9</th>
      <td>56f8f65b9b226e1400dd1202</td>
      <td>Near_East</td>
      <td>The use of the term Middle East as a region of international affairs apparently began in British and American diplomatic circles quite independently of each other over concern for the security of the same country: Iran, then known to the west as Persia. In 1900 Thomas Edward Gordon published an article, The Problem of the Middle East, which began:</td>
      <td>Where did the use of the term Middle East as a region of international affairs begin?</td>
      <td>{'text': ['in British and American diplomatic circles'], 'answer_start': [86]}</td>
    </tr>
  </tbody>
</table>


## é¢„å¤„ç†æ•°æ®

**Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰å°±åƒä¸€ä½ã€Œè¯­è¨€æ‹†è§£ä¸“å®¶ã€**ï¼Œä¸“é—¨å¸®è®¡ç®—æœºç†è§£äººç±»æ–‡å­—ã€‚å®ƒçš„æ ¸å¿ƒä½œç”¨å¯ä»¥ç”¨ä¸‰æ­¥è¯´æ¸…æ¥šï¼š

---

### 1ï¸âƒ£ **æ‹†è§£æ–‡æœ¬**  
æŠŠå¥å­æ‹†æˆ **æ¨¡å‹è®¤è¯†çš„ç‰‡æ®µ**ï¼ˆè¯æˆ–å­è¯ï¼‰ã€‚  
ä¾‹å¦‚ï¼š  
`"æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"` â†’ `["æˆ‘", "çˆ±", "è‡ªç„¶", "è¯­è¨€", "å¤„ç†"]`  
ï¼ˆè‹±æ–‡å¦‚ `"Hugging Face"` â†’ `["Hug", "##ging", "Face"]`ï¼‰

---

### 2ï¸âƒ£ **æ·»åŠ ã€Œæš—å·ã€**  
æ’å…¥æ¨¡å‹éœ€è¦çš„**ç‰¹æ®Šæ ‡è®°**ï¼Œæ¯”å¦‚ï¼š  
- **`[CLS]`**ï¼šå¼€å¤´æ ‡è®°ï¼ˆBERTç”¨ï¼‰  
- **`[SEP]`**ï¼šåˆ†éš”æ ‡è®°ï¼ˆåŒºåˆ†å¥å­ï¼‰  
```python
"ä½ å¥½å—ï¼Ÿ" â†’ ["[CLS]", "ä½ ", "å¥½", "å—", "ï¼Ÿ", "[SEP]"]
```

---

### 3ï¸âƒ£ **è½¬æˆå¯†ç æ•°å­—**  
æŠŠæ¯ä¸ªè¯æ¢æˆ**æ¨¡å‹è¯æ±‡è¡¨é‡Œçš„IDå·**ï¼Œç±»ä¼¼å¯†ç æœ¬ï¼š  
```python
["[CLS]", "ä½ ", "å¥½", "å—"] â†’ [101, 872, 1962, 3221, 102]
```

---

### ğŸŒ° **å®é™…æ•ˆæœç¤ºä¾‹**  
ä½ è¾“å…¥ï¼š`"ä»Šå¤©å¦é—¨å¤©æ°”å¦‚ä½•ï¼Ÿ"`  
Tokenizerå¤„ç†åè¾“å‡ºï¼š  
```python
{
  "input_ids": [101, 791, 1921, 1762, 1377, 1442, 3221, 102],
  "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1]  # æ ‡è®°å“ªäº›æ˜¯æœ‰æ•ˆå†…å®¹
}
```
æ¨¡å‹çœ‹åˆ°è¿™äº›æ•°å­—å°±èƒ½åˆ†æè¯­ä¹‰ï¼Œç”Ÿæˆå›ç­”å•¦ï¼

---

### ğŸ¤– **ä¸åŒæ¨¡å‹çš„å·®å¼‚**  
- **BERTç±»**ï¼šæ‹†è¯è¾ƒç»†ï¼ŒåŠ `[CLS]`/`[SEP]`  
- **GPTç±»**ï¼šæŒ‰å­—èŠ‚æ‹†åˆ†ï¼ŒåŠ `<|endoftext|>`  
- **å¤šè¯­è¨€æ¨¡å‹**ï¼šæ”¯æŒä¸­/è‹±/æ—¥ç­‰æ··åˆæ‹†åˆ†  

ä¸€å¥è¯æ€»ç»“ï¼š**Tokenizerå°±æ˜¯æŠŠäººç±»è¯­è¨€ã€Œç¿»è¯‘ã€æˆAIèƒ½æ‡‚çš„æ•°å­—å¯†ç ï¼** ğŸ˜Š


```python
from transformers import AutoTokenizer
    
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
```

**AutoTokenizer å°±åƒã€Œä¸‡èƒ½é€‚é…å™¨ã€**  
â€”â€”ä½ åªéœ€è¦å‘Šè¯‰å®ƒç”¨å“ªä¸ªAIæ¨¡å‹ï¼ˆæ¯”å¦‚BERTã€GPT-3ï¼‰ï¼Œå®ƒå°±ä¼šè‡ªåŠ¨åŒ¹é…å¯¹åº”çš„æ–‡å­—ç¿»è¯‘è§„åˆ™ã€‚

ä¸¾ä¸ªæ —å­ğŸŒ°ï¼š  
- ä½ æƒ³ç”¨ **BERT** æ¨¡å‹ â†’ å®ƒè‡ªåŠ¨åŠ è½½BERTçš„åˆ†è¯è§„åˆ™  
  ```python
  tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
  ```
- ä½ æƒ³ç”¨ **GPT** æ¨¡å‹ â†’ å®ƒè‡ªåŠ¨åˆ‡æ¢æˆGPTçš„åˆ†è¯æ–¹å¼  
  ```python
  tokenizer = AutoTokenizer.from_pretrained("gpt2")
  ```

**å¥½å¤„**ï¼šä¸ç”¨è®°ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨åå­—ï¼ˆæ¯”å¦‚`BertTokenizer`ã€`GPT2Tokenizer`ï¼‰ï¼Œä¸€ä¸ª`AutoTokenizer`é€šåƒæ‰€æœ‰æ¨¡å‹ï¼Œå°±åƒä¸‡èƒ½å……ç”µå™¨ä¸€æ ·æ–¹ä¾¿ï¼

---

### å¯¹æ¯”ç¤ºä¾‹ï¼ˆæ‰‹åŠ¨ vs è‡ªåŠ¨ï¼‰
| æ–¹å¼          | æ‰‹åŠ¨é€‰æ‹©åˆ†è¯å™¨                   | AutoTokenizer                  |
|---------------|----------------------------------|---------------------------------|
| **BERTæ¨¡å‹**  | `from transformers import BertTokenizer`<br>`tokenizer = BertTokenizer.from_pretrained("bert-base")` | `AutoTokenizer.from_pretrained("bert-base")` |
| **GPTæ¨¡å‹**   | `from transformers import GPT2Tokenizer`<br>`tokenizer = GPT2Tokenizer.from_pretrained("gpt2")` | `AutoTokenizer.from_pretrained("gpt2")` |

---

âš ï¸ **æ³¨æ„**ï¼šåå­—è¦å¯¹ï¼ˆæ¯”å¦‚`bert-base-chinese`ä¸èƒ½å†™æˆ`bert-chinese`ï¼‰ï¼Œå¦åˆ™è¿™ä¸ªä¸‡èƒ½å……ç”µå™¨ä¹Ÿä¼šæ‰¾ä¸åˆ°æ’å£~

ä»¥ä¸‹æ–­è¨€ç¡®ä¿æˆ‘ä»¬çš„ Tokenizers ä½¿ç”¨çš„æ˜¯ FastTokenizerï¼ˆRust å®ç°ï¼Œé€Ÿåº¦å’ŒåŠŸèƒ½æ€§ä¸Šæœ‰ä¸€å®šä¼˜åŠ¿ï¼‰ã€‚


```python
import transformers
assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)
# ç›´æ¥æ‰“å°åˆ¤æ–­ç»“æœ
print("æ˜¯å¦æ˜¯å¿«é€Ÿç‰ˆåˆ†è¯å™¨:", isinstance(tokenizer, transformers.PreTrainedTokenizerFast))

# æˆ–æ›´è¯¦ç»†çš„è¾“å‡º
if isinstance(tokenizer, transformers.PreTrainedTokenizerFast):
    print("âœ… Tokenizer æ˜¯å¿«é€Ÿç‰ˆ (PreTrainedTokenizerFast)")
else:
    print("âŒ Tokenizer æ˜¯æ™®é€šç‰ˆ (PreTrainedTokenizer)")

```

    æ˜¯å¦æ˜¯å¿«é€Ÿç‰ˆåˆ†è¯å™¨: True
    âœ… Tokenizer æ˜¯å¿«é€Ÿç‰ˆ (PreTrainedTokenizerFast)


**PreTrainedTokenizer å°±åƒä¸ªã€Œæ–‡å­—ç¿»è¯‘å®˜ã€**ï¼Œä¸“é—¨å¸® AI æ¨¡å‹å’Œäººç±»æ–‡å­—æ‰“äº¤é“ã€‚  

ä¸¾ä¸ªæ —å­ğŸŒ°ï¼š  
ä½ æƒ³é—® AI "å¦é—¨ä»Šå¤©çƒ­å—ï¼Ÿ"  
â¡ï¸ **ç¿»è¯‘å®˜çš„å·¥ä½œ**ï¼š  
1. æŠŠè¿™å¥è¯åˆ‡æˆå°å—ï¼š`["å¦é—¨", "ä»Šå¤©", "çƒ­", "å—"]`  
2. å·å·åŠ æš—å·ï¼š`[å¼€å¤´æš—å·] å¦é—¨ ä»Šå¤© çƒ­ å— [ç»“å°¾æš—å·]`  
3. è½¬æˆå¯†ç æ•°å­—ï¼š`[101, 2345, 567, 8910, 102]`  

ç„¶å AI å°±èƒ½çœ‹æ‡‚è¿™äº›æ•°å­—å¯†ç ï¼Œç»™å‡ºå›ç­”å•¦ï¼  
ï¼ˆåè¿‡æ¥ä¹Ÿä¼šæŠŠ AI çš„æ•°å­—å¯†ç ç¿»è¯‘æˆäººç±»æ–‡å­—ç»™ä½ çœ‹ï¼‰

`assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)`

è¿™ä¸ªæ­¥éª¤æ˜¯**å¯é€‰çš„å®‰å…¨æ£€æŸ¥**ï¼Œä¸»è¦ä¸ºäº†ç¡®ä¿ä½ åŠ è½½çš„æ˜¯**å¿«é€Ÿç‰ˆåˆ†è¯å™¨ï¼ˆPreTrainedTokenizerFastï¼‰**ï¼Œè€Œä¸æ˜¯æ—§ç‰ˆçš„æ…¢é€Ÿåˆ†è¯å™¨ï¼ˆPreTrainedTokenizerï¼‰ã€‚ä¸æ£€æŸ¥ä¹Ÿèƒ½è¿è¡Œï¼Œä½†å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é—®é¢˜ï¼š

---

### ğŸ¤” **ä¸ºä»€ä¹ˆè¦åŒºåˆ† Fast å’Œæ™®é€šç‰ˆï¼Ÿ**
| ç‰¹æ€§                | PreTrainedTokenizerFastï¼ˆå¿«é€Ÿç‰ˆï¼‰          | PreTrainedTokenizerï¼ˆæ™®é€šç‰ˆï¼‰       |
|---------------------|--------------------------------------------|-------------------------------------|
| **åº•å±‚å®ç°**         | Rustè¯­è¨€ç¼–å†™ï¼ˆé€Ÿåº¦å¿«ï¼‰                     | Pythonå®ç°ï¼ˆé€Ÿåº¦æ…¢ï¼‰                 |
| **æ‰¹å¤„ç†æ”¯æŒ**       | âœ… åŸç”Ÿæ”¯æŒï¼ˆå¦‚`batch_encode_plus`ï¼‰        | âŒ éœ€æ‰‹åŠ¨å¾ªç¯å¤„ç†                    |
| **ç‰¹æ®Šæ ‡è®°å¤„ç†**     | è‡ªåŠ¨ç®¡ç†ï¼ˆå¦‚å¡«å……ã€æˆªæ–­ï¼‰                   | éœ€æ‰‹åŠ¨é…ç½®                          |
| **å…¸å‹åœºæ™¯**         | ç”Ÿäº§ç¯å¢ƒã€å¤§æ•°æ®å¤„ç†                        | æ•™å­¦æˆ–å…¼å®¹æ—§ä»£ç                      |

---

### ğŸ’¥ **ä¸æ£€æŸ¥å¯èƒ½å¸¦æ¥çš„é—®é¢˜**
1. **æ€§èƒ½ä¸‹é™**ï¼šå¤„ç†1000æ¡æ–‡æœ¬æ—¶ï¼Œå¿«é€Ÿç‰ˆå¯èƒ½æ¯”æ™®é€šç‰ˆå¿«**5-10å€**ã€‚
2. **åŠŸèƒ½ç¼ºå¤±**ï¼šæ™®é€šç‰ˆå¯èƒ½ç¼ºå°‘æŸäº›APIï¼ˆå¦‚`decode`çš„`skip_special_tokens`å‚æ•°ï¼‰ã€‚
3. **æ„å¤–é”™è¯¯**ï¼šæŸäº›åº“ï¼ˆå¦‚Datasetsï¼‰é»˜è®¤è¦æ±‚å¿«é€Ÿç‰ˆåˆ†è¯å™¨ã€‚

---

### ğŸŒ° **å®é™…æ¡ˆä¾‹**
å‡è®¾ä½ çš„`model_checkpoint`æ„å¤–æŒ‡å‘äº†ä¸€ä¸ªæ²¡æœ‰å¿«é€Ÿç‰ˆçš„æ¨¡å‹ï¼š
```python
model_checkpoint = "some-old-model"  # å‡è®¾è¯¥æ¨¡å‹åªæœ‰æ™®é€šç‰ˆåˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
# æ­¤æ—¶ tokenizer æ˜¯ PreTrainedTokenizer è€Œé Fast ç‰ˆ
# åç»­è°ƒç”¨ batch_encode_plus å¯èƒ½æŠ¥é”™ï¼
```

é€šè¿‡`assert`æ£€æŸ¥ï¼Œå¯ä»¥**æå‰å‘ç°é—®é¢˜**ï¼Œé¿å…åç»­ä»£ç å´©æºƒã€‚

---

### ğŸ”§ **æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚æœä¸åšæ–­è¨€ï¼‰**
1. **ç›´æ¥ä½¿ç”¨**ï¼šå¦‚æœç¡®å®šæ¨¡å‹æœ‰å¿«é€Ÿç‰ˆï¼Œå¯ä»¥è·³è¿‡æ£€æŸ¥ã€‚
2. **é™çº§å¤„ç†**ï¼šæ•è·å¼‚å¸¸å¹¶æ”¹ç”¨æ™®é€šç‰ˆé€»è¾‘ï¼š
```python
if not isinstance(tokenizer, PreTrainedTokenizerFast):
    print("è­¦å‘Šï¼šä½¿ç”¨æ…¢é€Ÿåˆ†è¯å™¨ï¼Œæ€§èƒ½å¯èƒ½å—å½±å“ï¼")
    # æ‰‹åŠ¨å¤„ç†æ™®é€šç‰ˆçš„é™åˆ¶
```

---

æ€»ç»“ï¼šè¿™ä¸ªæ–­è¨€æ˜¯**é˜²å¾¡æ€§ç¼–ç¨‹**çš„ä½“ç°ï¼Œç¡®ä¿ä»£ç åœ¨æ€§èƒ½å’ŒåŠŸèƒ½ä¸ŠæŒ‰é¢„æœŸè¿è¡Œã€‚å¯¹äºå…³é”®é¡¹ç›®å»ºè®®ä¿ç•™ï¼Œä¸ªäººå®éªŒå¯è·³è¿‡ã€‚

æ‚¨å¯ä»¥åœ¨å¤§æ¨¡å‹è¡¨ä¸ŠæŸ¥çœ‹å“ªç§ç±»å‹çš„æ¨¡å‹å…·æœ‰å¯ç”¨çš„å¿«é€Ÿæ ‡è®°å™¨ï¼Œå“ªç§ç±»å‹æ²¡æœ‰ã€‚

æ‚¨å¯ä»¥ç›´æ¥åœ¨ä¸¤ä¸ªå¥å­ä¸Šè°ƒç”¨æ­¤æ ‡è®°å™¨ï¼ˆä¸€ä¸ªç”¨äºç­”æ¡ˆï¼Œä¸€ä¸ªç”¨äºä¸Šä¸‹æ–‡ï¼‰ï¼š


```python
tokenizer("What is your name?", "My name is Sylvain.")
```




    {'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}




```python
 tokenizer("How are you?")
```




    {'input_ids': [101, 2129, 2024, 2017, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}



### Tokenizer è¿›é˜¶æ“ä½œ

åœ¨é—®ç­”é¢„å¤„ç†ä¸­çš„ä¸€ä¸ªç‰¹å®šé—®é¢˜æ˜¯å¦‚ä½•å¤„ç†éå¸¸é•¿çš„æ–‡æ¡£ã€‚

åœ¨å…¶ä»–ä»»åŠ¡ä¸­ï¼Œå½“æ–‡æ¡£çš„é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€å¤§å¥å­é•¿åº¦æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæˆªæ–­å®ƒä»¬ï¼Œä½†åœ¨è¿™é‡Œï¼Œåˆ é™¤ä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†å¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬ä¸¢å¤±æ­£åœ¨å¯»æ‰¾çš„ç­”æ¡ˆã€‚

ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å…è®¸æ•°æ®é›†ä¸­çš„ä¸€ä¸ªï¼ˆé•¿ï¼‰ç¤ºä¾‹ç”Ÿæˆå¤šä¸ªè¾“å…¥ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„é•¿åº¦éƒ½å°äºæ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆæˆ–æˆ‘ä»¬è®¾ç½®çš„è¶…å‚æ•°ï¼‰ã€‚


```python
# The maximum length of a feature (question and context)
max_length = 384 
# The authorized overlap between two part of the context when splitting it is needed.
doc_stride = 128 
```

---

### **ä¸ºä½•è®¾ç½® `max_length=384`ï¼Ÿ**
1. **æ¨¡å‹é™åˆ¶**  
   BERTç­‰æ¨¡å‹æœ€å¤§æ”¯æŒ **512 tokens**ï¼Œéœ€ä¸ºä»¥ä¸‹å†…å®¹ç•™ç©ºé—´ï¼š  
   - **é—®é¢˜æœ¬èº«**ï¼ˆçº¦20-30 tokensï¼‰  
   - **ç‰¹æ®Šæ ‡è®°**ï¼ˆå¦‚ `[CLS]`ã€`[SEP]`ï¼Œå 3-5 tokensï¼‰  
   - **ç­”æ¡ˆä½ç½®**ï¼ˆé¿å…è¢«æˆªæ–­ï¼‰

2. **ç»éªŒæ¯”ä¾‹**  
   å¯ç”¨ä¸Šä¸‹æ–‡é•¿åº¦ â‰ˆ æ€»é•¿çš„ **75%**ï¼ˆ512Ã—0.75â‰ˆ384ï¼‰ï¼Œå¹³è¡¡è¦†ç›–ç‡å’Œè®¡ç®—æ•ˆç‡ã€‚

3. **åˆ†å—ä¼˜åŒ–**  
   ç»“åˆ `doc_stride=128`ï¼ˆé‡å é‡ï¼‰ï¼Œç¡®ä¿ç­”æ¡ˆåœ¨è‡³å°‘ä¸€ä¸ªåˆ†å—ä¸­å®Œæ•´å‡ºç°ã€‚

---

### **å®é™…æ¡ˆä¾‹**  
- **è¾“å…¥**ï¼šé—®é¢˜ï¼ˆ20 tokensï¼‰+ ä¸Šä¸‹æ–‡ï¼ˆ500 tokensï¼‰  
- **å¤„ç†**ï¼š  
  1. åˆ†å—1ï¼šé—®é¢˜ + ä¸Šä¸‹æ–‡0-363  
  2. åˆ†å—2ï¼šé—®é¢˜ + ä¸Šä¸‹æ–‡236-500ï¼ˆä¸åˆ†å—1é‡å 128 tokensï¼‰  
- **ç»“æœ**ï¼šå³ä½¿ç­”æ¡ˆåœ¨360-400åŒºé—´ï¼Œä¹Ÿèƒ½è¢«åˆ†å—2è¦†ç›–ã€‚

---

### **è°ƒæ•´å»ºè®®**
- **çŸ­æ–‡æœ¬ä»»åŠ¡**ï¼šç›´æ¥è®¾ä¸º512  
- **è¶…é•¿æ–‡æ¡£**ï¼šå¯é™ä½åˆ°256ï¼ˆéœ€æ›´å¤šåˆ†å—ï¼‰  
- **æ”¯æŒæ›´é•¿æ¨¡å‹**ï¼šå¦‚æ”¯æŒ1024ï¼Œå¯è®¾ä¸º768  

ä¸€å¥è¯æ€»ç»“ï¼š**384æ˜¯å¹³è¡¡æ¨¡å‹é™åˆ¶ã€ç­”æ¡ˆå®Œæ•´æ€§å’Œè®¡ç®—æ•ˆç‡çš„ç»éªŒå€¼ã€‚**

`doc_stride=128` çš„åŸç†ä¸ `max_length=384` ç±»ä¼¼ï¼Œä½†å…³æ³¨ç‚¹ä¸åŒã€‚ä»¥ä¸‹æ˜¯ç®€æ´æ¸…æ™°çš„è§£é‡Šï¼š

---

### **ä¸ºä½•è®¾ç½® `doc_stride=128`ï¼Ÿ**
1. **æ ¸å¿ƒç›®çš„**  
   **é¿å…ç­”æ¡ˆè¢«åˆ‡å‰²åœ¨åˆ†å—è¾¹ç•Œ**ã€‚é€šè¿‡è®¾ç½®åˆ†å—é—´çš„é‡å åŒºåŸŸï¼Œç¡®ä¿å³ä½¿ç­”æ¡ˆä½äºåˆ†å—è¾¹ç¼˜ï¼Œä¹Ÿèƒ½è¢«è‡³å°‘ä¸€ä¸ªå®Œæ•´åˆ†å—è¦†ç›–ã€‚

2. **ç»éªŒå…¬å¼**  
   `doc_stride` â‰ˆ `max_length` çš„ **1/3~1/4**ï¼ˆå¦‚ `384/3â‰ˆ128`ï¼‰ï¼Œå¹³è¡¡ï¼š
   - **è®¡ç®—æ•ˆç‡**ï¼ˆåˆ†å—è¶Šå°‘è¶Šå¥½ï¼‰
   - **ç­”æ¡ˆè¦†ç›–ç‡**ï¼ˆé‡å è¶Šå¤šè¶Šå®‰å…¨ï¼‰

---

### **åˆ†å—é€»è¾‘ç¤ºä¾‹**
- **å‚æ•°**ï¼š
  - `max_length=384`ï¼ˆæ€»é•¿åº¦ï¼‰
  - é—®é¢˜é•¿åº¦ = 20 tokens
  - å¯ç”¨ä¸Šä¸‹æ–‡é•¿åº¦ = `384 - 20 - 3ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰â‰ˆ 361 tokens`
  - `doc_stride=128`
- **åˆ†å—æ­¥é•¿** = `361 - 128 = 233 tokens`

| åˆ†å— | èµ·å§‹ä½ç½® | ç»“æŸä½ç½® | è¦†ç›–çš„ä¸Šä¸‹æ–‡èŒƒå›´ |
|------|----------|----------|------------------|
| 1    | 0        | 360      | tokens 0-360     |
| 2    | 233      | 593      | tokens 233-593   |
| 3    | 466      | 826      | tokens 466-826   |

- **å‡è®¾ç­”æ¡ˆåœ¨ tokens 350-370**ï¼š
  - åˆ†å—1ï¼šè¦†ç›–åˆ°360 â†’ ç­”æ¡ˆéƒ¨åˆ†æˆªæ–­ï¼ˆ350-360ä¿ç•™ï¼‰
  - åˆ†å—2ï¼šä»233å¼€å§‹ â†’ å®Œæ•´è¦†ç›–ç­”æ¡ˆï¼ˆ350-370ï¼‰

---

### **å…³é”®å½±å“**
| `doc_stride` å€¼ | ä¼˜ç‚¹               | ç¼ºç‚¹                 |
|-----------------|--------------------|----------------------|
| **è¾ƒå°ï¼ˆå¦‚64ï¼‰** | ç­”æ¡ˆè¦†ç›–ç‡â†‘        | åˆ†å—æ•°é‡â†‘ï¼Œè®¡ç®—é‡â†‘   |
| **è¾ƒå¤§ï¼ˆå¦‚192ï¼‰**| åˆ†å—æ•°é‡â†“ï¼Œé€Ÿåº¦â†‘   | æ¼ç­”é£é™©â†‘            |

---

### **è°ƒæ•´å»ºè®®**
- **çŸ­ç­”æ¡ˆä»»åŠ¡**ï¼ˆå¦‚å®ä½“æŠ½å–ï¼‰ï¼š`doc_stride=64~128`
- **é•¿ç­”æ¡ˆä»»åŠ¡**ï¼ˆå¦‚æ®µè½æ€»ç»“ï¼‰ï¼š`doc_stride=128~256`

ä¸€å¥è¯æ€»ç»“ï¼š**`doc_stride=128` æ˜¯ç»éªŒæ€§å‚æ•°ï¼Œé€šè¿‡åˆ†å—é‡å å¹³è¡¡æ•ˆç‡ä¸ç­”æ¡ˆå®Œæ•´æ€§ã€‚**

å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹å‚æ•°ï¼š
- **`max_length = 10`**ï¼ˆæ¯ä¸ªç‰‡æ®µæœ€å¤šåŒ…å«10ä¸ªå­—ç¬¦ï¼‰
- **`doc_stride = 4`**ï¼ˆç›¸é‚»ç‰‡æ®µé‡å 4ä¸ªå­—ç¬¦ï¼‰

---

### **åˆ‡å‰²è¿‡ç¨‹**
åŸå§‹æ–‡æœ¬ï¼š`ABCDEFGHIJKLMN`ï¼ˆå‡è®¾æ¯ä¸ªå­—æ¯ä»£è¡¨ä¸€ä¸ªtokenï¼‰

1. **ç¬¬ä¸€ä¸ªç‰‡æ®µ**ï¼š  
   - å–å‰10ä¸ªå­—ç¬¦ â†’ `ABCDEFGHIJ`ï¼ˆAåˆ°Jï¼‰
   - ç»“æŸä½ç½®ï¼šç¬¬10ä¸ªå­—ç¬¦ï¼ˆJï¼‰

2. **ç¬¬äºŒä¸ªç‰‡æ®µ**ï¼š  
   - èµ·å§‹ä½ç½® = å‰ç‰‡æ®µçš„èµ·å§‹ä½ç½® + (`max_length - doc_stride`) = 0 + (10 - 4) = 6  
     ï¼ˆå³ä»ç¬¬7ä¸ªå­—ç¬¦å¼€å§‹ï¼Œå¯¹åº”å­—æ¯ `G`ï¼‰
   - å®é™…å­—ç¬¦ï¼š`GHIJKLMN`ï¼ˆGåˆ°Nï¼Œå…±8ä¸ªå­—ç¬¦ï¼Œä¸è¶³10ä¸ªåˆ™ä¿ç•™ï¼‰
   - é‡å éƒ¨åˆ†ï¼š`GHIJ`ï¼ˆä¸å‰ä¸€ç‰‡æ®µçš„å4ä¸ªå­—ç¬¦é‡å ï¼‰

---

### **å›¾ç¤ºåˆ‡å‰²æ•ˆæœ**
```
åŸå§‹æ–‡æœ¬ï¼š A B C D E F G H I J K L M N
ç‰‡æ®µ1ï¼š    [A B C D E F G H I J]          â†’ é•¿åº¦10
ç‰‡æ®µ2ï¼š            [G H I J K L M N]      â†’ èµ·å§‹ä½ç½®6ï¼Œé‡å 4ä¸ªå­—ç¬¦
```

---

### **ä¸ºä»€ä¹ˆéœ€è¦é‡å ï¼Ÿ**
å‡è®¾ç­”æ¡ˆåœ¨ `H I J K` åŒºåŸŸï¼š
- **æ— é‡å **ï¼šå¯èƒ½è¢«æˆªæ–­åœ¨ç‰‡æ®µ1æœ«å°¾æˆ–ç‰‡æ®µ2å¼€å¤´
- **æœ‰é‡å **ï¼šç¡®ä¿ç­”æ¡ˆå®Œæ•´åŒ…å«åœ¨è‡³å°‘ä¸€ä¸ªç‰‡æ®µä¸­

---

### **å®é™…é—®ç­”ä¸­çš„å‚æ•°**
å½“ `max_length=384` ä¸” `doc_stride=128` æ—¶ï¼Œé€»è¾‘å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯æ•°å€¼æ›´å¤§ã€‚è¿™ç§æ»‘åŠ¨çª—å£åˆ‡å‰²æ˜¯å¤„ç†é•¿æ–‡æœ¬é—®ç­”çš„å¸¸ç”¨ç­–ç•¥ï¼ ğŸ˜Š

#### è¶…å‡ºæœ€å¤§é•¿åº¦çš„æ–‡æœ¬æ•°æ®å¤„ç†

ä¸‹é¢ï¼Œæˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­æ‰¾å‡ºä¸€ä¸ªè¶…è¿‡æœ€å¤§é•¿åº¦ï¼ˆ384ï¼‰çš„æ–‡æœ¬ï¼š


```python
for i, example in enumerate(datasets["train"]):
    if len(tokenizer(example["question"], example["context"])["input_ids"]) > 384:
        break
# æŒ‘é€‰å‡ºæ¥è¶…è¿‡384ï¼ˆæœ€å¤§é•¿åº¦ï¼‰çš„æ•°æ®æ ·ä¾‹
example = datasets["train"][i]
```


```python
len(tokenizer(example["question"], example["context"])["input_ids"])
```




    437



#### æˆªæ–­ä¸Šä¸‹æ–‡ä¸ä¿ç•™è¶…å‡ºéƒ¨åˆ†


```python
len(tokenizer(example["question"],
              example["context"],
              max_length=max_length,
              truncation="only_second")["input_ids"])
```




    384



#### å…³äºæˆªæ–­çš„ç­–ç•¥

- ç›´æ¥æˆªæ–­è¶…å‡ºéƒ¨åˆ†: truncation=`only_second`
- ä»…æˆªæ–­ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ï¼Œä¿ç•™é—®é¢˜ï¼ˆquestionï¼‰ï¼š`return_overflowing_tokens=True` & è®¾ç½®`stride`



```python
tokenized_example = tokenizer(
    example["question"],
    example["context"],
    max_length=max_length,
    truncation="only_second",
    return_overflowing_tokens=True,
    stride=doc_stride
)
```

ä½¿ç”¨æ­¤ç­–ç•¥æˆªæ–­åï¼ŒTokenizer å°†è¿”å›å¤šä¸ª `input_ids` åˆ—è¡¨ã€‚


```python
[len(x) for x in tokenized_example["input_ids"]]
```




    [384, 192]



è§£ç ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ï¼Œå¯ä»¥çœ‹åˆ°é‡å çš„éƒ¨åˆ†ï¼š


```python
for x in tokenized_example["input_ids"][:2]:
    print(tokenizer.decode(x))
```

    [CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single " crazy in love ", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song " single ladies ( put a ring on it ) " and the top - five songs " if i were a boy " and " halo ". achieving the accomplishment of becoming her longest - running hot 100 single in her career, " halo "'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful " sweet dreams ", and singles " diva ", " ego ", " broken - hearted girl " and " video phone ". the music video for " single ladies " has been parodied and imitated around the world, spawning the " first major dance craze " of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's " you belong with me ", led to kanye west interrupting the ceremony and beyonce [SEP]
    [CLS] beyonce got married in 2008 to whom? [SEP] single ladies " has been parodied and imitated around the world, spawning the " first major dance craze " of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's " you belong with me ", led to kanye west interrupting the ceremony and beyonce improvising a re - presentation of swift's award during her own acceptance speech. in march 2009, beyonce embarked on the i am... world tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $ 119. 5 million. [SEP]


#### ä½¿ç”¨ offsets_mapping è·å–åŸå§‹çš„ input_ids

è®¾ç½® `return_offsets_mapping=True`ï¼Œå°†ä½¿å¾—æˆªæ–­åˆ†å‰²ç”Ÿæˆçš„å¤šä¸ª input_ids åˆ—è¡¨ä¸­çš„ tokenï¼Œé€šè¿‡æ˜ å°„ä¿ç•™åŸå§‹æ–‡æœ¬çš„ input_idsã€‚

å¦‚ä¸‹æ‰€ç¤ºï¼šç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆ[CLS]ï¼‰çš„èµ·å§‹å’Œç»“æŸå­—ç¬¦éƒ½æ˜¯ï¼ˆ0, 0ï¼‰ï¼Œå› ä¸ºå®ƒä¸å¯¹åº”é—®é¢˜/ç­”æ¡ˆçš„ä»»ä½•éƒ¨åˆ†ï¼Œç„¶åç¬¬äºŒä¸ªæ ‡è®°ä¸é—®é¢˜(question)çš„å­—ç¬¦0åˆ°3ç›¸åŒ.


```python
tokenized_example = tokenizer(
    example["question"],            # ç¬¬ä¸€ä¸ªå‚æ•°ï¼šé—®é¢˜æ–‡æœ¬
    example["context"],             # ç¬¬äºŒä¸ªå‚æ•°ï¼šä¸Šä¸‹æ–‡æ–‡æœ¬
    max_length=max_length,          # æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚384ï¼‰
    truncation="only_second",       # å…³é”®å‚æ•°1ï¼šæˆªæ–­ç­–ç•¥
    return_overflowing_tokens=True, # å…³é”®å‚æ•°2ï¼šè¿”å›åˆ†å—ç»“æœ
    return_offsets_mapping=True,    # å…³é”®å‚æ•°3ï¼šè¿”å›å­—ç¬¦çº§ä½ç½®æ˜ å°„
    return_token_type_ids=True,     # æ˜¾å¼è¦æ±‚è¿”å› token_type_ids
    stride=doc_stride               # åˆ†å—æ»‘åŠ¨æ­¥é•¿ï¼ˆå¦‚128ï¼‰
)
print(tokenized_example["offset_mapping"][0][:100])
```

    [(0, 0), (0, 7), (8, 11), (12, 19), (20, 22), (23, 27), (28, 30), (31, 35), (35, 36), (0, 0), (0, 2), (3, 8), (9, 10), (10, 11), (12, 16), (16, 17), (18, 25), (26, 33), (34, 37), (38, 39), (39, 40), (41, 44), (45, 53), (54, 62), (63, 68), (69, 77), (78, 80), (81, 82), (83, 88), (89, 93), (93, 96), (97, 99), (100, 103), (104, 113), (114, 119), (120, 123), (124, 127), (128, 133), (134, 140), (141, 146), (146, 147), (148, 149), (150, 152), (152, 153), (153, 154), (154, 155), (156, 161), (162, 168), (168, 169), (170, 172), (173, 182), (182, 183), (183, 184), (185, 189), (190, 194), (195, 197), (198, 205), (206, 208), (208, 209), (210, 214), (214, 215), (216, 217), (218, 220), (220, 221), (221, 222), (222, 223), (224, 229), (230, 236), (237, 240), (241, 249), (250, 252), (253, 261), (262, 264), (264, 265), (266, 270), (271, 273), (274, 277), (278, 284), (285, 291), (291, 292), (293, 296), (297, 302), (303, 311), (312, 322), (323, 330), (330, 331), (331, 332), (333, 338), (339, 342), (343, 348), (349, 355), (355, 356), (357, 366), (367, 373), (374, 377), (378, 384), (385, 387), (388, 391), (392, 396), (397, 403)]


---

### **å‚æ•°è¯¦è§£**
#### 1. `truncation="only_second"`
- **ä½œç”¨**ï¼š**åªæˆªæ–­ç¬¬äºŒä¸ªå‚æ•°ï¼ˆä¸Šä¸‹æ–‡ï¼‰**ï¼Œä¿æŒç¬¬ä¸€ä¸ªå‚æ•°ï¼ˆé—®é¢˜ï¼‰å®Œæ•´
- **åœºæ™¯**ï¼šå½“ `é—®é¢˜+ä¸Šä¸‹æ–‡` æ€»é•¿åº¦è¶…è¿‡ `max_length` æ—¶ï¼Œä¼˜å…ˆä¿ç•™é—®é¢˜å®Œæ•´æ€§
- **ç¤ºä¾‹**ï¼š
  ```python
  # è¾“å…¥ï¼šé—®é¢˜é•¿åº¦20ï¼Œä¸Šä¸‹æ–‡é•¿åº¦400 â†’ æ€»é•¿åº¦420 > 384
  # å¤„ç†ï¼šæˆªæ–­ä¸Šä¸‹æ–‡ä¸º 384-20-3ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰= 361 tokens
  ```

#### 2. `return_overflowing_tokens=True`
- **ä½œç”¨**ï¼š**è¿”å›åˆ†å—åçš„å¤šä¸ªè¾“å…¥ç‰¹å¾**ï¼ˆå½“è¾“å…¥è¿‡é•¿æ—¶è‡ªåŠ¨åˆ†å‰²ï¼‰
- **è¾“å‡ºå­—æ®µ**ï¼š`overflow_to_sample_mapping`ï¼ˆåˆ†å—å¯¹åº”åŸå§‹æ ·æœ¬çš„ç´¢å¼•ï¼‰
- **åˆ†å—é€»è¾‘**ï¼š
  - å°†é•¿ä¸Šä¸‹æ–‡æŒ‰ `max_length - é—®é¢˜é•¿åº¦` åˆ‡å‰²
  - ç›¸é‚»åˆ†å—é‡å  `stride` tokensï¼ˆç¡®ä¿ç­”æ¡ˆä¸è¢«åˆ‡å‰²ï¼‰

#### 3. `return_offsets_mapping=True`
- **ä½œç”¨**ï¼š**è¿”å›æ¯ä¸ª token åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®**ï¼ˆèµ·å§‹å’Œç»“æŸç´¢å¼•ï¼‰
- **è¾“å‡ºå­—æ®µ**ï¼š`offset_mapping`ï¼ˆåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ `(start, end)` å…ƒç»„ï¼‰
- **å…³é”®ç”¨é€”**ï¼šå°†æ¨¡å‹é¢„æµ‹çš„ token ä½ç½®æ˜ å°„å›åŸå§‹æ–‡æœ¬ï¼ˆå¦‚å®šä½ç­”æ¡ˆï¼‰

---

### **`offset_mapping` ç¤ºä¾‹è§£æ**
```python
# å‡è®¾æ‰“å°ç»“æœå‰5ä¸ªå…ƒç´ ï¼š
[(0, 0), (0, 3), (4, 7), (8, 11), (12, 15), ...]

# å¯¹åº”å«ä¹‰ï¼š
# [CLS]  What    is    your   name?  [SEP] ...
# (0,0) (0,3) (4,7) (8,11) (12,15)   ...
```
- **ç‰¹æ®Šæ ‡è®°**ï¼š`[CLS]`ã€`[SEP]` ç­‰æ— å¯¹åº”æ–‡æœ¬ â†’ `(0, 0)`
- **é—®é¢˜éƒ¨åˆ†**ï¼šå­—ç¬¦ç´¢å¼•ä»é—®é¢˜æ–‡æœ¬çš„èµ·å§‹ä½ç½®è®¡ç®—
- **ä¸Šä¸‹æ–‡éƒ¨åˆ†**ï¼šå­—ç¬¦ç´¢å¼•ä»ä¸Šä¸‹æ–‡æ–‡æœ¬çš„èµ·å§‹ä½ç½®è®¡ç®—ï¼ˆéœ€æ³¨æ„é—®é¢˜æ–‡æœ¬é•¿åº¦ï¼‰

---

### **å‚æ•°ååŒä½œç”¨**
| å‚æ•°ç»„åˆ                        | å®é™…æ•ˆæœ                                                                 |
|---------------------------------|------------------------------------------------------------------------|
| `truncation="only_second"` + `return_overflowing_tokens=True` | å°†é•¿ä¸Šä¸‹æ–‡åˆ‡å‰²ä¸ºå¤šä¸ªåˆ†å—ï¼Œæ¯ä¸ªåˆ†å—åŒ…å«å®Œæ•´é—®é¢˜å’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡ |
| `return_offsets_mapping=True`   | æä¾›åˆ†å—ä¸­æ¯ä¸ª token åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„ä½ç½®ï¼Œç”¨äºç­”æ¡ˆä½ç½®æ˜ å°„               |

---

### **åº”ç”¨åœºæ™¯**
1. **è®­ç»ƒé˜¶æ®µ**ï¼šå°†ç­”æ¡ˆçš„å­—ç¬¦ä½ç½®è½¬æ¢ä¸ºåˆ†å—å†…çš„ token ä½ç½®
2. **æ¨ç†é˜¶æ®µ**ï¼šå°†æ¨¡å‹é¢„æµ‹çš„ token ä½ç½®åå‘æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡
3. **æ•°æ®éªŒè¯**ï¼šæ£€æŸ¥åˆ†å—æ˜¯å¦è¦†ç›–æ­£ç¡®ç­”æ¡ˆçš„åŸå§‹ä½ç½®

---

é€šè¿‡è¿™ä¸‰ä¸ªå‚æ•°ï¼Œå®ç°äº†é•¿æ–‡æœ¬é—®ç­”ä»»åŠ¡ä¸­ **è¾“å…¥åˆ†å—å¤„ç†** å’Œ **ä½ç½®ç²¾ç¡®æ˜ å°„** çš„æ ¸å¿ƒéœ€æ±‚ã€‚

å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ˜ å°„æ¥æ‰¾åˆ°ç­”æ¡ˆåœ¨ç»™å®šç‰¹å¾ä¸­çš„èµ·å§‹å’Œç»“æŸæ ‡è®°çš„ä½ç½®ã€‚

æˆ‘ä»¬åªéœ€åŒºåˆ†åç§»çš„å“ªäº›éƒ¨åˆ†å¯¹åº”äºé—®é¢˜ï¼Œå“ªäº›éƒ¨åˆ†å¯¹åº”äºä¸Šä¸‹æ–‡ã€‚


```python
first_token_id = tokenized_example["input_ids"][0][1]
offsets = tokenized_example["offset_mapping"][0][1]
print(first_token_id)
print(offsets)
print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example["question"][offsets[0]:offsets[1]])
```

    20773
    (0, 7)
    beyonce Beyonce



```python
second_token_id = tokenized_example["input_ids"][0][2]
offsets = tokenized_example["offset_mapping"][0][2]
print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example["question"][offsets[0]:offsets[1]])
```

    got got



```python
first_token_id = tokenized_example["input_ids"][0][7]
offsets = tokenized_example["offset_mapping"][0][7]
print(first_token_id)
print(offsets)
print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example["question"][offsets[0]:offsets[1]])
```

    3183
    (31, 35)
    whom whom



```python
first_token_id = tokenized_example["input_ids"][0][10]
offsets = tokenized_example["offset_mapping"][0][10]
print(first_token_id)
print(offsets)
print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example["context"][offsets[0]:offsets[1]])
```

    2006
    (0, 2)
    on On



```python
# éå†æ¯ä¸ªåˆ†å—
for chunk_idx in range(len(tokenized_example["input_ids"])):
    print(f"\n=== åˆ†å— {chunk_idx} ===")
    
    # è·å–å½“å‰åˆ†å—çš„æ•°æ®
    input_ids = tokenized_example["input_ids"][chunk_idx]
    offset_mapping = tokenized_example["offset_mapping"][chunk_idx]
    token_type_ids = tokenized_example["token_type_ids"][chunk_idx]

    # éå†åˆ†å—å†…çš„æ¯ä¸ª token
    for token_idx, (token_id, offset, token_type) in enumerate(zip(input_ids, offset_mapping, token_type_ids)):
        # æ ¹æ® token_type é€‰æ‹©æ¥æºæ–‡æœ¬
        if token_type == 0:
            source_text = example["question"]
        else:
            source_text = example["context"]

        # å…³é”®ä¿®å¤ç‚¹ï¼šåˆ†è§£ offset å…ƒç»„ä¸º start å’Œ end
        start = offset[0]  # èµ·å§‹å­—ç¬¦ä½ç½®
        end = offset[1]    # ç»“æŸå­—ç¬¦ä½ç½®
        print(start, end)
        original_text = source_text[start:end]
        
        # è½¬æ¢ token_id ä¸ºå¯è¯»æ–‡æœ¬
        token_str = tokenizer.convert_ids_to_tokens([token_id])[0]  # å–åˆ—è¡¨ç¬¬ä¸€ä¸ªå…ƒç´ 
        
        # æ‰“å°ç»“æœ
        print(f"Token {token_idx}: {token_str} â†’ {original_text}")
```

    
    === åˆ†å— 0 ===
    0 0
    Token 0: [CLS] â†’ 
    0 7
    Token 1: beyonce â†’ Beyonce
    8 11
    Token 2: got â†’ got
    12 19
    Token 3: married â†’ married
    20 22
    Token 4: in â†’ in
    23 27
    Token 5: 2008 â†’ 2008
    28 30
    Token 6: to â†’ to
    31 35
    Token 7: whom â†’ whom
    35 36
    Token 8: ? â†’ ?
    0 0
    Token 9: [SEP] â†’ 
    0 2
    Token 10: on â†’ On
    3 8
    Token 11: april â†’ April
    9 10
    Token 12: 4 â†’ 4
    10 11
    Token 13: , â†’ ,
    12 16
    Token 14: 2008 â†’ 2008
    16 17
    Token 15: , â†’ ,
    18 25
    Token 16: beyonce â†’ BeyoncÃ©
    26 33
    Token 17: married â†’ married
    34 37
    Token 18: jay â†’ Jay
    38 39
    Token 19: z â†’ Z
    39 40
    Token 20: . â†’ .
    41 44
    Token 21: she â†’ She
    45 53
    Token 22: publicly â†’ publicly
    54 62
    Token 23: revealed â†’ revealed
    63 68
    Token 24: their â†’ their
    69 77
    Token 25: marriage â†’ marriage
    78 80
    Token 26: in â†’ in
    81 82
    Token 27: a â†’ a
    83 88
    Token 28: video â†’ video
    89 93
    Token 29: mont â†’ mont
    93 96
    Token 30: ##age â†’ age
    97 99
    Token 31: at â†’ at
    100 103
    Token 32: the â†’ the
    104 113
    Token 33: listening â†’ listening
    114 119
    Token 34: party â†’ party
    120 123
    Token 35: for â†’ for
    124 127
    Token 36: her â†’ her
    128 133
    Token 37: third â†’ third
    134 140
    Token 38: studio â†’ studio
    141 146
    Token 39: album â†’ album
    146 147
    Token 40: , â†’ ,
    148 149
    Token 41: i â†’ I
    150 152
    Token 42: am â†’ Am
    152 153
    Token 43: . â†’ .
    153 154
    Token 44: . â†’ .
    154 155
    Token 45: . â†’ .
    156 161
    Token 46: sasha â†’ Sasha
    162 168
    Token 47: fierce â†’ Fierce
    168 169
    Token 48: , â†’ ,
    170 172
    Token 49: in â†’ in
    173 182
    Token 50: manhattan â†’ Manhattan
    182 183
    Token 51: ' â†’ '
    183 184
    Token 52: s â†’ s
    185 189
    Token 53: sony â†’ Sony
    190 194
    Token 54: club â†’ Club
    195 197
    Token 55: on â†’ on
    198 205
    Token 56: october â†’ October
    206 208
    Token 57: 22 â†’ 22
    208 209
    Token 58: , â†’ ,
    210 214
    Token 59: 2008 â†’ 2008
    214 215
    Token 60: . â†’ .
    216 217
    Token 61: i â†’ I
    218 220
    Token 62: am â†’ Am
    220 221
    Token 63: . â†’ .
    221 222
    Token 64: . â†’ .
    222 223
    Token 65: . â†’ .
    224 229
    Token 66: sasha â†’ Sasha
    230 236
    Token 67: fierce â†’ Fierce
    237 240
    Token 68: was â†’ was
    241 249
    Token 69: released â†’ released
    250 252
    Token 70: on â†’ on
    253 261
    Token 71: november â†’ November
    262 264
    Token 72: 18 â†’ 18
    264 265
    Token 73: , â†’ ,
    266 270
    Token 74: 2008 â†’ 2008
    271 273
    Token 75: in â†’ in
    274 277
    Token 76: the â†’ the
    278 284
    Token 77: united â†’ United
    285 291
    Token 78: states â†’ States
    291 292
    Token 79: . â†’ .
    293 296
    Token 80: the â†’ The
    297 302
    Token 81: album â†’ album
    303 311
    Token 82: formally â†’ formally
    312 322
    Token 83: introduces â†’ introduces
    323 330
    Token 84: beyonce â†’ BeyoncÃ©
    330 331
    Token 85: ' â†’ '
    331 332
    Token 86: s â†’ s
    333 338
    Token 87: alter â†’ alter
    339 342
    Token 88: ego â†’ ego
    343 348
    Token 89: sasha â†’ Sasha
    349 355
    Token 90: fierce â†’ Fierce
    355 356
    Token 91: , â†’ ,
    357 366
    Token 92: conceived â†’ conceived
    367 373
    Token 93: during â†’ during
    374 377
    Token 94: the â†’ the
    378 384
    Token 95: making â†’ making
    385 387
    Token 96: of â†’ of
    388 391
    Token 97: her â†’ her
    392 396
    Token 98: 2003 â†’ 2003
    397 403
    Token 99: single â†’ single
    404 405
    Token 100: " â†’ "
    405 410
    Token 101: crazy â†’ Crazy
    411 413
    Token 102: in â†’ in
    414 418
    Token 103: love â†’ Love
    418 419
    Token 104: " â†’ "
    419 420
    Token 105: , â†’ ,
    421 428
    Token 106: selling â†’ selling
    429 431
    Token 107: 48 â†’ 48
    431 432
    Token 108: ##2 â†’ 2
    432 433
    Token 109: , â†’ ,
    433 436
    Token 110: 000 â†’ 000
    437 443
    Token 111: copies â†’ copies
    444 446
    Token 112: in â†’ in
    447 450
    Token 113: its â†’ its
    451 456
    Token 114: first â†’ first
    457 461
    Token 115: week â†’ week
    461 462
    Token 116: , â†’ ,
    463 471
    Token 117: debuting â†’ debuting
    472 476
    Token 118: atop â†’ atop
    477 480
    Token 119: the â†’ the
    481 490
    Token 120: billboard â†’ Billboard
    491 494
    Token 121: 200 â†’ 200
    494 495
    Token 122: , â†’ ,
    496 499
    Token 123: and â†’ and
    500 506
    Token 124: giving â†’ giving
    507 514
    Token 125: beyonce â†’ BeyoncÃ©
    515 518
    Token 126: her â†’ her
    519 524
    Token 127: third â†’ third
    525 536
    Token 128: consecutive â†’ consecutive
    537 543
    Token 129: number â†’ number
    543 544
    Token 130: - â†’ -
    544 547
    Token 131: one â†’ one
    548 553
    Token 132: album â†’ album
    554 556
    Token 133: in â†’ in
    557 560
    Token 134: the â†’ the
    561 563
    Token 135: us â†’ US
    563 564
    Token 136: . â†’ .
    565 568
    Token 137: the â†’ The
    569 574
    Token 138: album â†’ album
    575 583
    Token 139: featured â†’ featured
    584 587
    Token 140: the â†’ the
    588 594
    Token 141: number â†’ number
    594 595
    Token 142: - â†’ -
    595 598
    Token 143: one â†’ one
    599 603
    Token 144: song â†’ song
    604 605
    Token 145: " â†’ "
    605 611
    Token 146: single â†’ Single
    612 618
    Token 147: ladies â†’ Ladies
    619 620
    Token 148: ( â†’ (
    620 623
    Token 149: put â†’ Put
    624 625
    Token 150: a â†’ a
    626 630
    Token 151: ring â†’ Ring
    631 633
    Token 152: on â†’ on
    634 636
    Token 153: it â†’ It
    636 637
    Token 154: ) â†’ )
    637 638
    Token 155: " â†’ "
    639 642
    Token 156: and â†’ and
    643 646
    Token 157: the â†’ the
    647 650
    Token 158: top â†’ top
    650 651
    Token 159: - â†’ -
    651 655
    Token 160: five â†’ five
    656 661
    Token 161: songs â†’ songs
    662 663
    Token 162: " â†’ "
    663 665
    Token 163: if â†’ If
    666 667
    Token 164: i â†’ I
    668 672
    Token 165: were â†’ Were
    673 674
    Token 166: a â†’ a
    675 678
    Token 167: boy â†’ Boy
    678 679
    Token 168: " â†’ "
    680 683
    Token 169: and â†’ and
    684 685
    Token 170: " â†’ "
    685 689
    Token 171: halo â†’ Halo
    689 690
    Token 172: " â†’ "
    690 691
    Token 173: . â†’ .
    692 701
    Token 174: achieving â†’ Achieving
    702 705
    Token 175: the â†’ the
    706 720
    Token 176: accomplishment â†’ accomplishment
    721 723
    Token 177: of â†’ of
    724 732
    Token 178: becoming â†’ becoming
    733 736
    Token 179: her â†’ her
    737 744
    Token 180: longest â†’ longest
    744 745
    Token 181: - â†’ -
    745 752
    Token 182: running â†’ running
    753 756
    Token 183: hot â†’ Hot
    757 760
    Token 184: 100 â†’ 100
    761 767
    Token 185: single â†’ single
    768 770
    Token 186: in â†’ in
    771 774
    Token 187: her â†’ her
    775 781
    Token 188: career â†’ career
    781 782
    Token 189: , â†’ ,
    783 784
    Token 190: " â†’ "
    784 788
    Token 191: halo â†’ Halo
    788 789
    Token 192: " â†’ "
    789 790
    Token 193: ' â†’ '
    790 791
    Token 194: s â†’ s
    792 799
    Token 195: success â†’ success
    800 802
    Token 196: in â†’ in
    803 806
    Token 197: the â†’ the
    807 809
    Token 198: us â†’ US
    810 816
    Token 199: helped â†’ helped
    817 824
    Token 200: beyonce â†’ BeyoncÃ©
    825 831
    Token 201: attain â†’ attain
    832 836
    Token 202: more â†’ more
    837 840
    Token 203: top â†’ top
    840 841
    Token 204: - â†’ -
    841 844
    Token 205: ten â†’ ten
    845 852
    Token 206: singles â†’ singles
    853 855
    Token 207: on â†’ on
    856 859
    Token 208: the â†’ the
    860 864
    Token 209: list â†’ list
    865 869
    Token 210: than â†’ than
    870 873
    Token 211: any â†’ any
    874 879
    Token 212: other â†’ other
    880 885
    Token 213: woman â†’ woman
    886 892
    Token 214: during â†’ during
    893 896
    Token 215: the â†’ the
    897 902
    Token 216: 2000s â†’ 2000s
    902 903
    Token 217: . â†’ .
    904 906
    Token 218: it â†’ It
    907 911
    Token 219: also â†’ also
    912 920
    Token 220: included â†’ included
    921 924
    Token 221: the â†’ the
    925 935
    Token 222: successful â†’ successful
    936 937
    Token 223: " â†’ "
    937 942
    Token 224: sweet â†’ Sweet
    943 949
    Token 225: dreams â†’ Dreams
    949 950
    Token 226: " â†’ "
    950 951
    Token 227: , â†’ ,
    952 955
    Token 228: and â†’ and
    956 963
    Token 229: singles â†’ singles
    964 965
    Token 230: " â†’ "
    965 969
    Token 231: diva â†’ Diva
    969 970
    Token 232: " â†’ "
    970 971
    Token 233: , â†’ ,
    972 973
    Token 234: " â†’ "
    973 976
    Token 235: ego â†’ Ego
    976 977
    Token 236: " â†’ "
    977 978
    Token 237: , â†’ ,
    979 980
    Token 238: " â†’ "
    980 986
    Token 239: broken â†’ Broken
    986 987
    Token 240: - â†’ -
    987 994
    Token 241: hearted â†’ Hearted
    995 999
    Token 242: girl â†’ Girl
    999 1000
    Token 243: " â†’ "
    1001 1004
    Token 244: and â†’ and
    1005 1006
    Token 245: " â†’ "
    1006 1011
    Token 246: video â†’ Video
    1012 1017
    Token 247: phone â†’ Phone
    1017 1018
    Token 248: " â†’ "
    1018 1019
    Token 249: . â†’ .
    1020 1023
    Token 250: the â†’ The
    1024 1029
    Token 251: music â†’ music
    1030 1035
    Token 252: video â†’ video
    1036 1039
    Token 253: for â†’ for
    1040 1041
    Token 254: " â†’ "
    1041 1047
    Token 255: single â†’ Single
    1048 1054
    Token 256: ladies â†’ Ladies
    1054 1055
    Token 257: " â†’ "
    1056 1059
    Token 258: has â†’ has
    1060 1064
    Token 259: been â†’ been
    1065 1068
    Token 260: par â†’ par
    1068 1070
    Token 261: ##od â†’ od
    1070 1073
    Token 262: ##ied â†’ ied
    1074 1077
    Token 263: and â†’ and
    1078 1080
    Token 264: im â†’ im
    1080 1086
    Token 265: ##itated â†’ itated
    1087 1093
    Token 266: around â†’ around
    1094 1097
    Token 267: the â†’ the
    1098 1103
    Token 268: world â†’ world
    1103 1104
    Token 269: , â†’ ,
    1105 1113
    Token 270: spawning â†’ spawning
    1114 1117
    Token 271: the â†’ the
    1118 1119
    Token 272: " â†’ "
    1119 1124
    Token 273: first â†’ first
    1125 1130
    Token 274: major â†’ major
    1131 1136
    Token 275: dance â†’ dance
    1137 1139
    Token 276: cr â†’ cr
    1139 1141
    Token 277: ##az â†’ az
    1141 1142
    Token 278: ##e â†’ e
    1142 1143
    Token 279: " â†’ "
    1144 1146
    Token 280: of â†’ of
    1147 1150
    Token 281: the â†’ the
    1151 1159
    Token 282: internet â†’ Internet
    1160 1163
    Token 283: age â†’ age
    1164 1173
    Token 284: according â†’ according
    1174 1176
    Token 285: to â†’ to
    1177 1180
    Token 286: the â†’ the
    1181 1188
    Token 287: toronto â†’ Toronto
    1189 1193
    Token 288: star â†’ Star
    1193 1194
    Token 289: . â†’ .
    1195 1198
    Token 290: the â†’ The
    1199 1204
    Token 291: video â†’ video
    1205 1208
    Token 292: has â†’ has
    1209 1212
    Token 293: won â†’ won
    1213 1220
    Token 294: several â†’ several
    1221 1227
    Token 295: awards â†’ awards
    1227 1228
    Token 296: , â†’ ,
    1229 1238
    Token 297: including â†’ including
    1239 1243
    Token 298: best â†’ Best
    1244 1249
    Token 299: video â†’ Video
    1250 1252
    Token 300: at â†’ at
    1253 1256
    Token 301: the â†’ the
    1257 1261
    Token 302: 2009 â†’ 2009
    1262 1265
    Token 303: mtv â†’ MTV
    1266 1272
    Token 304: europe â†’ Europe
    1273 1278
    Token 305: music â†’ Music
    1279 1285
    Token 306: awards â†’ Awards
    1285 1286
    Token 307: , â†’ ,
    1287 1290
    Token 308: the â†’ the
    1291 1295
    Token 309: 2009 â†’ 2009
    1296 1304
    Token 310: scottish â†’ Scottish
    1305 1308
    Token 311: mob â†’ MOB
    1308 1309
    Token 312: ##o â†’ O
    1310 1316
    Token 313: awards â†’ Awards
    1316 1317
    Token 314: , â†’ ,
    1318 1321
    Token 315: and â†’ and
    1322 1325
    Token 316: the â†’ the
    1326 1330
    Token 317: 2009 â†’ 2009
    1331 1334
    Token 318: bet â†’ BET
    1335 1341
    Token 319: awards â†’ Awards
    1341 1342
    Token 320: . â†’ .
    1343 1345
    Token 321: at â†’ At
    1346 1349
    Token 322: the â†’ the
    1350 1354
    Token 323: 2009 â†’ 2009
    1355 1358
    Token 324: mtv â†’ MTV
    1359 1364
    Token 325: video â†’ Video
    1365 1370
    Token 326: music â†’ Music
    1371 1377
    Token 327: awards â†’ Awards
    1377 1378
    Token 328: , â†’ ,
    1379 1382
    Token 329: the â†’ the
    1383 1388
    Token 330: video â†’ video
    1389 1392
    Token 331: was â†’ was
    1393 1402
    Token 332: nominated â†’ nominated
    1403 1406
    Token 333: for â†’ for
    1407 1411
    Token 334: nine â†’ nine
    1412 1418
    Token 335: awards â†’ awards
    1418 1419
    Token 336: , â†’ ,
    1420 1430
    Token 337: ultimately â†’ ultimately
    1431 1438
    Token 338: winning â†’ winning
    1439 1444
    Token 339: three â†’ three
    1445 1454
    Token 340: including â†’ including
    1455 1460
    Token 341: video â†’ Video
    1461 1463
    Token 342: of â†’ of
    1464 1467
    Token 343: the â†’ the
    1468 1472
    Token 344: year â†’ Year
    1472 1473
    Token 345: . â†’ .
    1474 1477
    Token 346: its â†’ Its
    1478 1485
    Token 347: failure â†’ failure
    1486 1488
    Token 348: to â†’ to
    1489 1492
    Token 349: win â†’ win
    1493 1496
    Token 350: the â†’ the
    1497 1501
    Token 351: best â†’ Best
    1502 1508
    Token 352: female â†’ Female
    1509 1514
    Token 353: video â†’ Video
    1515 1523
    Token 354: category â†’ category
    1523 1524
    Token 355: , â†’ ,
    1525 1530
    Token 356: which â†’ which
    1531 1535
    Token 357: went â†’ went
    1536 1538
    Token 358: to â†’ to
    1539 1547
    Token 359: american â†’ American
    1548 1555
    Token 360: country â†’ country
    1556 1559
    Token 361: pop â†’ pop
    1560 1566
    Token 362: singer â†’ singer
    1567 1573
    Token 363: taylor â†’ Taylor
    1574 1579
    Token 364: swift â†’ Swift
    1579 1580
    Token 365: ' â†’ '
    1580 1581
    Token 366: s â†’ s
    1582 1583
    Token 367: " â†’ "
    1583 1586
    Token 368: you â†’ You
    1587 1593
    Token 369: belong â†’ Belong
    1594 1598
    Token 370: with â†’ with
    1599 1601
    Token 371: me â†’ Me
    1601 1602
    Token 372: " â†’ "
    1602 1603
    Token 373: , â†’ ,
    1604 1607
    Token 374: led â†’ led
    1608 1610
    Token 375: to â†’ to
    1611 1616
    Token 376: kanye â†’ Kanye
    1617 1621
    Token 377: west â†’ West
    1622 1634
    Token 378: interrupting â†’ interrupting
    1635 1638
    Token 379: the â†’ the
    1639 1647
    Token 380: ceremony â†’ ceremony
    1648 1651
    Token 381: and â†’ and
    1652 1659
    Token 382: beyonce â†’ BeyoncÃ©
    0 0
    Token 383: [SEP] â†’ 
    
    === åˆ†å— 1 ===
    0 0
    Token 0: [CLS] â†’ 
    0 7
    Token 1: beyonce â†’ Beyonce
    8 11
    Token 2: got â†’ got
    12 19
    Token 3: married â†’ married
    20 22
    Token 4: in â†’ in
    23 27
    Token 5: 2008 â†’ 2008
    28 30
    Token 6: to â†’ to
    31 35
    Token 7: whom â†’ whom
    35 36
    Token 8: ? â†’ ?
    0 0
    Token 9: [SEP] â†’ 
    1041 1047
    Token 10: single â†’ Single
    1048 1054
    Token 11: ladies â†’ Ladies
    1054 1055
    Token 12: " â†’ "
    1056 1059
    Token 13: has â†’ has
    1060 1064
    Token 14: been â†’ been
    1065 1068
    Token 15: par â†’ par
    1068 1070
    Token 16: ##od â†’ od
    1070 1073
    Token 17: ##ied â†’ ied
    1074 1077
    Token 18: and â†’ and
    1078 1080
    Token 19: im â†’ im
    1080 1086
    Token 20: ##itated â†’ itated
    1087 1093
    Token 21: around â†’ around
    1094 1097
    Token 22: the â†’ the
    1098 1103
    Token 23: world â†’ world
    1103 1104
    Token 24: , â†’ ,
    1105 1113
    Token 25: spawning â†’ spawning
    1114 1117
    Token 26: the â†’ the
    1118 1119
    Token 27: " â†’ "
    1119 1124
    Token 28: first â†’ first
    1125 1130
    Token 29: major â†’ major
    1131 1136
    Token 30: dance â†’ dance
    1137 1139
    Token 31: cr â†’ cr
    1139 1141
    Token 32: ##az â†’ az
    1141 1142
    Token 33: ##e â†’ e
    1142 1143
    Token 34: " â†’ "
    1144 1146
    Token 35: of â†’ of
    1147 1150
    Token 36: the â†’ the
    1151 1159
    Token 37: internet â†’ Internet
    1160 1163
    Token 38: age â†’ age
    1164 1173
    Token 39: according â†’ according
    1174 1176
    Token 40: to â†’ to
    1177 1180
    Token 41: the â†’ the
    1181 1188
    Token 42: toronto â†’ Toronto
    1189 1193
    Token 43: star â†’ Star
    1193 1194
    Token 44: . â†’ .
    1195 1198
    Token 45: the â†’ The
    1199 1204
    Token 46: video â†’ video
    1205 1208
    Token 47: has â†’ has
    1209 1212
    Token 48: won â†’ won
    1213 1220
    Token 49: several â†’ several
    1221 1227
    Token 50: awards â†’ awards
    1227 1228
    Token 51: , â†’ ,
    1229 1238
    Token 52: including â†’ including
    1239 1243
    Token 53: best â†’ Best
    1244 1249
    Token 54: video â†’ Video
    1250 1252
    Token 55: at â†’ at
    1253 1256
    Token 56: the â†’ the
    1257 1261
    Token 57: 2009 â†’ 2009
    1262 1265
    Token 58: mtv â†’ MTV
    1266 1272
    Token 59: europe â†’ Europe
    1273 1278
    Token 60: music â†’ Music
    1279 1285
    Token 61: awards â†’ Awards
    1285 1286
    Token 62: , â†’ ,
    1287 1290
    Token 63: the â†’ the
    1291 1295
    Token 64: 2009 â†’ 2009
    1296 1304
    Token 65: scottish â†’ Scottish
    1305 1308
    Token 66: mob â†’ MOB
    1308 1309
    Token 67: ##o â†’ O
    1310 1316
    Token 68: awards â†’ Awards
    1316 1317
    Token 69: , â†’ ,
    1318 1321
    Token 70: and â†’ and
    1322 1325
    Token 71: the â†’ the
    1326 1330
    Token 72: 2009 â†’ 2009
    1331 1334
    Token 73: bet â†’ BET
    1335 1341
    Token 74: awards â†’ Awards
    1341 1342
    Token 75: . â†’ .
    1343 1345
    Token 76: at â†’ At
    1346 1349
    Token 77: the â†’ the
    1350 1354
    Token 78: 2009 â†’ 2009
    1355 1358
    Token 79: mtv â†’ MTV
    1359 1364
    Token 80: video â†’ Video
    1365 1370
    Token 81: music â†’ Music
    1371 1377
    Token 82: awards â†’ Awards
    1377 1378
    Token 83: , â†’ ,
    1379 1382
    Token 84: the â†’ the
    1383 1388
    Token 85: video â†’ video
    1389 1392
    Token 86: was â†’ was
    1393 1402
    Token 87: nominated â†’ nominated
    1403 1406
    Token 88: for â†’ for
    1407 1411
    Token 89: nine â†’ nine
    1412 1418
    Token 90: awards â†’ awards
    1418 1419
    Token 91: , â†’ ,
    1420 1430
    Token 92: ultimately â†’ ultimately
    1431 1438
    Token 93: winning â†’ winning
    1439 1444
    Token 94: three â†’ three
    1445 1454
    Token 95: including â†’ including
    1455 1460
    Token 96: video â†’ Video
    1461 1463
    Token 97: of â†’ of
    1464 1467
    Token 98: the â†’ the
    1468 1472
    Token 99: year â†’ Year
    1472 1473
    Token 100: . â†’ .
    1474 1477
    Token 101: its â†’ Its
    1478 1485
    Token 102: failure â†’ failure
    1486 1488
    Token 103: to â†’ to
    1489 1492
    Token 104: win â†’ win
    1493 1496
    Token 105: the â†’ the
    1497 1501
    Token 106: best â†’ Best
    1502 1508
    Token 107: female â†’ Female
    1509 1514
    Token 108: video â†’ Video
    1515 1523
    Token 109: category â†’ category
    1523 1524
    Token 110: , â†’ ,
    1525 1530
    Token 111: which â†’ which
    1531 1535
    Token 112: went â†’ went
    1536 1538
    Token 113: to â†’ to
    1539 1547
    Token 114: american â†’ American
    1548 1555
    Token 115: country â†’ country
    1556 1559
    Token 116: pop â†’ pop
    1560 1566
    Token 117: singer â†’ singer
    1567 1573
    Token 118: taylor â†’ Taylor
    1574 1579
    Token 119: swift â†’ Swift
    1579 1580
    Token 120: ' â†’ '
    1580 1581
    Token 121: s â†’ s
    1582 1583
    Token 122: " â†’ "
    1583 1586
    Token 123: you â†’ You
    1587 1593
    Token 124: belong â†’ Belong
    1594 1598
    Token 125: with â†’ with
    1599 1601
    Token 126: me â†’ Me
    1601 1602
    Token 127: " â†’ "
    1602 1603
    Token 128: , â†’ ,
    1604 1607
    Token 129: led â†’ led
    1608 1610
    Token 130: to â†’ to
    1611 1616
    Token 131: kanye â†’ Kanye
    1617 1621
    Token 132: west â†’ West
    1622 1634
    Token 133: interrupting â†’ interrupting
    1635 1638
    Token 134: the â†’ the
    1639 1647
    Token 135: ceremony â†’ ceremony
    1648 1651
    Token 136: and â†’ and
    1652 1659
    Token 137: beyonce â†’ BeyoncÃ©
    1660 1663
    Token 138: imp â†’ imp
    1663 1666
    Token 139: ##rov â†’ rov
    1666 1671
    Token 140: ##ising â†’ ising
    1672 1673
    Token 141: a â†’ a
    1674 1676
    Token 142: re â†’ re
    1676 1677
    Token 143: - â†’ -
    1677 1689
    Token 144: presentation â†’ presentation
    1690 1692
    Token 145: of â†’ of
    1693 1698
    Token 146: swift â†’ Swift
    1698 1699
    Token 147: ' â†’ '
    1699 1700
    Token 148: s â†’ s
    1701 1706
    Token 149: award â†’ award
    1707 1713
    Token 150: during â†’ during
    1714 1717
    Token 151: her â†’ her
    1718 1721
    Token 152: own â†’ own
    1722 1732
    Token 153: acceptance â†’ acceptance
    1733 1739
    Token 154: speech â†’ speech
    1739 1740
    Token 155: . â†’ .
    1741 1743
    Token 156: in â†’ In
    1744 1749
    Token 157: march â†’ March
    1750 1754
    Token 158: 2009 â†’ 2009
    1754 1755
    Token 159: , â†’ ,
    1756 1763
    Token 160: beyonce â†’ BeyoncÃ©
    1764 1772
    Token 161: embarked â†’ embarked
    1773 1775
    Token 162: on â†’ on
    1776 1779
    Token 163: the â†’ the
    1780 1781
    Token 164: i â†’ I
    1782 1784
    Token 165: am â†’ Am
    1784 1785
    Token 166: . â†’ .
    1785 1786
    Token 167: . â†’ .
    1786 1787
    Token 168: . â†’ .
    1788 1793
    Token 169: world â†’ World
    1794 1798
    Token 170: tour â†’ Tour
    1798 1799
    Token 171: , â†’ ,
    1800 1803
    Token 172: her â†’ her
    1804 1810
    Token 173: second â†’ second
    1811 1821
    Token 174: headlining â†’ headlining
    1822 1831
    Token 175: worldwide â†’ worldwide
    1832 1839
    Token 176: concert â†’ concert
    1840 1844
    Token 177: tour â†’ tour
    1844 1845
    Token 178: , â†’ ,
    1846 1856
    Token 179: consisting â†’ consisting
    1857 1859
    Token 180: of â†’ of
    1860 1863
    Token 181: 108 â†’ 108
    1864 1869
    Token 182: shows â†’ shows
    1869 1870
    Token 183: , â†’ ,
    1871 1879
    Token 184: grossing â†’ grossing
    1880 1881
    Token 185: $ â†’ $
    1881 1884
    Token 186: 119 â†’ 119
    1884 1885
    Token 187: . â†’ .
    1885 1886
    Token 188: 5 â†’ 5
    1887 1894
    Token 189: million â†’ million
    1894 1895
    Token 190: . â†’ .
    0 0
    Token 191: [SEP] â†’ 


ç”¨æœ€ç®€å•çš„æ¯”å–»è§£é‡Šè¿™æ®µä»£ç ï¼š

**1. åˆ†å—ï¼ˆåˆ‡ä¹¦ï¼‰**  
- å°±åƒä¸€æœ¬åšä¹¦æ‹†æˆå‡ æœ¬å°å†Œå­ï¼Œæ¯æœ¬æœ€å¤š512é¡µï¼ˆæ¨¡å‹ä¸€æ¬¡è¯»ä¸å®Œé•¿æ–‡æœ¬ï¼‰

**2. æ–‡å­—å˜æ•°å­—ï¼ˆåŠ å¯†ï¼‰**  
- æŠŠæ¯ä¸ªå­—å˜æˆæ•°å­—å¯†ç ï¼Œæ¯”å¦‚ "è´"â†’100ï¼Œ"çˆ·"â†’101  
- `input_ids` å°±æ˜¯è¿™äº›å¯†ç ç»„æˆçš„åˆ—è¡¨ï¼š[100, 101, ...]

**3. è®°ä½ç½®ï¼ˆä¹¦ç­¾ï¼‰**  
- `offset_mapping` è®°å½•æ¯ä¸ªå¯†ç åœ¨åŸæ–‡çš„ä½ç½®ï¼Œæ¯”å¦‚ (0,2) è¡¨ç¤ºå‰ä¸¤ä¸ªå­—

**4. åŒºåˆ†é—®é¢˜å’Œç­”æ¡ˆï¼ˆè´´æ ‡ç­¾ï¼‰**  
- `token_type_ids=0` è¡¨ç¤ºæ–‡å­—æ¥è‡ªé—®é¢˜ï¼ˆå¦‚ "è´çˆ·å“ªå¹´ç»“å©šï¼Ÿ"ï¼‰  
- `token_type_ids=1` è¡¨ç¤ºæ–‡å­—æ¥è‡ªç­”æ¡ˆï¼ˆå¦‚ "2000å¹´..."ï¼‰

**5. æ‰¾å¯¹åº”æ–‡å­—ï¼ˆè§£å¯†ï¼‰**  
- ç”¨å¯†ç æœ¬æŠŠæ•°å­—è½¬å›æ–‡å­—  
- æ ¹æ®ä½ç½®æ ‡ç­¾ï¼Œä»é—®é¢˜æˆ–ç­”æ¡ˆæ–‡æœ¬æˆªå–å¯¹åº”æ–‡å­—

**å°±åƒè¿™æ ·ï¼š**  
å¯†ç  `100` â†’ æŸ¥å¯†ç æœ¬ â†’ æ˜¯"è´" â†’ åœ¨é—®é¢˜ç¬¬0-2ä¸ªä½ç½® â†’ æˆªå–"è´çˆ·"

æ•´ä¸ªè¿‡ç¨‹è®©è®¡ç®—æœºåƒäººç±»ä¸€æ ·ï¼šå…ˆçœ‹é—®é¢˜ï¼Œå†å¿«é€Ÿç¿»ä¹¦æ‰¾ç­”æ¡ˆä½ç½®ã€‚


```python
example["question"]
```




    'Beyonce got married in 2008 to whom?'




```python
example["context"]
```




    'On April 4, 2008, BeyoncÃ© married Jay Z. She publicly revealed their marriage in a video montage at the listening party for her third studio album, I Am... Sasha Fierce, in Manhattan\'s Sony Club on October 22, 2008. I Am... Sasha Fierce was released on November 18, 2008 in the United States. The album formally introduces BeyoncÃ©\'s alter ego Sasha Fierce, conceived during the making of her 2003 single "Crazy in Love", selling 482,000 copies in its first week, debuting atop the Billboard 200, and giving BeyoncÃ© her third consecutive number-one album in the US. The album featured the number-one song "Single Ladies (Put a Ring on It)" and the top-five songs "If I Were a Boy" and "Halo". Achieving the accomplishment of becoming her longest-running Hot 100 single in her career, "Halo"\'s success in the US helped BeyoncÃ© attain more top-ten singles on the list than any other woman during the 2000s. It also included the successful "Sweet Dreams", and singles "Diva", "Ego", "Broken-Hearted Girl" and "Video Phone". The music video for "Single Ladies" has been parodied and imitated around the world, spawning the "first major dance craze" of the Internet age according to the Toronto Star. The video has won several awards, including Best Video at the 2009 MTV Europe Music Awards, the 2009 Scottish MOBO Awards, and the 2009 BET Awards. At the 2009 MTV Video Music Awards, the video was nominated for nine awards, ultimately winning three including Video of the Year. Its failure to win the Best Female Video category, which went to American country pop singer Taylor Swift\'s "You Belong with Me", led to Kanye West interrupting the ceremony and BeyoncÃ© improvising a re-presentation of Swift\'s award during her own acceptance speech. In March 2009, BeyoncÃ© embarked on the I Am... World Tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $119.5 million.'



å€ŸåŠ©`tokenized_example`çš„`sequence_ids`æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿çš„åŒºåˆ†tokençš„æ¥æºç¼–å·ï¼š

- å¯¹äºç‰¹æ®Šæ ‡è®°ï¼šè¿”å›Noneï¼Œ
- å¯¹äºæ­£æ–‡Tokenï¼šè¿”å›å¥å­ç¼–å·ï¼ˆä»0å¼€å§‹ç¼–å·ï¼‰ã€‚

ç»¼ä¸Šï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿çš„åœ¨ä¸€ä¸ªè¾“å…¥ç‰¹å¾ä¸­æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸ Tokenã€‚


```python
sequence_ids = tokenized_example.sequence_ids()
print(sequence_ids)
```

    [None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]



```python
answers = example["answers"]
start_char = answers["answer_start"][0]
end_char = start_char + len(answers["text"][0])

# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹æ ‡è®°ç´¢å¼•ã€‚
token_start_index = 0
while sequence_ids[token_start_index] != 1:
    token_start_index += 1

# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„ç»“æŸæ ‡è®°ç´¢å¼•ã€‚
token_end_index = len(tokenized_example["input_ids"][0]) - 1
while sequence_ids[token_end_index] != 1:
    token_end_index -= 1

# æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºspanèŒƒå›´ï¼ˆå¦‚æœè¶…å‡ºèŒƒå›´ï¼Œè¯¥ç‰¹å¾å°†ä»¥CLSæ ‡è®°ç´¢å¼•æ ‡è®°ï¼‰ã€‚
offsets = tokenized_example["offset_mapping"][0]
if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
    # å°†token_start_indexå’Œtoken_end_indexç§»åŠ¨åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚
    # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç§»åˆ°æœ€åä¸€ä¸ªæ ‡è®°ä¹‹åï¼ˆè¾¹ç•Œæƒ…å†µï¼‰ã€‚
    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
        token_start_index += 1
    start_position = token_start_index - 1
    while offsets[token_end_index][1] >= end_char:
        token_end_index -= 1
    end_position = token_end_index + 1
    print(start_position, end_position)
else:
    print("ç­”æ¡ˆä¸åœ¨æ­¤ç‰¹å¾ä¸­ã€‚")

```

    18 19


æ‰“å°æ£€æŸ¥æ˜¯å¦å‡†ç¡®æ‰¾åˆ°äº†èµ·å§‹ä½ç½®ï¼š


```python
# é€šè¿‡æŸ¥æ‰¾ offset mapping ä½ç½®ï¼Œè§£ç  context ä¸­çš„ç­”æ¡ˆ 
print(tokenizer.decode(tokenized_example["input_ids"][0][start_position: end_position+1]))
# ç›´æ¥æ‰“å° æ•°æ®é›†ä¸­çš„æ ‡å‡†ç­”æ¡ˆï¼ˆanswer["text"])
print(answers["text"][0])
```

    jay z
    Jay Z


#### å…³äºå¡«å……çš„ç­–ç•¥

- å¯¹äºæ²¡æœ‰è¶…è¿‡æœ€å¤§é•¿åº¦çš„æ–‡æœ¬ï¼Œå¡«å……è¡¥é½é•¿åº¦ã€‚
- å¯¹äºéœ€è¦å·¦ä¾§å¡«å……çš„æ¨¡å‹ï¼Œäº¤æ¢ question å’Œ context é¡ºåº


```python
pad_on_right = tokenizer.padding_side == "right"
```

### æ•´åˆä»¥ä¸Šæ‰€æœ‰é¢„å¤„ç†æ­¥éª¤

è®©æˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹æ•´åˆåˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°è®­ç»ƒé›†ã€‚

é’ˆå¯¹ä¸å¯å›ç­”çš„æƒ…å†µï¼ˆä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œç­”æ¡ˆåœ¨å¦ä¸€ä¸ªç‰¹å¾ä¸­ï¼‰ï¼Œæˆ‘ä»¬ä¸ºå¼€å§‹å’Œç»“æŸä½ç½®éƒ½è®¾ç½®äº†clsç´¢å¼•ã€‚

å¦‚æœallow_impossible_answersæ ‡å¿—ä¸ºFalseï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç®€å•åœ°ä»è®­ç»ƒé›†ä¸­ä¸¢å¼ƒè¿™äº›ç¤ºä¾‹ã€‚


```python
def prepare_train_features(examples):
    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§å¯èƒ½æœ‰å¾ˆå¤šç©ºç™½å­—ç¬¦ï¼Œè¿™å¯¹æˆ‘ä»¬æ²¡æœ‰ç”¨ï¼Œè€Œä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡çš„æˆªæ–­å¤±è´¥
    # ï¼ˆæ ‡è®°åŒ–çš„é—®é¢˜å°†å ç”¨å¤§é‡ç©ºé—´ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ é™¤å·¦ä¾§çš„ç©ºç™½å­—ç¬¦ã€‚
    examples["question"] = [q.lstrip() for q in examples["question"]]

    # ä½¿ç”¨æˆªæ–­å’Œå¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†ä¿ç•™æº¢å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨æ­¥å¹…ï¼ˆstrideï¼‰ã€‚
    # å½“ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªç¤ºä¾‹å¯èƒ½æä¾›å¤šä¸ªç‰¹å¾ï¼Œå…¶ä¸­æ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡æœ‰ä¸€äº›é‡å ã€‚
    tokenized_examples = tokenizer(
        examples["question" if pad_on_right else "context"],
        examples["context" if pad_on_right else "question"],
        truncation="only_second" if pad_on_right else "only_first",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç»™æˆ‘ä»¬æä¾›å¤šä¸ªç‰¹å¾ï¼ˆå¦‚æœå®ƒå…·æœ‰å¾ˆé•¿çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æä¾›äº†è¿™ä¸ªæ˜ å°„å…³ç³»ã€‚
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")
    # åç§»æ˜ å°„å°†ä¸ºæˆ‘ä»¬æä¾›ä»ä»¤ç‰Œåˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬è®¡ç®—å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ã€‚
    offset_mapping = tokenized_examples.pop("offset_mapping")

    # è®©æˆ‘ä»¬ä¸ºè¿™äº›ç¤ºä¾‹è¿›è¡Œæ ‡è®°ï¼
    tokenized_examples["start_positions"] = []
    tokenized_examples["end_positions"] = []

    for i, offsets in enumerate(offset_mapping):
        # æˆ‘ä»¬å°†ä½¿ç”¨ CLS ç‰¹æ®Š token çš„ç´¢å¼•æ¥æ ‡è®°ä¸å¯èƒ½çš„ç­”æ¡ˆã€‚
        input_ids = tokenized_examples["input_ids"][i]
        cls_index = input_ids.index(tokenizer.cls_token_id)

        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£ä¸Šä¸‹æ–‡å’Œé—®é¢˜æ˜¯ä»€ä¹ˆï¼‰ã€‚
        sequence_ids = tokenized_examples.sequence_ids(i)

        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥æä¾›å¤šä¸ªè·¨åº¦ï¼Œè¿™æ˜¯åŒ…å«æ­¤æ–‡æœ¬è·¨åº¦çš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]
        # å¦‚æœæ²¡æœ‰ç»™å‡ºç­”æ¡ˆï¼Œåˆ™å°†cls_indexè®¾ç½®ä¸ºç­”æ¡ˆã€‚
        if len(answers["answer_start"]) == 0:
            tokenized_examples["start_positions"].append(cls_index)
            tokenized_examples["end_positions"].append(cls_index)
        else:
            # ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹å’Œç»“æŸå­—ç¬¦ç´¢å¼•ã€‚
            start_char = answers["answer_start"][0]
            end_char = start_char + len(answers["text"][0])

            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹ä»¤ç‰Œç´¢å¼•ã€‚
            token_start_index = 0
            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
                token_start_index += 1

            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„ç»“æŸä»¤ç‰Œç´¢å¼•ã€‚
            token_end_index = len(input_ids) - 1
            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
                token_end_index -= 1

            # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºè·¨åº¦ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥ç‰¹å¾çš„æ ‡ç­¾å°†ä½¿ç”¨CLSç´¢å¼•ï¼‰ã€‚
            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                # å¦åˆ™ï¼Œå°†token_start_indexå’Œtoken_end_indexç§»åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚
                # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼ˆè¾¹ç¼˜æƒ…å†µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€åä¸€ä¸ªåç§»ä¹‹åç»§ç»­ã€‚
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
                    token_start_index += 1
                tokenized_examples["start_positions"].append(token_start_index - 1)
                while offsets[token_end_index][1] >= end_char:
                    token_end_index -= 1
                tokenized_examples["end_positions"].append(token_end_index + 1)

    return tokenized_examples
```

#### datasets.map çš„è¿›é˜¶ä½¿ç”¨

ä½¿ç”¨ `datasets.map` æ–¹æ³•å°† `prepare_train_features` åº”ç”¨äºæ‰€æœ‰è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®ï¼š

- batched: æ‰¹é‡å¤„ç†æ•°æ®ã€‚
- remove_columns: å› ä¸ºé¢„å¤„ç†æ›´æ”¹äº†æ ·æœ¬çš„æ•°é‡ï¼Œæ‰€ä»¥åœ¨åº”ç”¨å®ƒæ—¶éœ€è¦åˆ é™¤æ—§åˆ—ã€‚
- load_from_cache_fileï¼šæ˜¯å¦ä½¿ç”¨datasetsåº“çš„è‡ªåŠ¨ç¼“å­˜

datasets åº“é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®ï¼Œå®ç°äº†é«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹ä¼ é€’ç»™ map çš„å‡½æ•°æ˜¯å¦å·²æ›´æ”¹ï¼ˆå› æ­¤éœ€è¦ä¸ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰ã€‚å¦‚æœåœ¨è°ƒç”¨ map æ—¶è®¾ç½® `load_from_cache_file=False`ï¼Œå¯ä»¥å¼ºåˆ¶é‡æ–°åº”ç”¨é¢„å¤„ç†ã€‚


```python
tokenized_datasets = datasets.map(prepare_train_features,
                                  batched=True,
                                  remove_columns=datasets["train"].column_names)
```


    Map:   0%|          | 0/87599 [00:00<?, ? examples/s]



    Map:   0%|          | 0/10570 [00:00<?, ? examples/s]


## å¾®è°ƒæ¨¡å‹

ç°åœ¨æˆ‘ä»¬çš„æ•°æ®å·²ç»å‡†å¤‡å¥½ç”¨äºè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒã€‚

ç”±äºæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯é—®ç­”ï¼Œæˆ‘ä»¬ä½¿ç”¨ `AutoModelForQuestionAnswering` ç±»ã€‚(å¯¹æ¯” Yelp è¯„è®ºæ‰“åˆ†ä½¿ç”¨çš„æ˜¯ `AutoModelForSequenceClassification` ç±»ï¼‰

è­¦å‘Šé€šçŸ¥æˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆ`vocab_transform` å’Œ `vocab_layer_norm` å±‚ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡ï¼ˆ`pre_classifier` å’Œ `classifier` å±‚ï¼‰ã€‚åœ¨å¾®è°ƒæ¨¡å‹æƒ…å†µä¸‹æ˜¯ç»å¯¹æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ é™¤ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å¤´éƒ¨ï¼Œå¹¶ç”¨ä¸€ä¸ªæ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒï¼Œå¯¹äºè¿™ä¸ªæ–°å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“ä¼šè­¦å‘Šæˆ‘ä»¬åœ¨ç”¨å®ƒè¿›è¡Œæ¨ç†ä¹‹å‰åº”è¯¥å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ã€‚


```python
from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer

model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)
```

    Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


#### è®­ç»ƒè¶…å‚æ•°ï¼ˆTrainingArgumentsï¼‰


```python
batch_size=64
model_dir = f"models/{model_checkpoint}-finetuned-squad"

args = TrainingArguments(
    output_dir=model_dir,
    evaluation_strategy = "epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    num_train_epochs=3,
    weight_decay=0.01,
)
```

#### Data Collatorï¼ˆæ•°æ®æ•´ç†å™¨ï¼‰

æ•°æ®æ•´ç†å™¨å°†è®­ç»ƒæ•°æ®æ•´ç†ä¸ºæ‰¹æ¬¡æ•°æ®ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒæ—¶çš„æ‰¹æ¬¡å¤„ç†ã€‚æœ¬æ•™ç¨‹ä½¿ç”¨é»˜è®¤çš„ `default_data_collator`ã€‚



```python
from transformers import default_data_collator

data_collator = default_data_collator
```

### å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼ˆTrainerï¼‰

ä¸ºäº†å‡å°‘è®­ç»ƒæ—¶é—´ï¼ˆéœ€è¦å¤§é‡ç®—åŠ›æ”¯æŒï¼‰ï¼Œæˆ‘ä»¬ä¸åœ¨æœ¬æ•™ç¨‹çš„è®­ç»ƒæ¨¡å‹è¿‡ç¨‹ä¸­è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€‚

è€Œæ˜¯è®­ç»ƒå®Œæˆåï¼Œå†ç‹¬ç«‹è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚


```python
trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

    Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.


#### GPU ä½¿ç”¨æƒ…å†µ

è®­ç»ƒæ•°æ®ä¸æ¨¡å‹é…ç½®ï¼š

- SQUAD v1.1
- model_checkpoint = "distilbert-base-uncased"
- batch_size = 64

NVIDIA GPU ä½¿ç”¨æƒ…å†µï¼š

```shell
Every 1.0s: nvidia-smi                                                   Wed Dec 20 15:39:57 2023

Wed Dec 20 15:39:57 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |
| N/A   67C    P0              67W /  70W |  14617MiB / 15360MiB |    100%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     16384      C   /root/miniconda3/bin/python               14612MiB |
+---------------------------------------------------------------------------------------+
```


```python
trainer.train()
```



    <div>

      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [4152/4152 2:23:19, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1.491100</td>
      <td>1.249441</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.108800</td>
      <td>1.161671</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.975700</td>
      <td>1.158766</td>
    </tr>
  </tbody>
</table><p>


    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3500 already exists and is non-empty.Saving will proceed but saved results may be invalid.
    Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-4000 already exists and is non-empty.Saving will proceed but saved results may be invalid.





    TrainOutput(global_step=4152, training_loss=1.3038662743246854, metrics={'train_runtime': 8602.4737, 'train_samples_per_second': 30.872, 'train_steps_per_second': 0.483, 'total_flos': 2.602335381127373e+16, 'train_loss': 1.3038662743246854, 'epoch': 3.0})



### è®­ç»ƒå®Œæˆåï¼Œç¬¬ä¸€æ—¶é—´ä¿å­˜æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚


```python
model_to_save = trainer.save_model(model_dir)
```

## æ¨¡å‹è¯„ä¼°

**è¯„ä¼°æ¨¡å‹è¾“å‡ºéœ€è¦ä¸€äº›é¢å¤–çš„å¤„ç†ï¼šå°†æ¨¡å‹çš„é¢„æµ‹æ˜ å°„å›ä¸Šä¸‹æ–‡çš„éƒ¨åˆ†ã€‚**

æ¨¡å‹ç›´æ¥è¾“å‡ºçš„æ˜¯é¢„æµ‹ç­”æ¡ˆçš„`èµ·å§‹ä½ç½®`å’Œ`ç»“æŸä½ç½®`çš„**logits**


```python
import torch

for batch in trainer.get_eval_dataloader():
    break
batch = {k: v.to(trainer.args.device) for k, v in batch.items()}
with torch.no_grad():
    output = trainer.model(**batch)
output.keys()
```




    odict_keys(['loss', 'start_logits', 'end_logits'])



æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç±»ä¼¼å­—å…¸çš„å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æŸå¤±ï¼ˆå› ä¸ºæˆ‘ä»¬æä¾›äº†æ ‡ç­¾ï¼‰ï¼Œä»¥åŠèµ·å§‹å’Œç»“æŸlogitsã€‚æˆ‘ä»¬ä¸éœ€è¦æŸå¤±æ¥è¿›è¡Œé¢„æµ‹ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹logitsï¼š


```python
output.start_logits.shape, output.end_logits.shape
```




    (torch.Size([64, 384]), torch.Size([64, 384]))




```python
output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)
```




    (tensor([ 46,  57,  78,  43, 118, 108,  72,  35, 108,  34,  73,  41,  80,  91,
             156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  41,  35,  42,  77,
              11,  44,  27, 133,  66,  40,  87,  44,  43,  41, 127,  26,  28,  33,
              87, 127,  95,  25,  43, 132,  42,  29,  44,  46,  24,  44,  65,  58,
              81,  14,  59,  72,  25,  36,  57,  43], device='cuda:0'),
     tensor([ 47,  58,  81,  44, 118, 109,  75,  37, 109,  36,  76,  42,  83,  94,
             158,  35,  83,  94,  83,  60,  80,  31,  43,  54,  42,  35,  43,  80,
              13,  45,  28, 133,  66,  41,  89,  45,  44,  42, 127,  27,  30,  34,
              32, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,
              81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))



#### å¦‚ä½•ä»æ¨¡å‹è¾“å‡ºçš„ä½ç½® logit ç»„åˆæˆç­”æ¡ˆ

æˆ‘ä»¬æœ‰æ¯ä¸ªç‰¹å¾å’Œæ¯ä¸ªæ ‡è®°çš„logitã€‚åœ¨æ¯ä¸ªç‰¹å¾ä¸­ä¸ºæ¯ä¸ªæ ‡è®°é¢„æµ‹ç­”æ¡ˆæœ€æ˜æ˜¾çš„æ–¹æ³•æ˜¯ï¼Œå°†èµ·å§‹logitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºèµ·å§‹ä½ç½®ï¼Œå°†ç»“æŸlogitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºç»“æŸä½ç½®ã€‚

åœ¨è®¸å¤šæƒ…å†µä¸‹è¿™ç§æ–¹å¼æ•ˆæœå¾ˆå¥½ï¼Œä½†æ˜¯å¦‚æœæ­¤é¢„æµ‹ç»™å‡ºäº†ä¸å¯èƒ½çš„ç»“æœè¯¥æ€ä¹ˆåŠï¼Ÿæ¯”å¦‚ï¼šèµ·å§‹ä½ç½®å¯èƒ½å¤§äºç»“æŸä½ç½®ï¼Œæˆ–è€…æŒ‡å‘é—®é¢˜ä¸­çš„æ–‡æœ¬ç‰‡æ®µè€Œä¸æ˜¯ç­”æ¡ˆã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æŸ¥çœ‹ç¬¬äºŒå¥½çš„é¢„æµ‹ï¼Œçœ‹å®ƒæ˜¯å¦ç»™å‡ºäº†ä¸€ä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼Œå¹¶é€‰æ‹©å®ƒã€‚

é€‰æ‹©ç¬¬äºŒå¥½çš„ç­”æ¡ˆå¹¶ä¸åƒé€‰æ‹©æœ€ä½³ç­”æ¡ˆé‚£ä¹ˆå®¹æ˜“ï¼š
- å®ƒæ˜¯èµ·å§‹logitsä¸­ç¬¬äºŒä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­æœ€ä½³ç´¢å¼•å—ï¼Ÿ
- è¿˜æ˜¯èµ·å§‹logitsä¸­æœ€ä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­ç¬¬äºŒä½³ç´¢å¼•ï¼Ÿ
- å¦‚æœç¬¬äºŒå¥½çš„ç­”æ¡ˆä¹Ÿä¸å¯èƒ½ï¼Œé‚£ä¹ˆå¯¹äºç¬¬ä¸‰å¥½çš„ç­”æ¡ˆï¼Œæƒ…å†µä¼šæ›´åŠ æ£˜æ‰‹ã€‚

ä¸ºäº†å¯¹ç­”æ¡ˆè¿›è¡Œåˆ†ç±»ï¼Œ
1. å°†ä½¿ç”¨é€šè¿‡æ·»åŠ èµ·å§‹å’Œç»“æŸlogitsè·å¾—çš„åˆ†æ•°
1. è®¾è®¡ä¸€ä¸ªåä¸º`n_best_size`çš„è¶…å‚æ•°ï¼Œé™åˆ¶ä¸å¯¹æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆè¿›è¡Œæ’åºã€‚
1. æˆ‘ä»¬å°†é€‰æ‹©èµ·å§‹å’Œç»“æŸlogitsä¸­çš„æœ€ä½³ç´¢å¼•ï¼Œå¹¶æ”¶é›†è¿™äº›é¢„æµ‹çš„æ‰€æœ‰ç­”æ¡ˆã€‚
1. åœ¨æ£€æŸ¥æ¯ä¸€ä¸ªæ˜¯å¦æœ‰æ•ˆåï¼Œæˆ‘ä»¬å°†æŒ‰ç…§å…¶åˆ†æ•°å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œå¹¶ä¿ç•™æœ€ä½³çš„ç­”æ¡ˆã€‚

ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šæ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ï¼š


```python
n_best_size = 20
```


```python
import numpy as np

start_logits = output.start_logits[0].cpu().numpy()
end_logits = output.end_logits[0].cpu().numpy()

# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š
start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()

valid_answers = []

# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ
for start_index in start_indexes:
    for end_index in end_indexes:
        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
            valid_answers.append(
                {
                    "score": start_logits[start_index] + end_logits[end_index],
                    "text": ""  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²
                }
            )

```


ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å®ƒä»¬çš„å¾—åˆ†å¯¹`valid_answers`è¿›è¡Œæ’åºï¼Œå¹¶ä»…ä¿ç•™æœ€ä½³ç­”æ¡ˆã€‚å”¯ä¸€å‰©ä¸‹çš„é—®é¢˜æ˜¯å¦‚ä½•æ£€æŸ¥ç»™å®šçš„è·¨åº¦æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼ˆè€Œä¸æ˜¯é—®é¢˜ä¸­ï¼‰ï¼Œä»¥åŠå¦‚ä½•è·å–å…¶ä¸­çš„æ–‡æœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‘æˆ‘ä»¬çš„éªŒè¯ç‰¹å¾æ·»åŠ ä¸¤ä¸ªå†…å®¹ï¼š

- ç”Ÿæˆè¯¥ç‰¹å¾çš„ç¤ºä¾‹çš„IDï¼ˆå› ä¸ºæ¯ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œå¦‚å‰æ‰€ç¤ºï¼‰ï¼›
- åç§»æ˜ å°„ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›ä»æ ‡è®°ç´¢å¼•åˆ°ä¸Šä¸‹æ–‡ä¸­å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ç¨å¾®ä¸åŒäº`prepare_train_features`æ¥é‡æ–°å¤„ç†éªŒè¯é›†ï¼š


```python
def prepare_validation_features(examples):
    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚
    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½
    examples["question"] = [q.lstrip() for q in examples["question"]]

    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ
    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚
    tokenized_examples = tokenizer(
        examples["question" if pad_on_right else "context"],
        examples["context" if pad_on_right else "question"],
        truncation="only_second" if pad_on_right else "only_first",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚
    tokenized_examples["example_id"] = []

    for i in range(len(tokenized_examples["input_ids"])):
        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚
        sequence_ids = tokenized_examples.sequence_ids(i)
        context_index = 1 if pad_on_right else 0

        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚
        sample_index = sample_mapping[i]
        tokenized_examples["example_id"].append(examples["id"][sample_index])

        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚
        tokenized_examples["offset_mapping"][i] = [
            (o if sequence_ids[k] == context_index else None)
            for k, o in enumerate(tokenized_examples["offset_mapping"][i])
        ]

    return tokenized_examples

```

å°†`prepare_validation_features`åº”ç”¨åˆ°æ•´ä¸ªéªŒè¯é›†ï¼š


```python
validation_features = datasets["validation"].map(
    prepare_validation_features,
    batched=True,
    remove_columns=datasets["validation"].column_names
)
```


    Map:   0%|          | 0/10570 [00:00<?, ? examples/s]


Now we can grab the predictions for all features by using the `Trainer.predict` method:


```python
raw_predictions = trainer.predict(validation_features)
```





`Trainer`ä¼šéšè—æ¨¡å‹ä¸ä½¿ç”¨çš„åˆ—ï¼ˆåœ¨è¿™é‡Œæ˜¯`example_id`å’Œ`offset_mapping`ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬è¿›è¡Œåå¤„ç†ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬é‡æ–°è®¾ç½®å›æ¥ï¼š


```python
validation_features.set_format(type=validation_features.format["type"], columns=list(validation_features.features.keys()))
```

ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹è¿›ä¹‹å‰çš„æµ‹è¯•ï¼š

ç”±äºåœ¨åç§»æ˜ å°„ä¸­ï¼Œå½“å®ƒå¯¹åº”äºé—®é¢˜çš„ä¸€éƒ¨åˆ†æ—¶ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºNoneï¼Œå› æ­¤å¯ä»¥è½»æ¾æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å®Œå…¨åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä»è€ƒè™‘ä¸­æ’é™¤éå¸¸é•¿çš„ç­”æ¡ˆï¼ˆå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ï¼‰ã€‚

å±•å¼€è¯´ä¸‹å…·ä½“å®ç°ï¼š
- é¦–å…ˆä»æ¨¡å‹è¾“å‡ºä¸­è·å–èµ·å§‹å’Œç»“æŸçš„é€»è¾‘å€¼ï¼ˆlogitsï¼‰ï¼Œè¿™äº›å€¼è¡¨æ˜ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­å¯èƒ½å¼€å§‹å’Œç»“æŸçš„ä½ç½®ã€‚
- ç„¶åï¼Œå®ƒä½¿ç”¨åç§»æ˜ å°„ï¼ˆoffset_mappingï¼‰æ¥æ‰¾åˆ°è¿™äº›é€»è¾‘å€¼åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å…·ä½“ä½ç½®ã€‚
- æ¥ä¸‹æ¥ï¼Œä»£ç éå†å¯èƒ½çš„å¼€å§‹å’Œç»“æŸç´¢å¼•ç»„åˆï¼Œæ’é™¤é‚£äº›ä¸åœ¨ä¸Šä¸‹æ–‡èŒƒå›´å†…æˆ–é•¿åº¦ä¸åˆé€‚çš„ç­”æ¡ˆã€‚
- å¯¹äºæœ‰æ•ˆçš„ç­”æ¡ˆï¼Œå®ƒè®¡ç®—å‡ºä¸€ä¸ªåˆ†æ•°ï¼ˆåŸºäºå¼€å§‹å’Œç»“æŸé€»è¾‘å€¼çš„å’Œï¼‰ï¼Œå¹¶å°†ç­”æ¡ˆåŠå…¶åˆ†æ•°å­˜å‚¨èµ·æ¥ã€‚
- æœ€åï¼Œå®ƒæ ¹æ®åˆ†æ•°å¯¹ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œå¹¶è¿”å›å¾—åˆ†æœ€é«˜çš„å‡ ä¸ªç­”æ¡ˆã€‚


```python
max_answer_length = 30
```


```python
start_logits = output.start_logits[0].cpu().numpy()
end_logits = output.end_logits[0].cpu().numpy()
offset_mapping = validation_features[0]["offset_mapping"]

# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•
context = datasets["validation"][0]["context"]

# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š
start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
valid_answers = []
for start_index in start_indexes:
    for end_index in end_indexes:
        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚
        if (
            start_index >= len(offset_mapping)
            or end_index >= len(offset_mapping)
            or offset_mapping[start_index] is None
            or offset_mapping[end_index] is None
        ):
            continue
        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚
        if end_index < start_index or end_index - start_index + 1 > max_answer_length:
            continue
        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­
            start_char = offset_mapping[start_index][0]
            end_char = offset_mapping[end_index][1]
            valid_answers.append(
                {
                    "score": start_logits[start_index] + end_logits[end_index],
                    "text": context[start_char: end_char]
                }
            )

valid_answers = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[:n_best_size]
valid_answers

```




    [{'score': 15.986347, 'text': 'Denver Broncos'},
     {'score': 14.585561,
      'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 13.152991, 'text': 'Carolina Panthers'},
     {'score': 12.38233, 'text': 'Broncos'},
     {'score': 10.981544,
      'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 10.852013,
      'text': 'American Football Conference (AFC) champion Denver Broncos'},
     {'score': 10.635618,
      'text': 'The American Football Conference (AFC) champion Denver Broncos'},
     {'score': 10.283654, 'text': 'Denver'},
     {'score': 9.451225,
      'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 9.234833,
      'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 8.7582445,
      'text': 'Denver Broncos defeated the National Football Conference'},
     {'score': 8.187819,
      'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},
     {'score': 8.134832, 'text': 'Panthers'},
     {'score': 8.092252,
      'text': 'Denver Broncos defeated the National Football Conference (NFC)'},
     {'score': 7.7162285,
      'text': 'the National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 7.595868,
      'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10'},
     {'score': 7.382572,
      'text': 'National Football Conference (NFC) champion Carolina Panthers'},
     {'score': 7.320059,
      'text': 'Denver Broncos defeated the National Football Conference (NFC'},
     {'score': 6.755249, 'text': 'Carolina'},
     {'score': 6.728976, 'text': 'champion Denver Broncos'}]



æ‰“å°æ¯”è¾ƒæ¨¡å‹è¾“å‡ºå’Œæ ‡å‡†ç­”æ¡ˆï¼ˆGround-truthï¼‰æ˜¯å¦ä¸€è‡´:


```python
datasets["validation"][0]["answers"]
```




    {'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],
     'answer_start': [177, 177, 177]}



**æ¨¡å‹æœ€é«˜æ¦‚ç‡çš„è¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´**

æ­£å¦‚ä¸Šé¢çš„ä»£ç æ‰€ç¤ºï¼Œè¿™åœ¨ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šå¾ˆå®¹æ˜“ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å®ƒæ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚

å¯¹äºå…¶ä»–ç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªç¤ºä¾‹ä¸å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„å…³ç³»ã€‚

æ­¤å¤–ï¼Œç”±äºä¸€ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å°†ç”±ç»™å®šç¤ºä¾‹ç”Ÿæˆçš„æ‰€æœ‰ç‰¹å¾ä¸­çš„æ‰€æœ‰ç­”æ¡ˆæ±‡é›†åœ¨ä¸€èµ·ï¼Œç„¶åé€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚

ä¸‹é¢çš„ä»£ç æ„å»ºäº†ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•åˆ°å…¶å¯¹åº”ç‰¹å¾ç´¢å¼•çš„æ˜ å°„å…³ç³»ï¼š


```python
import collections

examples = datasets["validation"]
features = validation_features

example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
features_per_example = collections.defaultdict(list)
for i, feature in enumerate(features):
    features_per_example[example_id_to_index[feature["example_id"]]].append(i)
```

å½“`squad_v2 = True`æ—¶ï¼Œæœ‰ä¸€å®šæ¦‚ç‡å‡ºç°ä¸å¯èƒ½çš„ç­”æ¡ˆï¼ˆimpossible answer)ã€‚

ä¸Šé¢çš„ä»£ç ä»…ä¿ç•™åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬è¿˜éœ€è¦è·å–ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°ï¼ˆå…¶èµ·å§‹å’Œç»“æŸç´¢å¼•å¯¹åº”äºCLSæ ‡è®°çš„ç´¢å¼•ï¼‰ã€‚

å½“ä¸€ä¸ªç¤ºä¾‹ç”Ÿæˆå¤šä¸ªç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ‰€æœ‰ç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆéƒ½é¢„æµ‹å‡ºç°ä¸å¯èƒ½ç­”æ¡ˆæ—¶ï¼ˆå› ä¸ºä¸€ä¸ªç‰¹å¾å¯èƒ½ä¹‹æ‰€ä»¥èƒ½å¤Ÿé¢„æµ‹å‡ºä¸å¯èƒ½ç­”æ¡ˆï¼Œæ˜¯å› ä¸ºç­”æ¡ˆä¸åœ¨å®ƒå¯ä»¥è®¿é—®çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼‰ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸€ä¸ªç¤ºä¾‹ä¸­ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°æ˜¯è¯¥ç¤ºä¾‹ç”Ÿæˆçš„æ¯ä¸ªç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°çš„æœ€å°å€¼ã€‚


```python
from tqdm.auto import tqdm

def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):
    all_start_logits, all_end_logits = raw_predictions
    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚
    example_id_to_index = {k: i for i, k in enumerate(examples["id"])}
    features_per_example = collections.defaultdict(list)
    for i, feature in enumerate(features):
        features_per_example[example_id_to_index[feature["example_id"]]].append(i)

    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚
    predictions = collections.OrderedDict()

    # æ—¥å¿—è®°å½•ã€‚
    print(f"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚")

    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼
    for example_index, example in enumerate(tqdm(examples)):
        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚
        feature_indices = features_per_example[example_index]

        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚
        valid_answers = []
        
        context = example["context"]
        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚
        for feature_index in feature_indices:
            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚
            start_logits = all_start_logits[feature_index]
            end_logits = all_end_logits[feature_index]
            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚
            offset_mapping = features[feature_index]["offset_mapping"]

            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚
            cls_index = features[feature_index]["input_ids"].index(tokenizer.cls_token_id)
            feature_null_score = start_logits[cls_index] + end_logits[cls_index]
            if min_null_score is None or min_null_score < feature_null_score:
                min_null_score = feature_null_score

            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚
            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
            for start_index in start_indexes:
                for end_index in end_indexes:
                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚
                    if (
                        start_index >= len(offset_mapping)
                        or end_index >= len(offset_mapping)
                        or offset_mapping[start_index] is None
                        or offset_mapping[end_index] is None
                    ):
                        continue
                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚
                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:
                        continue

                    start_char = offset_mapping[start_index][0]
                    end_char = offset_mapping[end_index][1]
                    valid_answers.append(
                        {
                            "score": start_logits[start_index] + end_logits[end_index],
                            "text": context[start_char: end_char]
                        }
                    )
        
        if len(valid_answers) > 0:
            best_answer = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[0]
        else:
            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚
            best_answer = {"text": "", "score": 0.0}
        
        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰
        if not squad_v2:
            predictions[example["id"]] = best_answer["text"]
        else:
            answer = best_answer["text"] if best_answer["score"] > min_null_score else ""
            predictions[example["id"]] = answer

    return predictions

```

åœ¨åŸå§‹ç»“æœä¸Šåº”ç”¨åå¤„ç†é—®ç­”ç»“æœï¼š


```python
final_predictions = postprocess_qa_predictions(datasets["validation"], validation_features, raw_predictions.predictions)
```

    æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚



      0%|          | 0/10570 [00:00<?, ?it/s]


ä½¿ç”¨ `datasets.load_metric` ä¸­åŠ è½½ `SQuAD v2` çš„è¯„ä¼°æŒ‡æ ‡


```python
from datasets import load_metric

metric = load_metric("squad_v2" if squad_v2 else "squad")
```

    /tmp/ipykernel_20254/2330875496.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate
      metric = load_metric("squad_v2" if squad_v2 else "squad")
    /root/miniconda3/lib/python3.11/site-packages/datasets/load.py:752: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/squad/squad.py
    You can avoid this message in future by passing the argument `trust_remote_code=True`.
    Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.
      warnings.warn(



    Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]



    Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]


æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°è¿›è¡Œè¯„ä¼°ã€‚

åªéœ€ç¨å¾®è°ƒæ•´ä¸€ä¸‹é¢„æµ‹å’Œæ ‡ç­¾çš„æ ¼å¼ï¼Œå› ä¸ºå®ƒæœŸæœ›çš„æ˜¯ä¸€ç³»åˆ—å­—å…¸è€Œä¸æ˜¯ä¸€ä¸ªå¤§å­—å…¸ã€‚

åœ¨ä½¿ç”¨`squad_v2`æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¾ç½®`no_answer_probability`å‚æ•°ï¼ˆæˆ‘ä»¬åœ¨è¿™é‡Œå°†å…¶è®¾ç½®ä¸º0.0ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬é€‰æ‹©äº†ç­”æ¡ˆï¼Œæˆ‘ä»¬å·²ç»å°†ç­”æ¡ˆè®¾ç½®ä¸ºç©ºï¼‰ã€‚


```python
if squad_v2:
    formatted_predictions = [{"id": k, "prediction_text": v, "no_answer_probability": 0.0} for k, v in final_predictions.items()]
else:
    formatted_predictions = [{"id": k, "prediction_text": v} for k, v in final_predictions.items()]
references = [{"id": ex["id"], "answers": ex["answers"]} for ex in datasets["validation"]]
metric.compute(predictions=formatted_predictions, references=references)
```




    {'exact_match': 74.88174077578051, 'f1': 83.6359321422016}




```python

```

### Homeworkï¼šåŠ è½½æœ¬åœ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¿›è¡Œè¯„ä¼°å’Œå†è®­ç»ƒæ›´é«˜çš„ F1 Score


```python
trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)
```


```python
trained_trainer = Trainer(
    trained_model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)
```

    Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.



```python

```
