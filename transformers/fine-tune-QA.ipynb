{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers å¾®è°ƒè¯­è¨€æ¨¡å‹-é—®ç­”ä»»åŠ¡\n",
    "\n",
    "æˆ‘ä»¬å·²ç»å­¦ä¼šä½¿ç”¨ Pipeline åŠ è½½æ”¯æŒé—®ç­”ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ¬æ•™ç¨‹ä»£ç å°†å±•ç¤ºå¦‚ä½•å¾®è°ƒè®­ç»ƒä¸€ä¸ªæ”¯æŒé—®ç­”ä»»åŠ¡çš„æ¨¡å‹ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼šå¾®è°ƒåçš„æ¨¡å‹ä»ç„¶æ˜¯é€šè¿‡æå–ä¸Šä¸‹æ–‡çš„å­ä¸²æ¥å›ç­”é—®é¢˜çš„ï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚**\n",
    "\n",
    "### æ¨¡å‹æ‰§è¡Œé—®ç­”æ•ˆæœç¤ºä¾‹\n",
    "\n",
    "![Widget inference representing the QA task](docs/images/question_answering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "# æ ¹æ®ä½ ä½¿ç”¨çš„æ¨¡å‹å’ŒGPUèµ„æºæƒ…å†µï¼Œè°ƒæ•´ä»¥ä¸‹å…³é”®å‚æ•°\n",
    "squad_v2 = True\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## ä¸‹è½½æ•°æ®é›†\n",
    "\n",
    "åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuADï¼‰](https://rajpurkar.github.io/SQuAD-explorer/)ã€‚\n",
    "\n",
    "### SQuAD æ•°æ®é›†\n",
    "\n",
    "**æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuAD)** æ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±ä¼—åŒ…å·¥ä½œè€…åœ¨ä¸€ç³»åˆ—ç»´åŸºç™¾ç§‘æ–‡ç« ä¸Šæå‡ºé—®é¢˜ç»„æˆã€‚æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ç›¸åº”é˜…è¯»æ®µè½ä¸­çš„æ–‡æœ¬ç‰‡æ®µæˆ–èŒƒå›´ï¼Œæˆ–è€…è¯¥é—®é¢˜å¯èƒ½æ— æ³•å›ç­”ã€‚\n",
    "\n",
    "SQuAD2.0å°†SQuAD1.1ä¸­çš„10ä¸‡ä¸ªé—®é¢˜ä¸ç”±ä¼—åŒ…å·¥ä½œè€…å¯¹æŠ—æ€§åœ°æ’°å†™çš„5ä¸‡å¤šä¸ªæ— æ³•å›ç­”çš„é—®é¢˜ç›¸ç»“åˆï¼Œä½¿å…¶çœ‹èµ·æ¥ä¸å¯å›ç­”çš„é—®é¢˜ç±»ä¼¼ã€‚è¦åœ¨SQuAD2.0ä¸Šè¡¨ç°è‰¯å¥½ï¼Œç³»ç»Ÿä¸ä»…å¿…é¡»åœ¨å¯èƒ½æ—¶å›ç­”é—®é¢˜ï¼Œè¿˜å¿…é¡»ç¡®å®šæ®µè½ä¸­æ²¡æœ‰æ”¯æŒä»»ä½•ç­”æ¡ˆï¼Œå¹¶æ”¾å¼ƒå›ç­”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
   },
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 130319\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 11873\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¯¹æ¯”æ•°æ®é›†\n",
    "\n",
    "ç›¸æ¯”å¿«é€Ÿå…¥é—¨ä½¿ç”¨çš„ Yelp è¯„è®ºæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° SQuAD è®­ç»ƒå’Œæµ‹è¯•é›†éƒ½æ–°å¢äº†ç”¨äºä¸Šä¸‹æ–‡ã€é—®é¢˜ä»¥åŠé—®é¢˜ç­”æ¡ˆçš„åˆ—ï¼š\n",
    "\n",
    "**YelpReviewFull Datasetï¼š**\n",
    "\n",
    "```json\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 650000\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 50000\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56d4d0c32ccc5a1400d83250',\n",
       " 'title': 'BeyoncÃ©',\n",
       " 'context': 'BeyoncÃ© is believed to have first started a relationship with Jay Z after a collaboration on \"\\'03 Bonnie & Clyde\", which appeared on his seventh album The Blueprint 2: The Gift & The Curse (2002). BeyoncÃ© appeared as Jay Z\\'s girlfriend in the music video for the song, which would further fuel speculation of their relationship. On April 4, 2008, BeyoncÃ© and Jay Z were married without publicity. As of April 2014, the couple have sold a combined 300 million records together. The couple are known for their private relationship, although they have appeared to become more relaxed in recent years. BeyoncÃ© suffered a miscarriage in 2010 or 2011, describing it as \"the saddest thing\" she had ever endured. She returned to the studio and wrote music in order to cope with the loss. In April 2011, BeyoncÃ© and Jay Z traveled to Paris in order to shoot the album cover for her 4, and unexpectedly became pregnant in Paris.',\n",
       " 'question': 'How many records combined have BeyoncÃ© and Jay Z sold?',\n",
       " 'answers': {'text': ['300 million'], 'answer_start': [447]}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä»ä¸Šä¸‹æ–‡ä¸­ç»„ç»‡å›å¤å†…å®¹\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç­”æ¡ˆæ˜¯é€šè¿‡å®ƒä»¬åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®ï¼ˆè¿™é‡Œæ˜¯ç¬¬515ä¸ªå­—ç¬¦ï¼‰ä»¥åŠå®ƒä»¬çš„å®Œæ•´æ–‡æœ¬è¡¨ç¤ºçš„ï¼Œè¿™æ˜¯ä¸Šé¢æåˆ°çš„ä¸Šä¸‹æ–‡çš„å­å­—ç¬¦ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57267a20708984140094c764</td>\n",
       "      <td>Department_store</td>\n",
       "      <td>All major cities have their distinctive local department stores, which anchored the downtown shopping district until the arrival of the malls in the 1960s. Washington, for example, after 1887 had Woodward &amp; Lothrop and Garfinckel's starting in 1905. Garfield's went bankrupt in 1990, as did Woodward &amp; Lothrop in 1994. Baltimore had four major department stores: Hutzler's was the prestige leader, followed by Hecht's, Hochschild's and Stewart's. They all operated branches in the suburbs, but all closed in the late twentieth century. By 2015, most locally owned department stores around the country had been consolidated into larger chains, or had closed down entirely.</td>\n",
       "      <td>In what year did Garfield's go bankrupt?</td>\n",
       "      <td>{'text': ['1990'], 'answer_start': [278]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5ad122d9645df0001a2d0eca</td>\n",
       "      <td>Labour_Party_(UK)</td>\n",
       "      <td>Support for the LRC was boosted by the 1901 Taff Vale Case, a dispute between strikers and a railway company that ended with the union being ordered to pay Â£23,000 damages for a strike. The judgement effectively made strikes illegal since employers could recoup the cost of lost business from the unions. The apparent acquiescence of the Conservative Government of Arthur Balfour to industrial and business interests (traditionally the allies of the Liberal Party in opposition to the Conservative's landed interests) intensified support for the LRC against a government that appeared to have little concern for the industrial proletariat and its problems.</td>\n",
       "      <td>What hurt support for the LRC?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5ace3e6532bba1001ae4a021</td>\n",
       "      <td>Avicenna</td>\n",
       "      <td>According to his autobiography, Avicenna had memorised the entire Quran by the age of 10. He learned Indian arithmetic from an Indian greengrocer,Ø¡Mahmoud Massahi and he began to learn more from a wandering scholar who gained a livelihood by curing the sick and teaching the young. He also studied Fiqh (Islamic jurisprudence) under the Sunni Hanafi scholar Ismail al-Zahid. Avicenna was taught some extent of philosophy books such as Introduction (Isagoge)'s Porphyry (philosopher), Euclid's Elements, Ptolemy's Almagest by an unpopular philosopher, Abu Abdullah Nateli, who claimed philosophizing.</td>\n",
       "      <td>When did Avicenna start studying the Quran?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5725ce2e271a42140099d20d</td>\n",
       "      <td>Hellenistic_period</td>\n",
       "      <td>The Odrysian Kingdom was a union of Thracian tribes under the kings of the powerful Odrysian tribe centered around the region of Thrace. Various parts of Thrace were under Macedonian rule under Philip II of Macedon, Alexander the Great, Lysimachus, Ptolemy II, and Philip V but were also often ruled by their own kings. The Thracians and Agrianes were widely used by Alexander as peltasts and light cavalry, forming about one fifth of his army. The Diadochi also used Thracian mercenaries in their armies and they were also used as colonists. The Odrysians used Greek as the language of administration and of the nobility. The nobility also adopted Greek fashions in dress, ornament and military equipment, spreading it to the other tribes. Thracian kings were among the first to be Hellenized.</td>\n",
       "      <td>Which kings wre among the first to be Hellenized?</td>\n",
       "      <td>{'text': ['Thracian'], 'answer_start': [741]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56d97744dc89441400fdb4cb</td>\n",
       "      <td>2008_Summer_Olympics_torch_relay</td>\n",
       "      <td>A Macau resident was arrested on April 26 for posting a message on cyberctm.com encouraging people to disrupt the relay. Both orchidbbs.com and cyberctm.com Internet forums were shut down from May 2 to 4. This fueled speculation that the shutdowns were targeting speeches against the relay. The head of the Bureau of Telecommunications Regulation has denied that the shutdowns of the websites were politically motivated. About 2,200 police were deployed on the streets, there were no interruptions.</td>\n",
       "      <td>In addition to cyberctm.com, what other website was shut down for two days?</td>\n",
       "      <td>{'text': ['orchidbbs.com'], 'answer_start': [126]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5a68bac68476ee001a58a7cc</td>\n",
       "      <td>Genocide</td>\n",
       "      <td>In 2007 the European Court of Human Rights (ECHR), noted in its judgement on Jorgic v. Germany case that in 1992 the majority of legal scholars took the narrow view that \"intent to destroy\" in the CPPCG meant the intended physical-biological destruction of the protected group and that this was still the majority opinion. But the ECHR also noted that a minority took a broader view and did not consider biological-physical destruction was necessary as the intent to destroy a national, racial, religious or ethnic group was enough to qualify as genocide.</td>\n",
       "      <td>What former case did the European Court of Human Rights draw on in 2002 to further refine qualifiers of genocide?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5a6fd6388abb0b001a675f9b</td>\n",
       "      <td>Alsace</td>\n",
       "      <td>This situation prevailed until 1639, when most of Alsace was conquered by France so as to keep it out of the hands of the Spanish Habsburgs, who wanted a clear road to their valuable and rebellious possessions in the Spanish Netherlands. Beset by enemies and seeking to gain a free hand in Hungary, the Habsburgs sold their Sundgau territory (mostly in Upper Alsace) to France in 1646, which had occupied it, for the sum of 1.2 million Thalers. When hostilities were concluded in 1648 with the Treaty of Westphalia, most of Alsace was recognized as part of France, although some towns remained independent. The treaty stipulations regarding Alsace were complex; although the French king gained sovereignty, existing rights and customs of the inhabitants were largely preserved. France continued to maintain its customs border along the Vosges mountains where it had been, leaving Alsace more economically oriented to neighbouring German-speaking lands. The German language remained in use in local administration, in schools, and at the (Lutheran) University of Strasbourg, which continued to draw students from other German-speaking lands. The 1685 Edict of Fontainebleau, by which the French king ordered the suppression of French Protestantism, was not applied in Alsace. France did endeavour to promote Catholicism; Strasbourg Cathedral, for example, which had been Lutheran from 1524 to 1681, was returned to the Catholic Church. However, compared to the rest of France, Alsace enjoyed a climate of religious tolerance.</td>\n",
       "      <td>When was Strasbourg Cathedral built?</td>\n",
       "      <td>{'text': [], 'answer_start': []}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56dfbd5e231d4119001abd5b</td>\n",
       "      <td>Internet_service_provider</td>\n",
       "      <td>ISPs provide Internet access, employing a range of technologies to connect users to their network. Available technologies have ranged from computer modems with acoustic couplers to telephone lines, to television cable (CATV), wireless Ethernet (wi-fi), and fiber optics.</td>\n",
       "      <td>what was an earlier technology used to connect to the internet?</td>\n",
       "      <td>{'text': ['telephone lines'], 'answer_start': [182]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56d2726659d6e41400145ffa</td>\n",
       "      <td>Buddhism</td>\n",
       "      <td>During the period of Late Mahayana Buddhism, four major types of thought developed: Madhyamaka, Yogacara, Tathagatagarbha, and Buddhist Logic as the last and most recent. In India, the two main philosophical schools of the Mahayana were the Madhyamaka and the later Yogacara. According to Dan Lusthaus, Madhyamaka and Yogacara have a great deal in common, and the commonality stems from early Buddhism. There were no great Indian teachers associated with tathagatagarbha thought.</td>\n",
       "      <td>In India the two main philosophical schools of the Mahayana were Madhyamaka and what else?</td>\n",
       "      <td>{'text': ['Yogacara'], 'answer_start': [96]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56f8f65b9b226e1400dd1202</td>\n",
       "      <td>Near_East</td>\n",
       "      <td>The use of the term Middle East as a region of international affairs apparently began in British and American diplomatic circles quite independently of each other over concern for the security of the same country: Iran, then known to the west as Persia. In 1900 Thomas Edward Gordon published an article, The Problem of the Middle East, which began:</td>\n",
       "      <td>Where did the use of the term Middle East as a region of international affairs begin?</td>\n",
       "      <td>{'text': ['in British and American diplomatic circles'], 'answer_start': [86]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## é¢„å¤„ç†æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰å°±åƒä¸€ä½ã€Œè¯­è¨€æ‹†è§£ä¸“å®¶ã€**ï¼Œä¸“é—¨å¸®è®¡ç®—æœºç†è§£äººç±»æ–‡å­—ã€‚å®ƒçš„æ ¸å¿ƒä½œç”¨å¯ä»¥ç”¨ä¸‰æ­¥è¯´æ¸…æ¥šï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ **æ‹†è§£æ–‡æœ¬**  \n",
    "æŠŠå¥å­æ‹†æˆ **æ¨¡å‹è®¤è¯†çš„ç‰‡æ®µ**ï¼ˆè¯æˆ–å­è¯ï¼‰ã€‚  \n",
    "ä¾‹å¦‚ï¼š  \n",
    "`\"æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†\"` â†’ `[\"æˆ‘\", \"çˆ±\", \"è‡ªç„¶\", \"è¯­è¨€\", \"å¤„ç†\"]`  \n",
    "ï¼ˆè‹±æ–‡å¦‚ `\"Hugging Face\"` â†’ `[\"Hug\", \"##ging\", \"Face\"]`ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **æ·»åŠ ã€Œæš—å·ã€**  \n",
    "æ’å…¥æ¨¡å‹éœ€è¦çš„**ç‰¹æ®Šæ ‡è®°**ï¼Œæ¯”å¦‚ï¼š  \n",
    "- **`[CLS]`**ï¼šå¼€å¤´æ ‡è®°ï¼ˆBERTç”¨ï¼‰  \n",
    "- **`[SEP]`**ï¼šåˆ†éš”æ ‡è®°ï¼ˆåŒºåˆ†å¥å­ï¼‰  \n",
    "```python\n",
    "\"ä½ å¥½å—ï¼Ÿ\" â†’ [\"[CLS]\", \"ä½ \", \"å¥½\", \"å—\", \"ï¼Ÿ\", \"[SEP]\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **è½¬æˆå¯†ç æ•°å­—**  \n",
    "æŠŠæ¯ä¸ªè¯æ¢æˆ**æ¨¡å‹è¯æ±‡è¡¨é‡Œçš„IDå·**ï¼Œç±»ä¼¼å¯†ç æœ¬ï¼š  \n",
    "```python\n",
    "[\"[CLS]\", \"ä½ \", \"å¥½\", \"å—\"] â†’ [101, 872, 1962, 3221, 102]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒ° **å®é™…æ•ˆæœç¤ºä¾‹**  \n",
    "ä½ è¾“å…¥ï¼š`\"ä»Šå¤©å¦é—¨å¤©æ°”å¦‚ä½•ï¼Ÿ\"`  \n",
    "Tokenizerå¤„ç†åè¾“å‡ºï¼š  \n",
    "```python\n",
    "{\n",
    "  \"input_ids\": [101, 791, 1921, 1762, 1377, 1442, 3221, 102],\n",
    "  \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1]  # æ ‡è®°å“ªäº›æ˜¯æœ‰æ•ˆå†…å®¹\n",
    "}\n",
    "```\n",
    "æ¨¡å‹çœ‹åˆ°è¿™äº›æ•°å­—å°±èƒ½åˆ†æè¯­ä¹‰ï¼Œç”Ÿæˆå›ç­”å•¦ï¼\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤– **ä¸åŒæ¨¡å‹çš„å·®å¼‚**  \n",
    "- **BERTç±»**ï¼šæ‹†è¯è¾ƒç»†ï¼ŒåŠ `[CLS]`/`[SEP]`  \n",
    "- **GPTç±»**ï¼šæŒ‰å­—èŠ‚æ‹†åˆ†ï¼ŒåŠ `<|endoftext|>`  \n",
    "- **å¤šè¯­è¨€æ¨¡å‹**ï¼šæ”¯æŒä¸­/è‹±/æ—¥ç­‰æ··åˆæ‹†åˆ†  \n",
    "\n",
    "ä¸€å¥è¯æ€»ç»“ï¼š**Tokenizerå°±æ˜¯æŠŠäººç±»è¯­è¨€ã€Œç¿»è¯‘ã€æˆAIèƒ½æ‡‚çš„æ•°å­—å¯†ç ï¼** ğŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AutoTokenizer å°±åƒã€Œä¸‡èƒ½é€‚é…å™¨ã€**  \n",
    "â€”â€”ä½ åªéœ€è¦å‘Šè¯‰å®ƒç”¨å“ªä¸ªAIæ¨¡å‹ï¼ˆæ¯”å¦‚BERTã€GPT-3ï¼‰ï¼Œå®ƒå°±ä¼šè‡ªåŠ¨åŒ¹é…å¯¹åº”çš„æ–‡å­—ç¿»è¯‘è§„åˆ™ã€‚\n",
    "\n",
    "ä¸¾ä¸ªæ —å­ğŸŒ°ï¼š  \n",
    "- ä½ æƒ³ç”¨ **BERT** æ¨¡å‹ â†’ å®ƒè‡ªåŠ¨åŠ è½½BERTçš„åˆ†è¯è§„åˆ™  \n",
    "  ```python\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "  ```\n",
    "- ä½ æƒ³ç”¨ **GPT** æ¨¡å‹ â†’ å®ƒè‡ªåŠ¨åˆ‡æ¢æˆGPTçš„åˆ†è¯æ–¹å¼  \n",
    "  ```python\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "  ```\n",
    "\n",
    "**å¥½å¤„**ï¼šä¸ç”¨è®°ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨åå­—ï¼ˆæ¯”å¦‚`BertTokenizer`ã€`GPT2Tokenizer`ï¼‰ï¼Œä¸€ä¸ª`AutoTokenizer`é€šåƒæ‰€æœ‰æ¨¡å‹ï¼Œå°±åƒä¸‡èƒ½å……ç”µå™¨ä¸€æ ·æ–¹ä¾¿ï¼\n",
    "\n",
    "---\n",
    "\n",
    "### å¯¹æ¯”ç¤ºä¾‹ï¼ˆæ‰‹åŠ¨ vs è‡ªåŠ¨ï¼‰\n",
    "| æ–¹å¼          | æ‰‹åŠ¨é€‰æ‹©åˆ†è¯å™¨                   | AutoTokenizer                  |\n",
    "|---------------|----------------------------------|---------------------------------|\n",
    "| **BERTæ¨¡å‹**  | `from transformers import BertTokenizer`<br>`tokenizer = BertTokenizer.from_pretrained(\"bert-base\")` | `AutoTokenizer.from_pretrained(\"bert-base\")` |\n",
    "| **GPTæ¨¡å‹**   | `from transformers import GPT2Tokenizer`<br>`tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")` | `AutoTokenizer.from_pretrained(\"gpt2\")` |\n",
    "\n",
    "---\n",
    "\n",
    "âš ï¸ **æ³¨æ„**ï¼šåå­—è¦å¯¹ï¼ˆæ¯”å¦‚`bert-base-chinese`ä¸èƒ½å†™æˆ`bert-chinese`ï¼‰ï¼Œå¦åˆ™è¿™ä¸ªä¸‡èƒ½å……ç”µå™¨ä¹Ÿä¼šæ‰¾ä¸åˆ°æ’å£~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "ä»¥ä¸‹æ–­è¨€ç¡®ä¿æˆ‘ä»¬çš„ Tokenizers ä½¿ç”¨çš„æ˜¯ FastTokenizerï¼ˆRust å®ç°ï¼Œé€Ÿåº¦å’ŒåŠŸèƒ½æ€§ä¸Šæœ‰ä¸€å®šä¼˜åŠ¿ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ˜¯å¦æ˜¯å¿«é€Ÿç‰ˆåˆ†è¯å™¨: True\n",
      "âœ… Tokenizer æ˜¯å¿«é€Ÿç‰ˆ (PreTrainedTokenizerFast)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "# ç›´æ¥æ‰“å°åˆ¤æ–­ç»“æœ\n",
    "print(\"æ˜¯å¦æ˜¯å¿«é€Ÿç‰ˆåˆ†è¯å™¨:\", isinstance(tokenizer, transformers.PreTrainedTokenizerFast))\n",
    "\n",
    "# æˆ–æ›´è¯¦ç»†çš„è¾“å‡º\n",
    "if isinstance(tokenizer, transformers.PreTrainedTokenizerFast):\n",
    "    print(\"âœ… Tokenizer æ˜¯å¿«é€Ÿç‰ˆ (PreTrainedTokenizerFast)\")\n",
    "else:\n",
    "    print(\"âŒ Tokenizer æ˜¯æ™®é€šç‰ˆ (PreTrainedTokenizer)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PreTrainedTokenizer å°±åƒä¸ªã€Œæ–‡å­—ç¿»è¯‘å®˜ã€**ï¼Œä¸“é—¨å¸® AI æ¨¡å‹å’Œäººç±»æ–‡å­—æ‰“äº¤é“ã€‚  \n",
    "\n",
    "ä¸¾ä¸ªæ —å­ğŸŒ°ï¼š  \n",
    "ä½ æƒ³é—® AI \"å¦é—¨ä»Šå¤©çƒ­å—ï¼Ÿ\"  \n",
    "â¡ï¸ **ç¿»è¯‘å®˜çš„å·¥ä½œ**ï¼š  \n",
    "1. æŠŠè¿™å¥è¯åˆ‡æˆå°å—ï¼š`[\"å¦é—¨\", \"ä»Šå¤©\", \"çƒ­\", \"å—\"]`  \n",
    "2. å·å·åŠ æš—å·ï¼š`[å¼€å¤´æš—å·] å¦é—¨ ä»Šå¤© çƒ­ å— [ç»“å°¾æš—å·]`  \n",
    "3. è½¬æˆå¯†ç æ•°å­—ï¼š`[101, 2345, 567, 8910, 102]`  \n",
    "\n",
    "ç„¶å AI å°±èƒ½çœ‹æ‡‚è¿™äº›æ•°å­—å¯†ç ï¼Œç»™å‡ºå›ç­”å•¦ï¼  \n",
    "ï¼ˆåè¿‡æ¥ä¹Ÿä¼šæŠŠ AI çš„æ•°å­—å¯†ç ç¿»è¯‘æˆäººç±»æ–‡å­—ç»™ä½ çœ‹ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)`\n",
    "\n",
    "è¿™ä¸ªæ­¥éª¤æ˜¯**å¯é€‰çš„å®‰å…¨æ£€æŸ¥**ï¼Œä¸»è¦ä¸ºäº†ç¡®ä¿ä½ åŠ è½½çš„æ˜¯**å¿«é€Ÿç‰ˆåˆ†è¯å™¨ï¼ˆPreTrainedTokenizerFastï¼‰**ï¼Œè€Œä¸æ˜¯æ—§ç‰ˆçš„æ…¢é€Ÿåˆ†è¯å™¨ï¼ˆPreTrainedTokenizerï¼‰ã€‚ä¸æ£€æŸ¥ä¹Ÿèƒ½è¿è¡Œï¼Œä½†å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é—®é¢˜ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤” **ä¸ºä»€ä¹ˆè¦åŒºåˆ† Fast å’Œæ™®é€šç‰ˆï¼Ÿ**\n",
    "| ç‰¹æ€§                | PreTrainedTokenizerFastï¼ˆå¿«é€Ÿç‰ˆï¼‰          | PreTrainedTokenizerï¼ˆæ™®é€šç‰ˆï¼‰       |\n",
    "|---------------------|--------------------------------------------|-------------------------------------|\n",
    "| **åº•å±‚å®ç°**         | Rustè¯­è¨€ç¼–å†™ï¼ˆé€Ÿåº¦å¿«ï¼‰                     | Pythonå®ç°ï¼ˆé€Ÿåº¦æ…¢ï¼‰                 |\n",
    "| **æ‰¹å¤„ç†æ”¯æŒ**       | âœ… åŸç”Ÿæ”¯æŒï¼ˆå¦‚`batch_encode_plus`ï¼‰        | âŒ éœ€æ‰‹åŠ¨å¾ªç¯å¤„ç†                    |\n",
    "| **ç‰¹æ®Šæ ‡è®°å¤„ç†**     | è‡ªåŠ¨ç®¡ç†ï¼ˆå¦‚å¡«å……ã€æˆªæ–­ï¼‰                   | éœ€æ‰‹åŠ¨é…ç½®                          |\n",
    "| **å…¸å‹åœºæ™¯**         | ç”Ÿäº§ç¯å¢ƒã€å¤§æ•°æ®å¤„ç†                        | æ•™å­¦æˆ–å…¼å®¹æ—§ä»£ç                      |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¥ **ä¸æ£€æŸ¥å¯èƒ½å¸¦æ¥çš„é—®é¢˜**\n",
    "1. **æ€§èƒ½ä¸‹é™**ï¼šå¤„ç†1000æ¡æ–‡æœ¬æ—¶ï¼Œå¿«é€Ÿç‰ˆå¯èƒ½æ¯”æ™®é€šç‰ˆå¿«**5-10å€**ã€‚\n",
    "2. **åŠŸèƒ½ç¼ºå¤±**ï¼šæ™®é€šç‰ˆå¯èƒ½ç¼ºå°‘æŸäº›APIï¼ˆå¦‚`decode`çš„`skip_special_tokens`å‚æ•°ï¼‰ã€‚\n",
    "3. **æ„å¤–é”™è¯¯**ï¼šæŸäº›åº“ï¼ˆå¦‚Datasetsï¼‰é»˜è®¤è¦æ±‚å¿«é€Ÿç‰ˆåˆ†è¯å™¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒ° **å®é™…æ¡ˆä¾‹**\n",
    "å‡è®¾ä½ çš„`model_checkpoint`æ„å¤–æŒ‡å‘äº†ä¸€ä¸ªæ²¡æœ‰å¿«é€Ÿç‰ˆçš„æ¨¡å‹ï¼š\n",
    "```python\n",
    "model_checkpoint = \"some-old-model\"  # å‡è®¾è¯¥æ¨¡å‹åªæœ‰æ™®é€šç‰ˆåˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# æ­¤æ—¶ tokenizer æ˜¯ PreTrainedTokenizer è€Œé Fast ç‰ˆ\n",
    "# åç»­è°ƒç”¨ batch_encode_plus å¯èƒ½æŠ¥é”™ï¼\n",
    "```\n",
    "\n",
    "é€šè¿‡`assert`æ£€æŸ¥ï¼Œå¯ä»¥**æå‰å‘ç°é—®é¢˜**ï¼Œé¿å…åç»­ä»£ç å´©æºƒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚æœä¸åšæ–­è¨€ï¼‰**\n",
    "1. **ç›´æ¥ä½¿ç”¨**ï¼šå¦‚æœç¡®å®šæ¨¡å‹æœ‰å¿«é€Ÿç‰ˆï¼Œå¯ä»¥è·³è¿‡æ£€æŸ¥ã€‚\n",
    "2. **é™çº§å¤„ç†**ï¼šæ•è·å¼‚å¸¸å¹¶æ”¹ç”¨æ™®é€šç‰ˆé€»è¾‘ï¼š\n",
    "```python\n",
    "if not isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "    print(\"è­¦å‘Šï¼šä½¿ç”¨æ…¢é€Ÿåˆ†è¯å™¨ï¼Œæ€§èƒ½å¯èƒ½å—å½±å“ï¼\")\n",
    "    # æ‰‹åŠ¨å¤„ç†æ™®é€šç‰ˆçš„é™åˆ¶\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "æ€»ç»“ï¼šè¿™ä¸ªæ–­è¨€æ˜¯**é˜²å¾¡æ€§ç¼–ç¨‹**çš„ä½“ç°ï¼Œç¡®ä¿ä»£ç åœ¨æ€§èƒ½å’ŒåŠŸèƒ½ä¸ŠæŒ‰é¢„æœŸè¿è¡Œã€‚å¯¹äºå…³é”®é¡¹ç›®å»ºè®®ä¿ç•™ï¼Œä¸ªäººå®éªŒå¯è·³è¿‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨å¯ä»¥åœ¨å¤§æ¨¡å‹è¡¨ä¸ŠæŸ¥çœ‹å“ªç§ç±»å‹çš„æ¨¡å‹å…·æœ‰å¯ç”¨çš„å¿«é€Ÿæ ‡è®°å™¨ï¼Œå“ªç§ç±»å‹æ²¡æœ‰ã€‚\n",
    "\n",
    "æ‚¨å¯ä»¥ç›´æ¥åœ¨ä¸¤ä¸ªå¥å­ä¸Šè°ƒç”¨æ­¤æ ‡è®°å™¨ï¼ˆä¸€ä¸ªç”¨äºç­”æ¡ˆï¼Œä¸€ä¸ªç”¨äºä¸Šä¸‹æ–‡ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2129, 2024, 2017, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenizer(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer è¿›é˜¶æ“ä½œ\n",
    "\n",
    "åœ¨é—®ç­”é¢„å¤„ç†ä¸­çš„ä¸€ä¸ªç‰¹å®šé—®é¢˜æ˜¯å¦‚ä½•å¤„ç†éå¸¸é•¿çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "åœ¨å…¶ä»–ä»»åŠ¡ä¸­ï¼Œå½“æ–‡æ¡£çš„é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€å¤§å¥å­é•¿åº¦æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæˆªæ–­å®ƒä»¬ï¼Œä½†åœ¨è¿™é‡Œï¼Œåˆ é™¤ä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†å¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬ä¸¢å¤±æ­£åœ¨å¯»æ‰¾çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å…è®¸æ•°æ®é›†ä¸­çš„ä¸€ä¸ªï¼ˆé•¿ï¼‰ç¤ºä¾‹ç”Ÿæˆå¤šä¸ªè¾“å…¥ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„é•¿åº¦éƒ½å°äºæ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆæˆ–æˆ‘ä»¬è®¾ç½®çš„è¶…å‚æ•°ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **ä¸ºä½•è®¾ç½® `max_length=384`ï¼Ÿ**\n",
    "1. **æ¨¡å‹é™åˆ¶**  \n",
    "   BERTç­‰æ¨¡å‹æœ€å¤§æ”¯æŒ **512 tokens**ï¼Œéœ€ä¸ºä»¥ä¸‹å†…å®¹ç•™ç©ºé—´ï¼š  \n",
    "   - **é—®é¢˜æœ¬èº«**ï¼ˆçº¦20-30 tokensï¼‰  \n",
    "   - **ç‰¹æ®Šæ ‡è®°**ï¼ˆå¦‚ `[CLS]`ã€`[SEP]`ï¼Œå 3-5 tokensï¼‰  \n",
    "   - **ç­”æ¡ˆä½ç½®**ï¼ˆé¿å…è¢«æˆªæ–­ï¼‰\n",
    "\n",
    "2. **ç»éªŒæ¯”ä¾‹**  \n",
    "   å¯ç”¨ä¸Šä¸‹æ–‡é•¿åº¦ â‰ˆ æ€»é•¿çš„ **75%**ï¼ˆ512Ã—0.75â‰ˆ384ï¼‰ï¼Œå¹³è¡¡è¦†ç›–ç‡å’Œè®¡ç®—æ•ˆç‡ã€‚\n",
    "\n",
    "3. **åˆ†å—ä¼˜åŒ–**  \n",
    "   ç»“åˆ `doc_stride=128`ï¼ˆé‡å é‡ï¼‰ï¼Œç¡®ä¿ç­”æ¡ˆåœ¨è‡³å°‘ä¸€ä¸ªåˆ†å—ä¸­å®Œæ•´å‡ºç°ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **å®é™…æ¡ˆä¾‹**  \n",
    "- **è¾“å…¥**ï¼šé—®é¢˜ï¼ˆ20 tokensï¼‰+ ä¸Šä¸‹æ–‡ï¼ˆ500 tokensï¼‰  \n",
    "- **å¤„ç†**ï¼š  \n",
    "  1. åˆ†å—1ï¼šé—®é¢˜ + ä¸Šä¸‹æ–‡0-363  \n",
    "  2. åˆ†å—2ï¼šé—®é¢˜ + ä¸Šä¸‹æ–‡236-500ï¼ˆä¸åˆ†å—1é‡å 128 tokensï¼‰  \n",
    "- **ç»“æœ**ï¼šå³ä½¿ç­”æ¡ˆåœ¨360-400åŒºé—´ï¼Œä¹Ÿèƒ½è¢«åˆ†å—2è¦†ç›–ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **è°ƒæ•´å»ºè®®**\n",
    "- **çŸ­æ–‡æœ¬ä»»åŠ¡**ï¼šç›´æ¥è®¾ä¸º512  \n",
    "- **è¶…é•¿æ–‡æ¡£**ï¼šå¯é™ä½åˆ°256ï¼ˆéœ€æ›´å¤šåˆ†å—ï¼‰  \n",
    "- **æ”¯æŒæ›´é•¿æ¨¡å‹**ï¼šå¦‚æ”¯æŒ1024ï¼Œå¯è®¾ä¸º768  \n",
    "\n",
    "ä¸€å¥è¯æ€»ç»“ï¼š**384æ˜¯å¹³è¡¡æ¨¡å‹é™åˆ¶ã€ç­”æ¡ˆå®Œæ•´æ€§å’Œè®¡ç®—æ•ˆç‡çš„ç»éªŒå€¼ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_stride=128` çš„åŸç†ä¸ `max_length=384` ç±»ä¼¼ï¼Œä½†å…³æ³¨ç‚¹ä¸åŒã€‚ä»¥ä¸‹æ˜¯ç®€æ´æ¸…æ™°çš„è§£é‡Šï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### **ä¸ºä½•è®¾ç½® `doc_stride=128`ï¼Ÿ**\n",
    "1. **æ ¸å¿ƒç›®çš„**  \n",
    "   **é¿å…ç­”æ¡ˆè¢«åˆ‡å‰²åœ¨åˆ†å—è¾¹ç•Œ**ã€‚é€šè¿‡è®¾ç½®åˆ†å—é—´çš„é‡å åŒºåŸŸï¼Œç¡®ä¿å³ä½¿ç­”æ¡ˆä½äºåˆ†å—è¾¹ç¼˜ï¼Œä¹Ÿèƒ½è¢«è‡³å°‘ä¸€ä¸ªå®Œæ•´åˆ†å—è¦†ç›–ã€‚\n",
    "\n",
    "2. **ç»éªŒå…¬å¼**  \n",
    "   `doc_stride` â‰ˆ `max_length` çš„ **1/3~1/4**ï¼ˆå¦‚ `384/3â‰ˆ128`ï¼‰ï¼Œå¹³è¡¡ï¼š\n",
    "   - **è®¡ç®—æ•ˆç‡**ï¼ˆåˆ†å—è¶Šå°‘è¶Šå¥½ï¼‰\n",
    "   - **ç­”æ¡ˆè¦†ç›–ç‡**ï¼ˆé‡å è¶Šå¤šè¶Šå®‰å…¨ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **åˆ†å—é€»è¾‘ç¤ºä¾‹**\n",
    "- **å‚æ•°**ï¼š\n",
    "  - `max_length=384`ï¼ˆæ€»é•¿åº¦ï¼‰\n",
    "  - é—®é¢˜é•¿åº¦ = 20 tokens\n",
    "  - å¯ç”¨ä¸Šä¸‹æ–‡é•¿åº¦ = `384 - 20 - 3ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰â‰ˆ 361 tokens`\n",
    "  - `doc_stride=128`\n",
    "- **åˆ†å—æ­¥é•¿** = `361 - 128 = 233 tokens`\n",
    "\n",
    "| åˆ†å— | èµ·å§‹ä½ç½® | ç»“æŸä½ç½® | è¦†ç›–çš„ä¸Šä¸‹æ–‡èŒƒå›´ |\n",
    "|------|----------|----------|------------------|\n",
    "| 1    | 0        | 360      | tokens 0-360     |\n",
    "| 2    | 233      | 593      | tokens 233-593   |\n",
    "| 3    | 466      | 826      | tokens 466-826   |\n",
    "\n",
    "- **å‡è®¾ç­”æ¡ˆåœ¨ tokens 350-370**ï¼š\n",
    "  - åˆ†å—1ï¼šè¦†ç›–åˆ°360 â†’ ç­”æ¡ˆéƒ¨åˆ†æˆªæ–­ï¼ˆ350-360ä¿ç•™ï¼‰\n",
    "  - åˆ†å—2ï¼šä»233å¼€å§‹ â†’ å®Œæ•´è¦†ç›–ç­”æ¡ˆï¼ˆ350-370ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **å…³é”®å½±å“**\n",
    "| `doc_stride` å€¼ | ä¼˜ç‚¹               | ç¼ºç‚¹                 |\n",
    "|-----------------|--------------------|----------------------|\n",
    "| **è¾ƒå°ï¼ˆå¦‚64ï¼‰** | ç­”æ¡ˆè¦†ç›–ç‡â†‘        | åˆ†å—æ•°é‡â†‘ï¼Œè®¡ç®—é‡â†‘   |\n",
    "| **è¾ƒå¤§ï¼ˆå¦‚192ï¼‰**| åˆ†å—æ•°é‡â†“ï¼Œé€Ÿåº¦â†‘   | æ¼ç­”é£é™©â†‘            |\n",
    "\n",
    "---\n",
    "\n",
    "### **è°ƒæ•´å»ºè®®**\n",
    "- **çŸ­ç­”æ¡ˆä»»åŠ¡**ï¼ˆå¦‚å®ä½“æŠ½å–ï¼‰ï¼š`doc_stride=64~128`\n",
    "- **é•¿ç­”æ¡ˆä»»åŠ¡**ï¼ˆå¦‚æ®µè½æ€»ç»“ï¼‰ï¼š`doc_stride=128~256`\n",
    "\n",
    "ä¸€å¥è¯æ€»ç»“ï¼š**`doc_stride=128` æ˜¯ç»éªŒæ€§å‚æ•°ï¼Œé€šè¿‡åˆ†å—é‡å å¹³è¡¡æ•ˆç‡ä¸ç­”æ¡ˆå®Œæ•´æ€§ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹å‚æ•°ï¼š\n",
    "- **`max_length = 10`**ï¼ˆæ¯ä¸ªç‰‡æ®µæœ€å¤šåŒ…å«10ä¸ªå­—ç¬¦ï¼‰\n",
    "- **`doc_stride = 4`**ï¼ˆç›¸é‚»ç‰‡æ®µé‡å 4ä¸ªå­—ç¬¦ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **åˆ‡å‰²è¿‡ç¨‹**\n",
    "åŸå§‹æ–‡æœ¬ï¼š`ABCDEFGHIJKLMN`ï¼ˆå‡è®¾æ¯ä¸ªå­—æ¯ä»£è¡¨ä¸€ä¸ªtokenï¼‰\n",
    "\n",
    "1. **ç¬¬ä¸€ä¸ªç‰‡æ®µ**ï¼š  \n",
    "   - å–å‰10ä¸ªå­—ç¬¦ â†’ `ABCDEFGHIJ`ï¼ˆAåˆ°Jï¼‰\n",
    "   - ç»“æŸä½ç½®ï¼šç¬¬10ä¸ªå­—ç¬¦ï¼ˆJï¼‰\n",
    "\n",
    "2. **ç¬¬äºŒä¸ªç‰‡æ®µ**ï¼š  \n",
    "   - èµ·å§‹ä½ç½® = å‰ç‰‡æ®µçš„èµ·å§‹ä½ç½® + (`max_length - doc_stride`) = 0 + (10 - 4) = 6  \n",
    "     ï¼ˆå³ä»ç¬¬7ä¸ªå­—ç¬¦å¼€å§‹ï¼Œå¯¹åº”å­—æ¯ `G`ï¼‰\n",
    "   - å®é™…å­—ç¬¦ï¼š`GHIJKLMN`ï¼ˆGåˆ°Nï¼Œå…±8ä¸ªå­—ç¬¦ï¼Œä¸è¶³10ä¸ªåˆ™ä¿ç•™ï¼‰\n",
    "   - é‡å éƒ¨åˆ†ï¼š`GHIJ`ï¼ˆä¸å‰ä¸€ç‰‡æ®µçš„å4ä¸ªå­—ç¬¦é‡å ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **å›¾ç¤ºåˆ‡å‰²æ•ˆæœ**\n",
    "```\n",
    "åŸå§‹æ–‡æœ¬ï¼š A B C D E F G H I J K L M N\n",
    "ç‰‡æ®µ1ï¼š    [A B C D E F G H I J]          â†’ é•¿åº¦10\n",
    "ç‰‡æ®µ2ï¼š            [G H I J K L M N]      â†’ èµ·å§‹ä½ç½®6ï¼Œé‡å 4ä¸ªå­—ç¬¦\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ä¸ºä»€ä¹ˆéœ€è¦é‡å ï¼Ÿ**\n",
    "å‡è®¾ç­”æ¡ˆåœ¨ `H I J K` åŒºåŸŸï¼š\n",
    "- **æ— é‡å **ï¼šå¯èƒ½è¢«æˆªæ–­åœ¨ç‰‡æ®µ1æœ«å°¾æˆ–ç‰‡æ®µ2å¼€å¤´\n",
    "- **æœ‰é‡å **ï¼šç¡®ä¿ç­”æ¡ˆå®Œæ•´åŒ…å«åœ¨è‡³å°‘ä¸€ä¸ªç‰‡æ®µä¸­\n",
    "\n",
    "---\n",
    "\n",
    "### **å®é™…é—®ç­”ä¸­çš„å‚æ•°**\n",
    "å½“ `max_length=384` ä¸” `doc_stride=128` æ—¶ï¼Œé€»è¾‘å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯æ•°å€¼æ›´å¤§ã€‚è¿™ç§æ»‘åŠ¨çª—å£åˆ‡å‰²æ˜¯å¤„ç†é•¿æ–‡æœ¬é—®ç­”çš„å¸¸ç”¨ç­–ç•¥ï¼ ğŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è¶…å‡ºæœ€å¤§é•¿åº¦çš„æ–‡æœ¬æ•°æ®å¤„ç†\n",
    "\n",
    "ä¸‹é¢ï¼Œæˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­æ‰¾å‡ºä¸€ä¸ªè¶…è¿‡æœ€å¤§é•¿åº¦ï¼ˆ384ï¼‰çš„æ–‡æœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "# æŒ‘é€‰å‡ºæ¥è¶…è¿‡384ï¼ˆæœ€å¤§é•¿åº¦ï¼‰çš„æ•°æ®æ ·ä¾‹\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "437"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æˆªæ–­ä¸Šä¸‹æ–‡ä¸ä¿ç•™è¶…å‡ºéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"],\n",
    "              example[\"context\"],\n",
    "              max_length=max_length,\n",
    "              truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…³äºæˆªæ–­çš„ç­–ç•¥\n",
    "\n",
    "- ç›´æ¥æˆªæ–­è¶…å‡ºéƒ¨åˆ†: truncation=`only_second`\n",
    "- ä»…æˆªæ–­ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ï¼Œä¿ç•™é—®é¢˜ï¼ˆquestionï¼‰ï¼š`return_overflowing_tokens=True` & è®¾ç½®`stride`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨æ­¤ç­–ç•¥æˆªæ–­åï¼ŒTokenizer å°†è¿”å›å¤šä¸ª `input_ids` åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 192]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è§£ç ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ï¼Œå¯ä»¥çœ‹åˆ°é‡å çš„éƒ¨åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] beyonce got married in 2008 to whom? [SEP] on april 4, 2008, beyonce married jay z. she publicly revealed their marriage in a video montage at the listening party for her third studio album, i am... sasha fierce, in manhattan's sony club on october 22, 2008. i am... sasha fierce was released on november 18, 2008 in the united states. the album formally introduces beyonce's alter ego sasha fierce, conceived during the making of her 2003 single \" crazy in love \", selling 482, 000 copies in its first week, debuting atop the billboard 200, and giving beyonce her third consecutive number - one album in the us. the album featured the number - one song \" single ladies ( put a ring on it ) \" and the top - five songs \" if i were a boy \" and \" halo \". achieving the accomplishment of becoming her longest - running hot 100 single in her career, \" halo \"'s success in the us helped beyonce attain more top - ten singles on the list than any other woman during the 2000s. it also included the successful \" sweet dreams \", and singles \" diva \", \" ego \", \" broken - hearted girl \" and \" video phone \". the music video for \" single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce [SEP]\n",
      "[CLS] beyonce got married in 2008 to whom? [SEP] single ladies \" has been parodied and imitated around the world, spawning the \" first major dance craze \" of the internet age according to the toronto star. the video has won several awards, including best video at the 2009 mtv europe music awards, the 2009 scottish mobo awards, and the 2009 bet awards. at the 2009 mtv video music awards, the video was nominated for nine awards, ultimately winning three including video of the year. its failure to win the best female video category, which went to american country pop singer taylor swift's \" you belong with me \", led to kanye west interrupting the ceremony and beyonce improvising a re - presentation of swift's award during her own acceptance speech. in march 2009, beyonce embarked on the i am... world tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $ 119. 5 million. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨ offsets_mapping è·å–åŸå§‹çš„ input_ids\n",
    "\n",
    "è®¾ç½® `return_offsets_mapping=True`ï¼Œå°†ä½¿å¾—æˆªæ–­åˆ†å‰²ç”Ÿæˆçš„å¤šä¸ª input_ids åˆ—è¡¨ä¸­çš„ tokenï¼Œé€šè¿‡æ˜ å°„ä¿ç•™åŸå§‹æ–‡æœ¬çš„ input_idsã€‚\n",
    "\n",
    "å¦‚ä¸‹æ‰€ç¤ºï¼šç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆ[CLS]ï¼‰çš„èµ·å§‹å’Œç»“æŸå­—ç¬¦éƒ½æ˜¯ï¼ˆ0, 0ï¼‰ï¼Œå› ä¸ºå®ƒä¸å¯¹åº”é—®é¢˜/ç­”æ¡ˆçš„ä»»ä½•éƒ¨åˆ†ï¼Œç„¶åç¬¬äºŒä¸ªæ ‡è®°ä¸é—®é¢˜(question)çš„å­—ç¬¦0åˆ°3ç›¸åŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 7), (8, 11), (12, 19), (20, 22), (23, 27), (28, 30), (31, 35), (35, 36), (0, 0), (0, 2), (3, 8), (9, 10), (10, 11), (12, 16), (16, 17), (18, 25), (26, 33), (34, 37), (38, 39), (39, 40), (41, 44), (45, 53), (54, 62), (63, 68), (69, 77), (78, 80), (81, 82), (83, 88), (89, 93), (93, 96), (97, 99), (100, 103), (104, 113), (114, 119), (120, 123), (124, 127), (128, 133), (134, 140), (141, 146), (146, 147), (148, 149), (150, 152), (152, 153), (153, 154), (154, 155), (156, 161), (162, 168), (168, 169), (170, 172), (173, 182), (182, 183), (183, 184), (185, 189), (190, 194), (195, 197), (198, 205), (206, 208), (208, 209), (210, 214), (214, 215), (216, 217), (218, 220), (220, 221), (221, 222), (222, 223), (224, 229), (230, 236), (237, 240), (241, 249), (250, 252), (253, 261), (262, 264), (264, 265), (266, 270), (271, 273), (274, 277), (278, 284), (285, 291), (291, 292), (293, 296), (297, 302), (303, 311), (312, 322), (323, 330), (330, 331), (331, 332), (333, 338), (339, 342), (343, 348), (349, 355), (355, 356), (357, 366), (367, 373), (374, 377), (378, 384), (385, 387), (388, 391), (392, 396), (397, 403)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],            # ç¬¬ä¸€ä¸ªå‚æ•°ï¼šé—®é¢˜æ–‡æœ¬\n",
    "    example[\"context\"],             # ç¬¬äºŒä¸ªå‚æ•°ï¼šä¸Šä¸‹æ–‡æ–‡æœ¬\n",
    "    max_length=max_length,          # æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚384ï¼‰\n",
    "    truncation=\"only_second\",       # å…³é”®å‚æ•°1ï¼šæˆªæ–­ç­–ç•¥\n",
    "    return_overflowing_tokens=True, # å…³é”®å‚æ•°2ï¼šè¿”å›åˆ†å—ç»“æœ\n",
    "    return_offsets_mapping=True,    # å…³é”®å‚æ•°3ï¼šè¿”å›å­—ç¬¦çº§ä½ç½®æ˜ å°„\n",
    "    return_token_type_ids=True,     # æ˜¾å¼è¦æ±‚è¿”å› token_type_ids\n",
    "    stride=doc_stride               # åˆ†å—æ»‘åŠ¨æ­¥é•¿ï¼ˆå¦‚128ï¼‰\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **å‚æ•°è¯¦è§£**\n",
    "#### 1. `truncation=\"only_second\"`\n",
    "- **ä½œç”¨**ï¼š**åªæˆªæ–­ç¬¬äºŒä¸ªå‚æ•°ï¼ˆä¸Šä¸‹æ–‡ï¼‰**ï¼Œä¿æŒç¬¬ä¸€ä¸ªå‚æ•°ï¼ˆé—®é¢˜ï¼‰å®Œæ•´\n",
    "- **åœºæ™¯**ï¼šå½“ `é—®é¢˜+ä¸Šä¸‹æ–‡` æ€»é•¿åº¦è¶…è¿‡ `max_length` æ—¶ï¼Œä¼˜å…ˆä¿ç•™é—®é¢˜å®Œæ•´æ€§\n",
    "- **ç¤ºä¾‹**ï¼š\n",
    "  ```python\n",
    "  # è¾“å…¥ï¼šé—®é¢˜é•¿åº¦20ï¼Œä¸Šä¸‹æ–‡é•¿åº¦400 â†’ æ€»é•¿åº¦420 > 384\n",
    "  # å¤„ç†ï¼šæˆªæ–­ä¸Šä¸‹æ–‡ä¸º 384-20-3ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰= 361 tokens\n",
    "  ```\n",
    "\n",
    "#### 2. `return_overflowing_tokens=True`\n",
    "- **ä½œç”¨**ï¼š**è¿”å›åˆ†å—åçš„å¤šä¸ªè¾“å…¥ç‰¹å¾**ï¼ˆå½“è¾“å…¥è¿‡é•¿æ—¶è‡ªåŠ¨åˆ†å‰²ï¼‰\n",
    "- **è¾“å‡ºå­—æ®µ**ï¼š`overflow_to_sample_mapping`ï¼ˆåˆ†å—å¯¹åº”åŸå§‹æ ·æœ¬çš„ç´¢å¼•ï¼‰\n",
    "- **åˆ†å—é€»è¾‘**ï¼š\n",
    "  - å°†é•¿ä¸Šä¸‹æ–‡æŒ‰ `max_length - é—®é¢˜é•¿åº¦` åˆ‡å‰²\n",
    "  - ç›¸é‚»åˆ†å—é‡å  `stride` tokensï¼ˆç¡®ä¿ç­”æ¡ˆä¸è¢«åˆ‡å‰²ï¼‰\n",
    "\n",
    "#### 3. `return_offsets_mapping=True`\n",
    "- **ä½œç”¨**ï¼š**è¿”å›æ¯ä¸ª token åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®**ï¼ˆèµ·å§‹å’Œç»“æŸç´¢å¼•ï¼‰\n",
    "- **è¾“å‡ºå­—æ®µ**ï¼š`offset_mapping`ï¼ˆåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ `(start, end)` å…ƒç»„ï¼‰\n",
    "- **å…³é”®ç”¨é€”**ï¼šå°†æ¨¡å‹é¢„æµ‹çš„ token ä½ç½®æ˜ å°„å›åŸå§‹æ–‡æœ¬ï¼ˆå¦‚å®šä½ç­”æ¡ˆï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **`offset_mapping` ç¤ºä¾‹è§£æ**\n",
    "```python\n",
    "# å‡è®¾æ‰“å°ç»“æœå‰5ä¸ªå…ƒç´ ï¼š\n",
    "[(0, 0), (0, 3), (4, 7), (8, 11), (12, 15), ...]\n",
    "\n",
    "# å¯¹åº”å«ä¹‰ï¼š\n",
    "# [CLS]  What    is    your   name?  [SEP] ...\n",
    "# (0,0) (0,3) (4,7) (8,11) (12,15)   ...\n",
    "```\n",
    "- **ç‰¹æ®Šæ ‡è®°**ï¼š`[CLS]`ã€`[SEP]` ç­‰æ— å¯¹åº”æ–‡æœ¬ â†’ `(0, 0)`\n",
    "- **é—®é¢˜éƒ¨åˆ†**ï¼šå­—ç¬¦ç´¢å¼•ä»é—®é¢˜æ–‡æœ¬çš„èµ·å§‹ä½ç½®è®¡ç®—\n",
    "- **ä¸Šä¸‹æ–‡éƒ¨åˆ†**ï¼šå­—ç¬¦ç´¢å¼•ä»ä¸Šä¸‹æ–‡æ–‡æœ¬çš„èµ·å§‹ä½ç½®è®¡ç®—ï¼ˆéœ€æ³¨æ„é—®é¢˜æ–‡æœ¬é•¿åº¦ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **å‚æ•°ååŒä½œç”¨**\n",
    "| å‚æ•°ç»„åˆ                        | å®é™…æ•ˆæœ                                                                 |\n",
    "|---------------------------------|------------------------------------------------------------------------|\n",
    "| `truncation=\"only_second\"` + `return_overflowing_tokens=True` | å°†é•¿ä¸Šä¸‹æ–‡åˆ‡å‰²ä¸ºå¤šä¸ªåˆ†å—ï¼Œæ¯ä¸ªåˆ†å—åŒ…å«å®Œæ•´é—®é¢˜å’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡ |\n",
    "| `return_offsets_mapping=True`   | æä¾›åˆ†å—ä¸­æ¯ä¸ª token åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„ä½ç½®ï¼Œç”¨äºç­”æ¡ˆä½ç½®æ˜ å°„               |\n",
    "\n",
    "---\n",
    "\n",
    "### **åº”ç”¨åœºæ™¯**\n",
    "1. **è®­ç»ƒé˜¶æ®µ**ï¼šå°†ç­”æ¡ˆçš„å­—ç¬¦ä½ç½®è½¬æ¢ä¸ºåˆ†å—å†…çš„ token ä½ç½®\n",
    "2. **æ¨ç†é˜¶æ®µ**ï¼šå°†æ¨¡å‹é¢„æµ‹çš„ token ä½ç½®åå‘æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡\n",
    "3. **æ•°æ®éªŒè¯**ï¼šæ£€æŸ¥åˆ†å—æ˜¯å¦è¦†ç›–æ­£ç¡®ç­”æ¡ˆçš„åŸå§‹ä½ç½®\n",
    "\n",
    "---\n",
    "\n",
    "é€šè¿‡è¿™ä¸‰ä¸ªå‚æ•°ï¼Œå®ç°äº†é•¿æ–‡æœ¬é—®ç­”ä»»åŠ¡ä¸­ **è¾“å…¥åˆ†å—å¤„ç†** å’Œ **ä½ç½®ç²¾ç¡®æ˜ å°„** çš„æ ¸å¿ƒéœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ˜ å°„æ¥æ‰¾åˆ°ç­”æ¡ˆåœ¨ç»™å®šç‰¹å¾ä¸­çš„èµ·å§‹å’Œç»“æŸæ ‡è®°çš„ä½ç½®ã€‚\n",
    "\n",
    "æˆ‘ä»¬åªéœ€åŒºåˆ†åç§»çš„å“ªäº›éƒ¨åˆ†å¯¹åº”äºé—®é¢˜ï¼Œå“ªäº›éƒ¨åˆ†å¯¹åº”äºä¸Šä¸‹æ–‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20773\n",
      "(0, 7)\n",
      "beyonce Beyonce\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(first_token_id)\n",
    "print(offsets)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got got\n"
     ]
    }
   ],
   "source": [
    "second_token_id = tokenized_example[\"input_ids\"][0][2]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][2]\n",
    "print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3183\n",
      "(31, 35)\n",
      "whom whom\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][7]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][7]\n",
    "print(first_token_id)\n",
    "print(offsets)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "(0, 2)\n",
      "on On\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][10]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][10]\n",
    "print(first_token_id)\n",
    "print(offsets)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"context\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== åˆ†å— 0 ===\n",
      "0 0\n",
      "Token 0: [CLS] â†’ \n",
      "0 7\n",
      "Token 1: beyonce â†’ Beyonce\n",
      "8 11\n",
      "Token 2: got â†’ got\n",
      "12 19\n",
      "Token 3: married â†’ married\n",
      "20 22\n",
      "Token 4: in â†’ in\n",
      "23 27\n",
      "Token 5: 2008 â†’ 2008\n",
      "28 30\n",
      "Token 6: to â†’ to\n",
      "31 35\n",
      "Token 7: whom â†’ whom\n",
      "35 36\n",
      "Token 8: ? â†’ ?\n",
      "0 0\n",
      "Token 9: [SEP] â†’ \n",
      "0 2\n",
      "Token 10: on â†’ On\n",
      "3 8\n",
      "Token 11: april â†’ April\n",
      "9 10\n",
      "Token 12: 4 â†’ 4\n",
      "10 11\n",
      "Token 13: , â†’ ,\n",
      "12 16\n",
      "Token 14: 2008 â†’ 2008\n",
      "16 17\n",
      "Token 15: , â†’ ,\n",
      "18 25\n",
      "Token 16: beyonce â†’ BeyoncÃ©\n",
      "26 33\n",
      "Token 17: married â†’ married\n",
      "34 37\n",
      "Token 18: jay â†’ Jay\n",
      "38 39\n",
      "Token 19: z â†’ Z\n",
      "39 40\n",
      "Token 20: . â†’ .\n",
      "41 44\n",
      "Token 21: she â†’ She\n",
      "45 53\n",
      "Token 22: publicly â†’ publicly\n",
      "54 62\n",
      "Token 23: revealed â†’ revealed\n",
      "63 68\n",
      "Token 24: their â†’ their\n",
      "69 77\n",
      "Token 25: marriage â†’ marriage\n",
      "78 80\n",
      "Token 26: in â†’ in\n",
      "81 82\n",
      "Token 27: a â†’ a\n",
      "83 88\n",
      "Token 28: video â†’ video\n",
      "89 93\n",
      "Token 29: mont â†’ mont\n",
      "93 96\n",
      "Token 30: ##age â†’ age\n",
      "97 99\n",
      "Token 31: at â†’ at\n",
      "100 103\n",
      "Token 32: the â†’ the\n",
      "104 113\n",
      "Token 33: listening â†’ listening\n",
      "114 119\n",
      "Token 34: party â†’ party\n",
      "120 123\n",
      "Token 35: for â†’ for\n",
      "124 127\n",
      "Token 36: her â†’ her\n",
      "128 133\n",
      "Token 37: third â†’ third\n",
      "134 140\n",
      "Token 38: studio â†’ studio\n",
      "141 146\n",
      "Token 39: album â†’ album\n",
      "146 147\n",
      "Token 40: , â†’ ,\n",
      "148 149\n",
      "Token 41: i â†’ I\n",
      "150 152\n",
      "Token 42: am â†’ Am\n",
      "152 153\n",
      "Token 43: . â†’ .\n",
      "153 154\n",
      "Token 44: . â†’ .\n",
      "154 155\n",
      "Token 45: . â†’ .\n",
      "156 161\n",
      "Token 46: sasha â†’ Sasha\n",
      "162 168\n",
      "Token 47: fierce â†’ Fierce\n",
      "168 169\n",
      "Token 48: , â†’ ,\n",
      "170 172\n",
      "Token 49: in â†’ in\n",
      "173 182\n",
      "Token 50: manhattan â†’ Manhattan\n",
      "182 183\n",
      "Token 51: ' â†’ '\n",
      "183 184\n",
      "Token 52: s â†’ s\n",
      "185 189\n",
      "Token 53: sony â†’ Sony\n",
      "190 194\n",
      "Token 54: club â†’ Club\n",
      "195 197\n",
      "Token 55: on â†’ on\n",
      "198 205\n",
      "Token 56: october â†’ October\n",
      "206 208\n",
      "Token 57: 22 â†’ 22\n",
      "208 209\n",
      "Token 58: , â†’ ,\n",
      "210 214\n",
      "Token 59: 2008 â†’ 2008\n",
      "214 215\n",
      "Token 60: . â†’ .\n",
      "216 217\n",
      "Token 61: i â†’ I\n",
      "218 220\n",
      "Token 62: am â†’ Am\n",
      "220 221\n",
      "Token 63: . â†’ .\n",
      "221 222\n",
      "Token 64: . â†’ .\n",
      "222 223\n",
      "Token 65: . â†’ .\n",
      "224 229\n",
      "Token 66: sasha â†’ Sasha\n",
      "230 236\n",
      "Token 67: fierce â†’ Fierce\n",
      "237 240\n",
      "Token 68: was â†’ was\n",
      "241 249\n",
      "Token 69: released â†’ released\n",
      "250 252\n",
      "Token 70: on â†’ on\n",
      "253 261\n",
      "Token 71: november â†’ November\n",
      "262 264\n",
      "Token 72: 18 â†’ 18\n",
      "264 265\n",
      "Token 73: , â†’ ,\n",
      "266 270\n",
      "Token 74: 2008 â†’ 2008\n",
      "271 273\n",
      "Token 75: in â†’ in\n",
      "274 277\n",
      "Token 76: the â†’ the\n",
      "278 284\n",
      "Token 77: united â†’ United\n",
      "285 291\n",
      "Token 78: states â†’ States\n",
      "291 292\n",
      "Token 79: . â†’ .\n",
      "293 296\n",
      "Token 80: the â†’ The\n",
      "297 302\n",
      "Token 81: album â†’ album\n",
      "303 311\n",
      "Token 82: formally â†’ formally\n",
      "312 322\n",
      "Token 83: introduces â†’ introduces\n",
      "323 330\n",
      "Token 84: beyonce â†’ BeyoncÃ©\n",
      "330 331\n",
      "Token 85: ' â†’ '\n",
      "331 332\n",
      "Token 86: s â†’ s\n",
      "333 338\n",
      "Token 87: alter â†’ alter\n",
      "339 342\n",
      "Token 88: ego â†’ ego\n",
      "343 348\n",
      "Token 89: sasha â†’ Sasha\n",
      "349 355\n",
      "Token 90: fierce â†’ Fierce\n",
      "355 356\n",
      "Token 91: , â†’ ,\n",
      "357 366\n",
      "Token 92: conceived â†’ conceived\n",
      "367 373\n",
      "Token 93: during â†’ during\n",
      "374 377\n",
      "Token 94: the â†’ the\n",
      "378 384\n",
      "Token 95: making â†’ making\n",
      "385 387\n",
      "Token 96: of â†’ of\n",
      "388 391\n",
      "Token 97: her â†’ her\n",
      "392 396\n",
      "Token 98: 2003 â†’ 2003\n",
      "397 403\n",
      "Token 99: single â†’ single\n",
      "404 405\n",
      "Token 100: \" â†’ \"\n",
      "405 410\n",
      "Token 101: crazy â†’ Crazy\n",
      "411 413\n",
      "Token 102: in â†’ in\n",
      "414 418\n",
      "Token 103: love â†’ Love\n",
      "418 419\n",
      "Token 104: \" â†’ \"\n",
      "419 420\n",
      "Token 105: , â†’ ,\n",
      "421 428\n",
      "Token 106: selling â†’ selling\n",
      "429 431\n",
      "Token 107: 48 â†’ 48\n",
      "431 432\n",
      "Token 108: ##2 â†’ 2\n",
      "432 433\n",
      "Token 109: , â†’ ,\n",
      "433 436\n",
      "Token 110: 000 â†’ 000\n",
      "437 443\n",
      "Token 111: copies â†’ copies\n",
      "444 446\n",
      "Token 112: in â†’ in\n",
      "447 450\n",
      "Token 113: its â†’ its\n",
      "451 456\n",
      "Token 114: first â†’ first\n",
      "457 461\n",
      "Token 115: week â†’ week\n",
      "461 462\n",
      "Token 116: , â†’ ,\n",
      "463 471\n",
      "Token 117: debuting â†’ debuting\n",
      "472 476\n",
      "Token 118: atop â†’ atop\n",
      "477 480\n",
      "Token 119: the â†’ the\n",
      "481 490\n",
      "Token 120: billboard â†’ Billboard\n",
      "491 494\n",
      "Token 121: 200 â†’ 200\n",
      "494 495\n",
      "Token 122: , â†’ ,\n",
      "496 499\n",
      "Token 123: and â†’ and\n",
      "500 506\n",
      "Token 124: giving â†’ giving\n",
      "507 514\n",
      "Token 125: beyonce â†’ BeyoncÃ©\n",
      "515 518\n",
      "Token 126: her â†’ her\n",
      "519 524\n",
      "Token 127: third â†’ third\n",
      "525 536\n",
      "Token 128: consecutive â†’ consecutive\n",
      "537 543\n",
      "Token 129: number â†’ number\n",
      "543 544\n",
      "Token 130: - â†’ -\n",
      "544 547\n",
      "Token 131: one â†’ one\n",
      "548 553\n",
      "Token 132: album â†’ album\n",
      "554 556\n",
      "Token 133: in â†’ in\n",
      "557 560\n",
      "Token 134: the â†’ the\n",
      "561 563\n",
      "Token 135: us â†’ US\n",
      "563 564\n",
      "Token 136: . â†’ .\n",
      "565 568\n",
      "Token 137: the â†’ The\n",
      "569 574\n",
      "Token 138: album â†’ album\n",
      "575 583\n",
      "Token 139: featured â†’ featured\n",
      "584 587\n",
      "Token 140: the â†’ the\n",
      "588 594\n",
      "Token 141: number â†’ number\n",
      "594 595\n",
      "Token 142: - â†’ -\n",
      "595 598\n",
      "Token 143: one â†’ one\n",
      "599 603\n",
      "Token 144: song â†’ song\n",
      "604 605\n",
      "Token 145: \" â†’ \"\n",
      "605 611\n",
      "Token 146: single â†’ Single\n",
      "612 618\n",
      "Token 147: ladies â†’ Ladies\n",
      "619 620\n",
      "Token 148: ( â†’ (\n",
      "620 623\n",
      "Token 149: put â†’ Put\n",
      "624 625\n",
      "Token 150: a â†’ a\n",
      "626 630\n",
      "Token 151: ring â†’ Ring\n",
      "631 633\n",
      "Token 152: on â†’ on\n",
      "634 636\n",
      "Token 153: it â†’ It\n",
      "636 637\n",
      "Token 154: ) â†’ )\n",
      "637 638\n",
      "Token 155: \" â†’ \"\n",
      "639 642\n",
      "Token 156: and â†’ and\n",
      "643 646\n",
      "Token 157: the â†’ the\n",
      "647 650\n",
      "Token 158: top â†’ top\n",
      "650 651\n",
      "Token 159: - â†’ -\n",
      "651 655\n",
      "Token 160: five â†’ five\n",
      "656 661\n",
      "Token 161: songs â†’ songs\n",
      "662 663\n",
      "Token 162: \" â†’ \"\n",
      "663 665\n",
      "Token 163: if â†’ If\n",
      "666 667\n",
      "Token 164: i â†’ I\n",
      "668 672\n",
      "Token 165: were â†’ Were\n",
      "673 674\n",
      "Token 166: a â†’ a\n",
      "675 678\n",
      "Token 167: boy â†’ Boy\n",
      "678 679\n",
      "Token 168: \" â†’ \"\n",
      "680 683\n",
      "Token 169: and â†’ and\n",
      "684 685\n",
      "Token 170: \" â†’ \"\n",
      "685 689\n",
      "Token 171: halo â†’ Halo\n",
      "689 690\n",
      "Token 172: \" â†’ \"\n",
      "690 691\n",
      "Token 173: . â†’ .\n",
      "692 701\n",
      "Token 174: achieving â†’ Achieving\n",
      "702 705\n",
      "Token 175: the â†’ the\n",
      "706 720\n",
      "Token 176: accomplishment â†’ accomplishment\n",
      "721 723\n",
      "Token 177: of â†’ of\n",
      "724 732\n",
      "Token 178: becoming â†’ becoming\n",
      "733 736\n",
      "Token 179: her â†’ her\n",
      "737 744\n",
      "Token 180: longest â†’ longest\n",
      "744 745\n",
      "Token 181: - â†’ -\n",
      "745 752\n",
      "Token 182: running â†’ running\n",
      "753 756\n",
      "Token 183: hot â†’ Hot\n",
      "757 760\n",
      "Token 184: 100 â†’ 100\n",
      "761 767\n",
      "Token 185: single â†’ single\n",
      "768 770\n",
      "Token 186: in â†’ in\n",
      "771 774\n",
      "Token 187: her â†’ her\n",
      "775 781\n",
      "Token 188: career â†’ career\n",
      "781 782\n",
      "Token 189: , â†’ ,\n",
      "783 784\n",
      "Token 190: \" â†’ \"\n",
      "784 788\n",
      "Token 191: halo â†’ Halo\n",
      "788 789\n",
      "Token 192: \" â†’ \"\n",
      "789 790\n",
      "Token 193: ' â†’ '\n",
      "790 791\n",
      "Token 194: s â†’ s\n",
      "792 799\n",
      "Token 195: success â†’ success\n",
      "800 802\n",
      "Token 196: in â†’ in\n",
      "803 806\n",
      "Token 197: the â†’ the\n",
      "807 809\n",
      "Token 198: us â†’ US\n",
      "810 816\n",
      "Token 199: helped â†’ helped\n",
      "817 824\n",
      "Token 200: beyonce â†’ BeyoncÃ©\n",
      "825 831\n",
      "Token 201: attain â†’ attain\n",
      "832 836\n",
      "Token 202: more â†’ more\n",
      "837 840\n",
      "Token 203: top â†’ top\n",
      "840 841\n",
      "Token 204: - â†’ -\n",
      "841 844\n",
      "Token 205: ten â†’ ten\n",
      "845 852\n",
      "Token 206: singles â†’ singles\n",
      "853 855\n",
      "Token 207: on â†’ on\n",
      "856 859\n",
      "Token 208: the â†’ the\n",
      "860 864\n",
      "Token 209: list â†’ list\n",
      "865 869\n",
      "Token 210: than â†’ than\n",
      "870 873\n",
      "Token 211: any â†’ any\n",
      "874 879\n",
      "Token 212: other â†’ other\n",
      "880 885\n",
      "Token 213: woman â†’ woman\n",
      "886 892\n",
      "Token 214: during â†’ during\n",
      "893 896\n",
      "Token 215: the â†’ the\n",
      "897 902\n",
      "Token 216: 2000s â†’ 2000s\n",
      "902 903\n",
      "Token 217: . â†’ .\n",
      "904 906\n",
      "Token 218: it â†’ It\n",
      "907 911\n",
      "Token 219: also â†’ also\n",
      "912 920\n",
      "Token 220: included â†’ included\n",
      "921 924\n",
      "Token 221: the â†’ the\n",
      "925 935\n",
      "Token 222: successful â†’ successful\n",
      "936 937\n",
      "Token 223: \" â†’ \"\n",
      "937 942\n",
      "Token 224: sweet â†’ Sweet\n",
      "943 949\n",
      "Token 225: dreams â†’ Dreams\n",
      "949 950\n",
      "Token 226: \" â†’ \"\n",
      "950 951\n",
      "Token 227: , â†’ ,\n",
      "952 955\n",
      "Token 228: and â†’ and\n",
      "956 963\n",
      "Token 229: singles â†’ singles\n",
      "964 965\n",
      "Token 230: \" â†’ \"\n",
      "965 969\n",
      "Token 231: diva â†’ Diva\n",
      "969 970\n",
      "Token 232: \" â†’ \"\n",
      "970 971\n",
      "Token 233: , â†’ ,\n",
      "972 973\n",
      "Token 234: \" â†’ \"\n",
      "973 976\n",
      "Token 235: ego â†’ Ego\n",
      "976 977\n",
      "Token 236: \" â†’ \"\n",
      "977 978\n",
      "Token 237: , â†’ ,\n",
      "979 980\n",
      "Token 238: \" â†’ \"\n",
      "980 986\n",
      "Token 239: broken â†’ Broken\n",
      "986 987\n",
      "Token 240: - â†’ -\n",
      "987 994\n",
      "Token 241: hearted â†’ Hearted\n",
      "995 999\n",
      "Token 242: girl â†’ Girl\n",
      "999 1000\n",
      "Token 243: \" â†’ \"\n",
      "1001 1004\n",
      "Token 244: and â†’ and\n",
      "1005 1006\n",
      "Token 245: \" â†’ \"\n",
      "1006 1011\n",
      "Token 246: video â†’ Video\n",
      "1012 1017\n",
      "Token 247: phone â†’ Phone\n",
      "1017 1018\n",
      "Token 248: \" â†’ \"\n",
      "1018 1019\n",
      "Token 249: . â†’ .\n",
      "1020 1023\n",
      "Token 250: the â†’ The\n",
      "1024 1029\n",
      "Token 251: music â†’ music\n",
      "1030 1035\n",
      "Token 252: video â†’ video\n",
      "1036 1039\n",
      "Token 253: for â†’ for\n",
      "1040 1041\n",
      "Token 254: \" â†’ \"\n",
      "1041 1047\n",
      "Token 255: single â†’ Single\n",
      "1048 1054\n",
      "Token 256: ladies â†’ Ladies\n",
      "1054 1055\n",
      "Token 257: \" â†’ \"\n",
      "1056 1059\n",
      "Token 258: has â†’ has\n",
      "1060 1064\n",
      "Token 259: been â†’ been\n",
      "1065 1068\n",
      "Token 260: par â†’ par\n",
      "1068 1070\n",
      "Token 261: ##od â†’ od\n",
      "1070 1073\n",
      "Token 262: ##ied â†’ ied\n",
      "1074 1077\n",
      "Token 263: and â†’ and\n",
      "1078 1080\n",
      "Token 264: im â†’ im\n",
      "1080 1086\n",
      "Token 265: ##itated â†’ itated\n",
      "1087 1093\n",
      "Token 266: around â†’ around\n",
      "1094 1097\n",
      "Token 267: the â†’ the\n",
      "1098 1103\n",
      "Token 268: world â†’ world\n",
      "1103 1104\n",
      "Token 269: , â†’ ,\n",
      "1105 1113\n",
      "Token 270: spawning â†’ spawning\n",
      "1114 1117\n",
      "Token 271: the â†’ the\n",
      "1118 1119\n",
      "Token 272: \" â†’ \"\n",
      "1119 1124\n",
      "Token 273: first â†’ first\n",
      "1125 1130\n",
      "Token 274: major â†’ major\n",
      "1131 1136\n",
      "Token 275: dance â†’ dance\n",
      "1137 1139\n",
      "Token 276: cr â†’ cr\n",
      "1139 1141\n",
      "Token 277: ##az â†’ az\n",
      "1141 1142\n",
      "Token 278: ##e â†’ e\n",
      "1142 1143\n",
      "Token 279: \" â†’ \"\n",
      "1144 1146\n",
      "Token 280: of â†’ of\n",
      "1147 1150\n",
      "Token 281: the â†’ the\n",
      "1151 1159\n",
      "Token 282: internet â†’ Internet\n",
      "1160 1163\n",
      "Token 283: age â†’ age\n",
      "1164 1173\n",
      "Token 284: according â†’ according\n",
      "1174 1176\n",
      "Token 285: to â†’ to\n",
      "1177 1180\n",
      "Token 286: the â†’ the\n",
      "1181 1188\n",
      "Token 287: toronto â†’ Toronto\n",
      "1189 1193\n",
      "Token 288: star â†’ Star\n",
      "1193 1194\n",
      "Token 289: . â†’ .\n",
      "1195 1198\n",
      "Token 290: the â†’ The\n",
      "1199 1204\n",
      "Token 291: video â†’ video\n",
      "1205 1208\n",
      "Token 292: has â†’ has\n",
      "1209 1212\n",
      "Token 293: won â†’ won\n",
      "1213 1220\n",
      "Token 294: several â†’ several\n",
      "1221 1227\n",
      "Token 295: awards â†’ awards\n",
      "1227 1228\n",
      "Token 296: , â†’ ,\n",
      "1229 1238\n",
      "Token 297: including â†’ including\n",
      "1239 1243\n",
      "Token 298: best â†’ Best\n",
      "1244 1249\n",
      "Token 299: video â†’ Video\n",
      "1250 1252\n",
      "Token 300: at â†’ at\n",
      "1253 1256\n",
      "Token 301: the â†’ the\n",
      "1257 1261\n",
      "Token 302: 2009 â†’ 2009\n",
      "1262 1265\n",
      "Token 303: mtv â†’ MTV\n",
      "1266 1272\n",
      "Token 304: europe â†’ Europe\n",
      "1273 1278\n",
      "Token 305: music â†’ Music\n",
      "1279 1285\n",
      "Token 306: awards â†’ Awards\n",
      "1285 1286\n",
      "Token 307: , â†’ ,\n",
      "1287 1290\n",
      "Token 308: the â†’ the\n",
      "1291 1295\n",
      "Token 309: 2009 â†’ 2009\n",
      "1296 1304\n",
      "Token 310: scottish â†’ Scottish\n",
      "1305 1308\n",
      "Token 311: mob â†’ MOB\n",
      "1308 1309\n",
      "Token 312: ##o â†’ O\n",
      "1310 1316\n",
      "Token 313: awards â†’ Awards\n",
      "1316 1317\n",
      "Token 314: , â†’ ,\n",
      "1318 1321\n",
      "Token 315: and â†’ and\n",
      "1322 1325\n",
      "Token 316: the â†’ the\n",
      "1326 1330\n",
      "Token 317: 2009 â†’ 2009\n",
      "1331 1334\n",
      "Token 318: bet â†’ BET\n",
      "1335 1341\n",
      "Token 319: awards â†’ Awards\n",
      "1341 1342\n",
      "Token 320: . â†’ .\n",
      "1343 1345\n",
      "Token 321: at â†’ At\n",
      "1346 1349\n",
      "Token 322: the â†’ the\n",
      "1350 1354\n",
      "Token 323: 2009 â†’ 2009\n",
      "1355 1358\n",
      "Token 324: mtv â†’ MTV\n",
      "1359 1364\n",
      "Token 325: video â†’ Video\n",
      "1365 1370\n",
      "Token 326: music â†’ Music\n",
      "1371 1377\n",
      "Token 327: awards â†’ Awards\n",
      "1377 1378\n",
      "Token 328: , â†’ ,\n",
      "1379 1382\n",
      "Token 329: the â†’ the\n",
      "1383 1388\n",
      "Token 330: video â†’ video\n",
      "1389 1392\n",
      "Token 331: was â†’ was\n",
      "1393 1402\n",
      "Token 332: nominated â†’ nominated\n",
      "1403 1406\n",
      "Token 333: for â†’ for\n",
      "1407 1411\n",
      "Token 334: nine â†’ nine\n",
      "1412 1418\n",
      "Token 335: awards â†’ awards\n",
      "1418 1419\n",
      "Token 336: , â†’ ,\n",
      "1420 1430\n",
      "Token 337: ultimately â†’ ultimately\n",
      "1431 1438\n",
      "Token 338: winning â†’ winning\n",
      "1439 1444\n",
      "Token 339: three â†’ three\n",
      "1445 1454\n",
      "Token 340: including â†’ including\n",
      "1455 1460\n",
      "Token 341: video â†’ Video\n",
      "1461 1463\n",
      "Token 342: of â†’ of\n",
      "1464 1467\n",
      "Token 343: the â†’ the\n",
      "1468 1472\n",
      "Token 344: year â†’ Year\n",
      "1472 1473\n",
      "Token 345: . â†’ .\n",
      "1474 1477\n",
      "Token 346: its â†’ Its\n",
      "1478 1485\n",
      "Token 347: failure â†’ failure\n",
      "1486 1488\n",
      "Token 348: to â†’ to\n",
      "1489 1492\n",
      "Token 349: win â†’ win\n",
      "1493 1496\n",
      "Token 350: the â†’ the\n",
      "1497 1501\n",
      "Token 351: best â†’ Best\n",
      "1502 1508\n",
      "Token 352: female â†’ Female\n",
      "1509 1514\n",
      "Token 353: video â†’ Video\n",
      "1515 1523\n",
      "Token 354: category â†’ category\n",
      "1523 1524\n",
      "Token 355: , â†’ ,\n",
      "1525 1530\n",
      "Token 356: which â†’ which\n",
      "1531 1535\n",
      "Token 357: went â†’ went\n",
      "1536 1538\n",
      "Token 358: to â†’ to\n",
      "1539 1547\n",
      "Token 359: american â†’ American\n",
      "1548 1555\n",
      "Token 360: country â†’ country\n",
      "1556 1559\n",
      "Token 361: pop â†’ pop\n",
      "1560 1566\n",
      "Token 362: singer â†’ singer\n",
      "1567 1573\n",
      "Token 363: taylor â†’ Taylor\n",
      "1574 1579\n",
      "Token 364: swift â†’ Swift\n",
      "1579 1580\n",
      "Token 365: ' â†’ '\n",
      "1580 1581\n",
      "Token 366: s â†’ s\n",
      "1582 1583\n",
      "Token 367: \" â†’ \"\n",
      "1583 1586\n",
      "Token 368: you â†’ You\n",
      "1587 1593\n",
      "Token 369: belong â†’ Belong\n",
      "1594 1598\n",
      "Token 370: with â†’ with\n",
      "1599 1601\n",
      "Token 371: me â†’ Me\n",
      "1601 1602\n",
      "Token 372: \" â†’ \"\n",
      "1602 1603\n",
      "Token 373: , â†’ ,\n",
      "1604 1607\n",
      "Token 374: led â†’ led\n",
      "1608 1610\n",
      "Token 375: to â†’ to\n",
      "1611 1616\n",
      "Token 376: kanye â†’ Kanye\n",
      "1617 1621\n",
      "Token 377: west â†’ West\n",
      "1622 1634\n",
      "Token 378: interrupting â†’ interrupting\n",
      "1635 1638\n",
      "Token 379: the â†’ the\n",
      "1639 1647\n",
      "Token 380: ceremony â†’ ceremony\n",
      "1648 1651\n",
      "Token 381: and â†’ and\n",
      "1652 1659\n",
      "Token 382: beyonce â†’ BeyoncÃ©\n",
      "0 0\n",
      "Token 383: [SEP] â†’ \n",
      "\n",
      "=== åˆ†å— 1 ===\n",
      "0 0\n",
      "Token 0: [CLS] â†’ \n",
      "0 7\n",
      "Token 1: beyonce â†’ Beyonce\n",
      "8 11\n",
      "Token 2: got â†’ got\n",
      "12 19\n",
      "Token 3: married â†’ married\n",
      "20 22\n",
      "Token 4: in â†’ in\n",
      "23 27\n",
      "Token 5: 2008 â†’ 2008\n",
      "28 30\n",
      "Token 6: to â†’ to\n",
      "31 35\n",
      "Token 7: whom â†’ whom\n",
      "35 36\n",
      "Token 8: ? â†’ ?\n",
      "0 0\n",
      "Token 9: [SEP] â†’ \n",
      "1041 1047\n",
      "Token 10: single â†’ Single\n",
      "1048 1054\n",
      "Token 11: ladies â†’ Ladies\n",
      "1054 1055\n",
      "Token 12: \" â†’ \"\n",
      "1056 1059\n",
      "Token 13: has â†’ has\n",
      "1060 1064\n",
      "Token 14: been â†’ been\n",
      "1065 1068\n",
      "Token 15: par â†’ par\n",
      "1068 1070\n",
      "Token 16: ##od â†’ od\n",
      "1070 1073\n",
      "Token 17: ##ied â†’ ied\n",
      "1074 1077\n",
      "Token 18: and â†’ and\n",
      "1078 1080\n",
      "Token 19: im â†’ im\n",
      "1080 1086\n",
      "Token 20: ##itated â†’ itated\n",
      "1087 1093\n",
      "Token 21: around â†’ around\n",
      "1094 1097\n",
      "Token 22: the â†’ the\n",
      "1098 1103\n",
      "Token 23: world â†’ world\n",
      "1103 1104\n",
      "Token 24: , â†’ ,\n",
      "1105 1113\n",
      "Token 25: spawning â†’ spawning\n",
      "1114 1117\n",
      "Token 26: the â†’ the\n",
      "1118 1119\n",
      "Token 27: \" â†’ \"\n",
      "1119 1124\n",
      "Token 28: first â†’ first\n",
      "1125 1130\n",
      "Token 29: major â†’ major\n",
      "1131 1136\n",
      "Token 30: dance â†’ dance\n",
      "1137 1139\n",
      "Token 31: cr â†’ cr\n",
      "1139 1141\n",
      "Token 32: ##az â†’ az\n",
      "1141 1142\n",
      "Token 33: ##e â†’ e\n",
      "1142 1143\n",
      "Token 34: \" â†’ \"\n",
      "1144 1146\n",
      "Token 35: of â†’ of\n",
      "1147 1150\n",
      "Token 36: the â†’ the\n",
      "1151 1159\n",
      "Token 37: internet â†’ Internet\n",
      "1160 1163\n",
      "Token 38: age â†’ age\n",
      "1164 1173\n",
      "Token 39: according â†’ according\n",
      "1174 1176\n",
      "Token 40: to â†’ to\n",
      "1177 1180\n",
      "Token 41: the â†’ the\n",
      "1181 1188\n",
      "Token 42: toronto â†’ Toronto\n",
      "1189 1193\n",
      "Token 43: star â†’ Star\n",
      "1193 1194\n",
      "Token 44: . â†’ .\n",
      "1195 1198\n",
      "Token 45: the â†’ The\n",
      "1199 1204\n",
      "Token 46: video â†’ video\n",
      "1205 1208\n",
      "Token 47: has â†’ has\n",
      "1209 1212\n",
      "Token 48: won â†’ won\n",
      "1213 1220\n",
      "Token 49: several â†’ several\n",
      "1221 1227\n",
      "Token 50: awards â†’ awards\n",
      "1227 1228\n",
      "Token 51: , â†’ ,\n",
      "1229 1238\n",
      "Token 52: including â†’ including\n",
      "1239 1243\n",
      "Token 53: best â†’ Best\n",
      "1244 1249\n",
      "Token 54: video â†’ Video\n",
      "1250 1252\n",
      "Token 55: at â†’ at\n",
      "1253 1256\n",
      "Token 56: the â†’ the\n",
      "1257 1261\n",
      "Token 57: 2009 â†’ 2009\n",
      "1262 1265\n",
      "Token 58: mtv â†’ MTV\n",
      "1266 1272\n",
      "Token 59: europe â†’ Europe\n",
      "1273 1278\n",
      "Token 60: music â†’ Music\n",
      "1279 1285\n",
      "Token 61: awards â†’ Awards\n",
      "1285 1286\n",
      "Token 62: , â†’ ,\n",
      "1287 1290\n",
      "Token 63: the â†’ the\n",
      "1291 1295\n",
      "Token 64: 2009 â†’ 2009\n",
      "1296 1304\n",
      "Token 65: scottish â†’ Scottish\n",
      "1305 1308\n",
      "Token 66: mob â†’ MOB\n",
      "1308 1309\n",
      "Token 67: ##o â†’ O\n",
      "1310 1316\n",
      "Token 68: awards â†’ Awards\n",
      "1316 1317\n",
      "Token 69: , â†’ ,\n",
      "1318 1321\n",
      "Token 70: and â†’ and\n",
      "1322 1325\n",
      "Token 71: the â†’ the\n",
      "1326 1330\n",
      "Token 72: 2009 â†’ 2009\n",
      "1331 1334\n",
      "Token 73: bet â†’ BET\n",
      "1335 1341\n",
      "Token 74: awards â†’ Awards\n",
      "1341 1342\n",
      "Token 75: . â†’ .\n",
      "1343 1345\n",
      "Token 76: at â†’ At\n",
      "1346 1349\n",
      "Token 77: the â†’ the\n",
      "1350 1354\n",
      "Token 78: 2009 â†’ 2009\n",
      "1355 1358\n",
      "Token 79: mtv â†’ MTV\n",
      "1359 1364\n",
      "Token 80: video â†’ Video\n",
      "1365 1370\n",
      "Token 81: music â†’ Music\n",
      "1371 1377\n",
      "Token 82: awards â†’ Awards\n",
      "1377 1378\n",
      "Token 83: , â†’ ,\n",
      "1379 1382\n",
      "Token 84: the â†’ the\n",
      "1383 1388\n",
      "Token 85: video â†’ video\n",
      "1389 1392\n",
      "Token 86: was â†’ was\n",
      "1393 1402\n",
      "Token 87: nominated â†’ nominated\n",
      "1403 1406\n",
      "Token 88: for â†’ for\n",
      "1407 1411\n",
      "Token 89: nine â†’ nine\n",
      "1412 1418\n",
      "Token 90: awards â†’ awards\n",
      "1418 1419\n",
      "Token 91: , â†’ ,\n",
      "1420 1430\n",
      "Token 92: ultimately â†’ ultimately\n",
      "1431 1438\n",
      "Token 93: winning â†’ winning\n",
      "1439 1444\n",
      "Token 94: three â†’ three\n",
      "1445 1454\n",
      "Token 95: including â†’ including\n",
      "1455 1460\n",
      "Token 96: video â†’ Video\n",
      "1461 1463\n",
      "Token 97: of â†’ of\n",
      "1464 1467\n",
      "Token 98: the â†’ the\n",
      "1468 1472\n",
      "Token 99: year â†’ Year\n",
      "1472 1473\n",
      "Token 100: . â†’ .\n",
      "1474 1477\n",
      "Token 101: its â†’ Its\n",
      "1478 1485\n",
      "Token 102: failure â†’ failure\n",
      "1486 1488\n",
      "Token 103: to â†’ to\n",
      "1489 1492\n",
      "Token 104: win â†’ win\n",
      "1493 1496\n",
      "Token 105: the â†’ the\n",
      "1497 1501\n",
      "Token 106: best â†’ Best\n",
      "1502 1508\n",
      "Token 107: female â†’ Female\n",
      "1509 1514\n",
      "Token 108: video â†’ Video\n",
      "1515 1523\n",
      "Token 109: category â†’ category\n",
      "1523 1524\n",
      "Token 110: , â†’ ,\n",
      "1525 1530\n",
      "Token 111: which â†’ which\n",
      "1531 1535\n",
      "Token 112: went â†’ went\n",
      "1536 1538\n",
      "Token 113: to â†’ to\n",
      "1539 1547\n",
      "Token 114: american â†’ American\n",
      "1548 1555\n",
      "Token 115: country â†’ country\n",
      "1556 1559\n",
      "Token 116: pop â†’ pop\n",
      "1560 1566\n",
      "Token 117: singer â†’ singer\n",
      "1567 1573\n",
      "Token 118: taylor â†’ Taylor\n",
      "1574 1579\n",
      "Token 119: swift â†’ Swift\n",
      "1579 1580\n",
      "Token 120: ' â†’ '\n",
      "1580 1581\n",
      "Token 121: s â†’ s\n",
      "1582 1583\n",
      "Token 122: \" â†’ \"\n",
      "1583 1586\n",
      "Token 123: you â†’ You\n",
      "1587 1593\n",
      "Token 124: belong â†’ Belong\n",
      "1594 1598\n",
      "Token 125: with â†’ with\n",
      "1599 1601\n",
      "Token 126: me â†’ Me\n",
      "1601 1602\n",
      "Token 127: \" â†’ \"\n",
      "1602 1603\n",
      "Token 128: , â†’ ,\n",
      "1604 1607\n",
      "Token 129: led â†’ led\n",
      "1608 1610\n",
      "Token 130: to â†’ to\n",
      "1611 1616\n",
      "Token 131: kanye â†’ Kanye\n",
      "1617 1621\n",
      "Token 132: west â†’ West\n",
      "1622 1634\n",
      "Token 133: interrupting â†’ interrupting\n",
      "1635 1638\n",
      "Token 134: the â†’ the\n",
      "1639 1647\n",
      "Token 135: ceremony â†’ ceremony\n",
      "1648 1651\n",
      "Token 136: and â†’ and\n",
      "1652 1659\n",
      "Token 137: beyonce â†’ BeyoncÃ©\n",
      "1660 1663\n",
      "Token 138: imp â†’ imp\n",
      "1663 1666\n",
      "Token 139: ##rov â†’ rov\n",
      "1666 1671\n",
      "Token 140: ##ising â†’ ising\n",
      "1672 1673\n",
      "Token 141: a â†’ a\n",
      "1674 1676\n",
      "Token 142: re â†’ re\n",
      "1676 1677\n",
      "Token 143: - â†’ -\n",
      "1677 1689\n",
      "Token 144: presentation â†’ presentation\n",
      "1690 1692\n",
      "Token 145: of â†’ of\n",
      "1693 1698\n",
      "Token 146: swift â†’ Swift\n",
      "1698 1699\n",
      "Token 147: ' â†’ '\n",
      "1699 1700\n",
      "Token 148: s â†’ s\n",
      "1701 1706\n",
      "Token 149: award â†’ award\n",
      "1707 1713\n",
      "Token 150: during â†’ during\n",
      "1714 1717\n",
      "Token 151: her â†’ her\n",
      "1718 1721\n",
      "Token 152: own â†’ own\n",
      "1722 1732\n",
      "Token 153: acceptance â†’ acceptance\n",
      "1733 1739\n",
      "Token 154: speech â†’ speech\n",
      "1739 1740\n",
      "Token 155: . â†’ .\n",
      "1741 1743\n",
      "Token 156: in â†’ In\n",
      "1744 1749\n",
      "Token 157: march â†’ March\n",
      "1750 1754\n",
      "Token 158: 2009 â†’ 2009\n",
      "1754 1755\n",
      "Token 159: , â†’ ,\n",
      "1756 1763\n",
      "Token 160: beyonce â†’ BeyoncÃ©\n",
      "1764 1772\n",
      "Token 161: embarked â†’ embarked\n",
      "1773 1775\n",
      "Token 162: on â†’ on\n",
      "1776 1779\n",
      "Token 163: the â†’ the\n",
      "1780 1781\n",
      "Token 164: i â†’ I\n",
      "1782 1784\n",
      "Token 165: am â†’ Am\n",
      "1784 1785\n",
      "Token 166: . â†’ .\n",
      "1785 1786\n",
      "Token 167: . â†’ .\n",
      "1786 1787\n",
      "Token 168: . â†’ .\n",
      "1788 1793\n",
      "Token 169: world â†’ World\n",
      "1794 1798\n",
      "Token 170: tour â†’ Tour\n",
      "1798 1799\n",
      "Token 171: , â†’ ,\n",
      "1800 1803\n",
      "Token 172: her â†’ her\n",
      "1804 1810\n",
      "Token 173: second â†’ second\n",
      "1811 1821\n",
      "Token 174: headlining â†’ headlining\n",
      "1822 1831\n",
      "Token 175: worldwide â†’ worldwide\n",
      "1832 1839\n",
      "Token 176: concert â†’ concert\n",
      "1840 1844\n",
      "Token 177: tour â†’ tour\n",
      "1844 1845\n",
      "Token 178: , â†’ ,\n",
      "1846 1856\n",
      "Token 179: consisting â†’ consisting\n",
      "1857 1859\n",
      "Token 180: of â†’ of\n",
      "1860 1863\n",
      "Token 181: 108 â†’ 108\n",
      "1864 1869\n",
      "Token 182: shows â†’ shows\n",
      "1869 1870\n",
      "Token 183: , â†’ ,\n",
      "1871 1879\n",
      "Token 184: grossing â†’ grossing\n",
      "1880 1881\n",
      "Token 185: $ â†’ $\n",
      "1881 1884\n",
      "Token 186: 119 â†’ 119\n",
      "1884 1885\n",
      "Token 187: . â†’ .\n",
      "1885 1886\n",
      "Token 188: 5 â†’ 5\n",
      "1887 1894\n",
      "Token 189: million â†’ million\n",
      "1894 1895\n",
      "Token 190: . â†’ .\n",
      "0 0\n",
      "Token 191: [SEP] â†’ \n"
     ]
    }
   ],
   "source": [
    "# éå†æ¯ä¸ªåˆ†å—\n",
    "for chunk_idx in range(len(tokenized_example[\"input_ids\"])):\n",
    "    print(f\"\\n=== åˆ†å— {chunk_idx} ===\")\n",
    "    \n",
    "    # è·å–å½“å‰åˆ†å—çš„æ•°æ®\n",
    "    input_ids = tokenized_example[\"input_ids\"][chunk_idx]\n",
    "    offset_mapping = tokenized_example[\"offset_mapping\"][chunk_idx]\n",
    "    token_type_ids = tokenized_example[\"token_type_ids\"][chunk_idx]\n",
    "\n",
    "    # éå†åˆ†å—å†…çš„æ¯ä¸ª token\n",
    "    for token_idx, (token_id, offset, token_type) in enumerate(zip(input_ids, offset_mapping, token_type_ids)):\n",
    "        # æ ¹æ® token_type é€‰æ‹©æ¥æºæ–‡æœ¬\n",
    "        if token_type == 0:\n",
    "            source_text = example[\"question\"]\n",
    "        else:\n",
    "            source_text = example[\"context\"]\n",
    "\n",
    "        # å…³é”®ä¿®å¤ç‚¹ï¼šåˆ†è§£ offset å…ƒç»„ä¸º start å’Œ end\n",
    "        start = offset[0]  # èµ·å§‹å­—ç¬¦ä½ç½®\n",
    "        end = offset[1]    # ç»“æŸå­—ç¬¦ä½ç½®\n",
    "        print(start, end)\n",
    "        original_text = source_text[start:end]\n",
    "        \n",
    "        # è½¬æ¢ token_id ä¸ºå¯è¯»æ–‡æœ¬\n",
    "        token_str = tokenizer.convert_ids_to_tokens([token_id])[0]  # å–åˆ—è¡¨ç¬¬ä¸€ä¸ªå…ƒç´ \n",
    "        \n",
    "        # æ‰“å°ç»“æœ\n",
    "        print(f\"Token {token_idx}: {token_str} â†’ {original_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”¨æœ€ç®€å•çš„æ¯”å–»è§£é‡Šè¿™æ®µä»£ç ï¼š\n",
    "\n",
    "**1. åˆ†å—ï¼ˆåˆ‡ä¹¦ï¼‰**  \n",
    "- å°±åƒä¸€æœ¬åšä¹¦æ‹†æˆå‡ æœ¬å°å†Œå­ï¼Œæ¯æœ¬æœ€å¤š512é¡µï¼ˆæ¨¡å‹ä¸€æ¬¡è¯»ä¸å®Œé•¿æ–‡æœ¬ï¼‰\n",
    "\n",
    "**2. æ–‡å­—å˜æ•°å­—ï¼ˆåŠ å¯†ï¼‰**  \n",
    "- æŠŠæ¯ä¸ªå­—å˜æˆæ•°å­—å¯†ç ï¼Œæ¯”å¦‚ \"è´\"â†’100ï¼Œ\"çˆ·\"â†’101  \n",
    "- `input_ids` å°±æ˜¯è¿™äº›å¯†ç ç»„æˆçš„åˆ—è¡¨ï¼š[100, 101, ...]\n",
    "\n",
    "**3. è®°ä½ç½®ï¼ˆä¹¦ç­¾ï¼‰**  \n",
    "- `offset_mapping` è®°å½•æ¯ä¸ªå¯†ç åœ¨åŸæ–‡çš„ä½ç½®ï¼Œæ¯”å¦‚ (0,2) è¡¨ç¤ºå‰ä¸¤ä¸ªå­—\n",
    "\n",
    "**4. åŒºåˆ†é—®é¢˜å’Œç­”æ¡ˆï¼ˆè´´æ ‡ç­¾ï¼‰**  \n",
    "- `token_type_ids=0` è¡¨ç¤ºæ–‡å­—æ¥è‡ªé—®é¢˜ï¼ˆå¦‚ \"è´çˆ·å“ªå¹´ç»“å©šï¼Ÿ\"ï¼‰  \n",
    "- `token_type_ids=1` è¡¨ç¤ºæ–‡å­—æ¥è‡ªç­”æ¡ˆï¼ˆå¦‚ \"2000å¹´...\"ï¼‰\n",
    "\n",
    "**5. æ‰¾å¯¹åº”æ–‡å­—ï¼ˆè§£å¯†ï¼‰**  \n",
    "- ç”¨å¯†ç æœ¬æŠŠæ•°å­—è½¬å›æ–‡å­—  \n",
    "- æ ¹æ®ä½ç½®æ ‡ç­¾ï¼Œä»é—®é¢˜æˆ–ç­”æ¡ˆæ–‡æœ¬æˆªå–å¯¹åº”æ–‡å­—\n",
    "\n",
    "**å°±åƒè¿™æ ·ï¼š**  \n",
    "å¯†ç  `100` â†’ æŸ¥å¯†ç æœ¬ â†’ æ˜¯\"è´\" â†’ åœ¨é—®é¢˜ç¬¬0-2ä¸ªä½ç½® â†’ æˆªå–\"è´çˆ·\"\n",
    "\n",
    "æ•´ä¸ªè¿‡ç¨‹è®©è®¡ç®—æœºåƒäººç±»ä¸€æ ·ï¼šå…ˆçœ‹é—®é¢˜ï¼Œå†å¿«é€Ÿç¿»ä¹¦æ‰¾ç­”æ¡ˆä½ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beyonce got married in 2008 to whom?'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On April 4, 2008, BeyoncÃ© married Jay Z. She publicly revealed their marriage in a video montage at the listening party for her third studio album, I Am... Sasha Fierce, in Manhattan\\'s Sony Club on October 22, 2008. I Am... Sasha Fierce was released on November 18, 2008 in the United States. The album formally introduces BeyoncÃ©\\'s alter ego Sasha Fierce, conceived during the making of her 2003 single \"Crazy in Love\", selling 482,000 copies in its first week, debuting atop the Billboard 200, and giving BeyoncÃ© her third consecutive number-one album in the US. The album featured the number-one song \"Single Ladies (Put a Ring on It)\" and the top-five songs \"If I Were a Boy\" and \"Halo\". Achieving the accomplishment of becoming her longest-running Hot 100 single in her career, \"Halo\"\\'s success in the US helped BeyoncÃ© attain more top-ten singles on the list than any other woman during the 2000s. It also included the successful \"Sweet Dreams\", and singles \"Diva\", \"Ego\", \"Broken-Hearted Girl\" and \"Video Phone\". The music video for \"Single Ladies\" has been parodied and imitated around the world, spawning the \"first major dance craze\" of the Internet age according to the Toronto Star. The video has won several awards, including Best Video at the 2009 MTV Europe Music Awards, the 2009 Scottish MOBO Awards, and the 2009 BET Awards. At the 2009 MTV Video Music Awards, the video was nominated for nine awards, ultimately winning three including Video of the Year. Its failure to win the Best Female Video category, which went to American country pop singer Taylor Swift\\'s \"You Belong with Me\", led to Kanye West interrupting the ceremony and BeyoncÃ© improvising a re-presentation of Swift\\'s award during her own acceptance speech. In March 2009, BeyoncÃ© embarked on the I Am... World Tour, her second headlining worldwide concert tour, consisting of 108 shows, grossing $119.5 million.'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å€ŸåŠ©`tokenized_example`çš„`sequence_ids`æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿çš„åŒºåˆ†tokençš„æ¥æºç¼–å·ï¼š\n",
    "\n",
    "- å¯¹äºç‰¹æ®Šæ ‡è®°ï¼šè¿”å›Noneï¼Œ\n",
    "- å¯¹äºæ­£æ–‡Tokenï¼šè¿”å›å¥å­ç¼–å·ï¼ˆä»0å¼€å§‹ç¼–å·ï¼‰ã€‚\n",
    "\n",
    "ç»¼ä¸Šï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿çš„åœ¨ä¸€ä¸ªè¾“å…¥ç‰¹å¾ä¸­æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸ Tokenã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18 19\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹æ ‡è®°ç´¢å¼•ã€‚\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„ç»“æŸæ ‡è®°ç´¢å¼•ã€‚\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºspanèŒƒå›´ï¼ˆå¦‚æœè¶…å‡ºèŒƒå›´ï¼Œè¯¥ç‰¹å¾å°†ä»¥CLSæ ‡è®°ç´¢å¼•æ ‡è®°ï¼‰ã€‚\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # å°†token_start_indexå’Œtoken_end_indexç§»åŠ¨åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚\n",
    "    # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç§»åˆ°æœ€åä¸€ä¸ªæ ‡è®°ä¹‹åï¼ˆè¾¹ç•Œæƒ…å†µï¼‰ã€‚\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"ç­”æ¡ˆä¸åœ¨æ­¤ç‰¹å¾ä¸­ã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°æ£€æŸ¥æ˜¯å¦å‡†ç¡®æ‰¾åˆ°äº†èµ·å§‹ä½ç½®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jay z\n",
      "Jay Z\n"
     ]
    }
   ],
   "source": [
    "# é€šè¿‡æŸ¥æ‰¾ offset mapping ä½ç½®ï¼Œè§£ç  context ä¸­çš„ç­”æ¡ˆ \n",
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "# ç›´æ¥æ‰“å° æ•°æ®é›†ä¸­çš„æ ‡å‡†ç­”æ¡ˆï¼ˆanswer[\"text\"])\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…³äºå¡«å……çš„ç­–ç•¥\n",
    "\n",
    "- å¯¹äºæ²¡æœ‰è¶…è¿‡æœ€å¤§é•¿åº¦çš„æ–‡æœ¬ï¼Œå¡«å……è¡¥é½é•¿åº¦ã€‚\n",
    "- å¯¹äºéœ€è¦å·¦ä¾§å¡«å……çš„æ¨¡å‹ï¼Œäº¤æ¢ question å’Œ context é¡ºåº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•´åˆä»¥ä¸Šæ‰€æœ‰é¢„å¤„ç†æ­¥éª¤\n",
    "\n",
    "è®©æˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹æ•´åˆåˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°è®­ç»ƒé›†ã€‚\n",
    "\n",
    "é’ˆå¯¹ä¸å¯å›ç­”çš„æƒ…å†µï¼ˆä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œç­”æ¡ˆåœ¨å¦ä¸€ä¸ªç‰¹å¾ä¸­ï¼‰ï¼Œæˆ‘ä»¬ä¸ºå¼€å§‹å’Œç»“æŸä½ç½®éƒ½è®¾ç½®äº†clsç´¢å¼•ã€‚\n",
    "\n",
    "å¦‚æœallow_impossible_answersæ ‡å¿—ä¸ºFalseï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç®€å•åœ°ä»è®­ç»ƒé›†ä¸­ä¸¢å¼ƒè¿™äº›ç¤ºä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§å¯èƒ½æœ‰å¾ˆå¤šç©ºç™½å­—ç¬¦ï¼Œè¿™å¯¹æˆ‘ä»¬æ²¡æœ‰ç”¨ï¼Œè€Œä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡çš„æˆªæ–­å¤±è´¥\n",
    "    # ï¼ˆæ ‡è®°åŒ–çš„é—®é¢˜å°†å ç”¨å¤§é‡ç©ºé—´ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ é™¤å·¦ä¾§çš„ç©ºç™½å­—ç¬¦ã€‚\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†ä¿ç•™æº¢å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨æ­¥å¹…ï¼ˆstrideï¼‰ã€‚\n",
    "    # å½“ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªç¤ºä¾‹å¯èƒ½æä¾›å¤šä¸ªç‰¹å¾ï¼Œå…¶ä¸­æ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡æœ‰ä¸€äº›é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç»™æˆ‘ä»¬æä¾›å¤šä¸ªç‰¹å¾ï¼ˆå¦‚æœå®ƒå…·æœ‰å¾ˆé•¿çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æä¾›äº†è¿™ä¸ªæ˜ å°„å…³ç³»ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # åç§»æ˜ å°„å°†ä¸ºæˆ‘ä»¬æä¾›ä»ä»¤ç‰Œåˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬è®¡ç®—å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ã€‚\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # è®©æˆ‘ä»¬ä¸ºè¿™äº›ç¤ºä¾‹è¿›è¡Œæ ‡è®°ï¼\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # æˆ‘ä»¬å°†ä½¿ç”¨ CLS ç‰¹æ®Š token çš„ç´¢å¼•æ¥æ ‡è®°ä¸å¯èƒ½çš„ç­”æ¡ˆã€‚\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£ä¸Šä¸‹æ–‡å’Œé—®é¢˜æ˜¯ä»€ä¹ˆï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥æä¾›å¤šä¸ªè·¨åº¦ï¼Œè¿™æ˜¯åŒ…å«æ­¤æ–‡æœ¬è·¨åº¦çš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # å¦‚æœæ²¡æœ‰ç»™å‡ºç­”æ¡ˆï¼Œåˆ™å°†cls_indexè®¾ç½®ä¸ºç­”æ¡ˆã€‚\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹å’Œç»“æŸå­—ç¬¦ç´¢å¼•ã€‚\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹ä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„ç»“æŸä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºè·¨åº¦ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥ç‰¹å¾çš„æ ‡ç­¾å°†ä½¿ç”¨CLSç´¢å¼•ï¼‰ã€‚\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # å¦åˆ™ï¼Œå°†token_start_indexå’Œtoken_end_indexç§»åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚\n",
    "                # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼ˆè¾¹ç¼˜æƒ…å†µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€åä¸€ä¸ªåç§»ä¹‹åç»§ç»­ã€‚\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "#### datasets.map çš„è¿›é˜¶ä½¿ç”¨\n",
    "\n",
    "ä½¿ç”¨ `datasets.map` æ–¹æ³•å°† `prepare_train_features` åº”ç”¨äºæ‰€æœ‰è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®ï¼š\n",
    "\n",
    "- batched: æ‰¹é‡å¤„ç†æ•°æ®ã€‚\n",
    "- remove_columns: å› ä¸ºé¢„å¤„ç†æ›´æ”¹äº†æ ·æœ¬çš„æ•°é‡ï¼Œæ‰€ä»¥åœ¨åº”ç”¨å®ƒæ—¶éœ€è¦åˆ é™¤æ—§åˆ—ã€‚\n",
    "- load_from_cache_fileï¼šæ˜¯å¦ä½¿ç”¨datasetsåº“çš„è‡ªåŠ¨ç¼“å­˜\n",
    "\n",
    "datasets åº“é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®ï¼Œå®ç°äº†é«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹ä¼ é€’ç»™ map çš„å‡½æ•°æ˜¯å¦å·²æ›´æ”¹ï¼ˆå› æ­¤éœ€è¦ä¸ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰ã€‚å¦‚æœåœ¨è°ƒç”¨ map æ—¶è®¾ç½® `load_from_cache_file=False`ï¼Œå¯ä»¥å¼ºåˆ¶é‡æ–°åº”ç”¨é¢„å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7014558b1240b3a16d6ea754784757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ba79be8af24e7fa52073b44a58b275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## å¾®è°ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "ç°åœ¨æˆ‘ä»¬çš„æ•°æ®å·²ç»å‡†å¤‡å¥½ç”¨äºè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒã€‚\n",
    "\n",
    "ç”±äºæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯é—®ç­”ï¼Œæˆ‘ä»¬ä½¿ç”¨ `AutoModelForQuestionAnswering` ç±»ã€‚(å¯¹æ¯” Yelp è¯„è®ºæ‰“åˆ†ä½¿ç”¨çš„æ˜¯ `AutoModelForSequenceClassification` ç±»ï¼‰\n",
    "\n",
    "è­¦å‘Šé€šçŸ¥æˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆ`vocab_transform` å’Œ `vocab_layer_norm` å±‚ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡ï¼ˆ`pre_classifier` å’Œ `classifier` å±‚ï¼‰ã€‚åœ¨å¾®è°ƒæ¨¡å‹æƒ…å†µä¸‹æ˜¯ç»å¯¹æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ é™¤ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å¤´éƒ¨ï¼Œå¹¶ç”¨ä¸€ä¸ªæ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒï¼Œå¯¹äºè¿™ä¸ªæ–°å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“ä¼šè­¦å‘Šæˆ‘ä»¬åœ¨ç”¨å®ƒè¿›è¡Œæ¨ç†ä¹‹å‰åº”è¯¥å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "#### è®­ç»ƒè¶…å‚æ•°ï¼ˆTrainingArgumentsï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collatorï¼ˆæ•°æ®æ•´ç†å™¨ï¼‰\n",
    "\n",
    "æ•°æ®æ•´ç†å™¨å°†è®­ç»ƒæ•°æ®æ•´ç†ä¸ºæ‰¹æ¬¡æ•°æ®ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒæ—¶çš„æ‰¹æ¬¡å¤„ç†ã€‚æœ¬æ•™ç¨‹ä½¿ç”¨é»˜è®¤çš„ `default_data_collator`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "### å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼ˆTrainerï¼‰\n",
    "\n",
    "ä¸ºäº†å‡å°‘è®­ç»ƒæ—¶é—´ï¼ˆéœ€è¦å¤§é‡ç®—åŠ›æ”¯æŒï¼‰ï¼Œæˆ‘ä»¬ä¸åœ¨æœ¬æ•™ç¨‹çš„è®­ç»ƒæ¨¡å‹è¿‡ç¨‹ä¸­è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "è€Œæ˜¯è®­ç»ƒå®Œæˆåï¼Œå†ç‹¬ç«‹è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU ä½¿ç”¨æƒ…å†µ\n",
    "\n",
    "è®­ç»ƒæ•°æ®ä¸æ¨¡å‹é…ç½®ï¼š\n",
    "\n",
    "- SQUAD v1.1\n",
    "- model_checkpoint = \"distilbert-base-uncased\"\n",
    "- batch_size = 64\n",
    "\n",
    "NVIDIA GPU ä½¿ç”¨æƒ…å†µï¼š\n",
    "\n",
    "```shell\n",
    "Every 1.0s: nvidia-smi                                                   Wed Dec 20 15:39:57 2023\n",
    "\n",
    "Wed Dec 20 15:39:57 2023\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |\n",
    "| N/A   67C    P0              67W /  70W |  14617MiB / 15360MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A     16384      C   /root/miniconda3/bin/python               14612MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4152/4152 2:23:19, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.491100</td>\n",
       "      <td>1.249441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.108800</td>\n",
       "      <td>1.161671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.975700</td>\n",
       "      <td>1.158766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-4000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4152, training_loss=1.3038662743246854, metrics={'train_runtime': 8602.4737, 'train_samples_per_second': 30.872, 'train_steps_per_second': 0.483, 'total_flos': 2.602335381127373e+16, 'train_loss': 1.3038662743246854, 'epoch': 3.0})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒå®Œæˆåï¼Œç¬¬ä¸€æ—¶é—´ä¿å­˜æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è¯„ä¼°æ¨¡å‹è¾“å‡ºéœ€è¦ä¸€äº›é¢å¤–çš„å¤„ç†ï¼šå°†æ¨¡å‹çš„é¢„æµ‹æ˜ å°„å›ä¸Šä¸‹æ–‡çš„éƒ¨åˆ†ã€‚**\n",
    "\n",
    "æ¨¡å‹ç›´æ¥è¾“å‡ºçš„æ˜¯é¢„æµ‹ç­”æ¡ˆçš„`èµ·å§‹ä½ç½®`å’Œ`ç»“æŸä½ç½®`çš„**logits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç±»ä¼¼å­—å…¸çš„å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æŸå¤±ï¼ˆå› ä¸ºæˆ‘ä»¬æä¾›äº†æ ‡ç­¾ï¼‰ï¼Œä»¥åŠèµ·å§‹å’Œç»“æŸlogitsã€‚æˆ‘ä»¬ä¸éœ€è¦æŸå¤±æ¥è¿›è¡Œé¢„æµ‹ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹logitsï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  43, 118, 108,  72,  35, 108,  34,  73,  41,  80,  91,\n",
       "         156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  41,  35,  42,  77,\n",
       "          11,  44,  27, 133,  66,  40,  87,  44,  43,  41, 127,  26,  28,  33,\n",
       "          87, 127,  95,  25,  43, 132,  42,  29,  44,  46,  24,  44,  65,  58,\n",
       "          81,  14,  59,  72,  25,  36,  57,  43], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  44, 118, 109,  75,  37, 109,  36,  76,  42,  83,  94,\n",
       "         158,  35,  83,  94,  83,  60,  80,  31,  43,  54,  42,  35,  43,  80,\n",
       "          13,  45,  28, 133,  66,  41,  89,  45,  44,  42, 127,  27,  30,  34,\n",
       "          32, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,\n",
       "          81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¦‚ä½•ä»æ¨¡å‹è¾“å‡ºçš„ä½ç½® logit ç»„åˆæˆç­”æ¡ˆ\n",
    "\n",
    "æˆ‘ä»¬æœ‰æ¯ä¸ªç‰¹å¾å’Œæ¯ä¸ªæ ‡è®°çš„logitã€‚åœ¨æ¯ä¸ªç‰¹å¾ä¸­ä¸ºæ¯ä¸ªæ ‡è®°é¢„æµ‹ç­”æ¡ˆæœ€æ˜æ˜¾çš„æ–¹æ³•æ˜¯ï¼Œå°†èµ·å§‹logitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºèµ·å§‹ä½ç½®ï¼Œå°†ç»“æŸlogitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºç»“æŸä½ç½®ã€‚\n",
    "\n",
    "åœ¨è®¸å¤šæƒ…å†µä¸‹è¿™ç§æ–¹å¼æ•ˆæœå¾ˆå¥½ï¼Œä½†æ˜¯å¦‚æœæ­¤é¢„æµ‹ç»™å‡ºäº†ä¸å¯èƒ½çš„ç»“æœè¯¥æ€ä¹ˆåŠï¼Ÿæ¯”å¦‚ï¼šèµ·å§‹ä½ç½®å¯èƒ½å¤§äºç»“æŸä½ç½®ï¼Œæˆ–è€…æŒ‡å‘é—®é¢˜ä¸­çš„æ–‡æœ¬ç‰‡æ®µè€Œä¸æ˜¯ç­”æ¡ˆã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æŸ¥çœ‹ç¬¬äºŒå¥½çš„é¢„æµ‹ï¼Œçœ‹å®ƒæ˜¯å¦ç»™å‡ºäº†ä¸€ä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼Œå¹¶é€‰æ‹©å®ƒã€‚\n",
    "\n",
    "é€‰æ‹©ç¬¬äºŒå¥½çš„ç­”æ¡ˆå¹¶ä¸åƒé€‰æ‹©æœ€ä½³ç­”æ¡ˆé‚£ä¹ˆå®¹æ˜“ï¼š\n",
    "- å®ƒæ˜¯èµ·å§‹logitsä¸­ç¬¬äºŒä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­æœ€ä½³ç´¢å¼•å—ï¼Ÿ\n",
    "- è¿˜æ˜¯èµ·å§‹logitsä¸­æœ€ä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­ç¬¬äºŒä½³ç´¢å¼•ï¼Ÿ\n",
    "- å¦‚æœç¬¬äºŒå¥½çš„ç­”æ¡ˆä¹Ÿä¸å¯èƒ½ï¼Œé‚£ä¹ˆå¯¹äºç¬¬ä¸‰å¥½çš„ç­”æ¡ˆï¼Œæƒ…å†µä¼šæ›´åŠ æ£˜æ‰‹ã€‚\n",
    "\n",
    "ä¸ºäº†å¯¹ç­”æ¡ˆè¿›è¡Œåˆ†ç±»ï¼Œ\n",
    "1. å°†ä½¿ç”¨é€šè¿‡æ·»åŠ èµ·å§‹å’Œç»“æŸlogitsè·å¾—çš„åˆ†æ•°\n",
    "1. è®¾è®¡ä¸€ä¸ªåä¸º`n_best_size`çš„è¶…å‚æ•°ï¼Œé™åˆ¶ä¸å¯¹æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆè¿›è¡Œæ’åºã€‚\n",
    "1. æˆ‘ä»¬å°†é€‰æ‹©èµ·å§‹å’Œç»“æŸlogitsä¸­çš„æœ€ä½³ç´¢å¼•ï¼Œå¹¶æ”¶é›†è¿™äº›é¢„æµ‹çš„æ‰€æœ‰ç­”æ¡ˆã€‚\n",
    "1. åœ¨æ£€æŸ¥æ¯ä¸€ä¸ªæ˜¯å¦æœ‰æ•ˆåï¼Œæˆ‘ä»¬å°†æŒ‰ç…§å…¶åˆ†æ•°å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œå¹¶ä¿ç•™æœ€ä½³çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šæ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å®ƒä»¬çš„å¾—åˆ†å¯¹`valid_answers`è¿›è¡Œæ’åºï¼Œå¹¶ä»…ä¿ç•™æœ€ä½³ç­”æ¡ˆã€‚å”¯ä¸€å‰©ä¸‹çš„é—®é¢˜æ˜¯å¦‚ä½•æ£€æŸ¥ç»™å®šçš„è·¨åº¦æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼ˆè€Œä¸æ˜¯é—®é¢˜ä¸­ï¼‰ï¼Œä»¥åŠå¦‚ä½•è·å–å…¶ä¸­çš„æ–‡æœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‘æˆ‘ä»¬çš„éªŒè¯ç‰¹å¾æ·»åŠ ä¸¤ä¸ªå†…å®¹ï¼š\n",
    "\n",
    "- ç”Ÿæˆè¯¥ç‰¹å¾çš„ç¤ºä¾‹çš„IDï¼ˆå› ä¸ºæ¯ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œå¦‚å‰æ‰€ç¤ºï¼‰ï¼›\n",
    "- åç§»æ˜ å°„ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›ä»æ ‡è®°ç´¢å¼•åˆ°ä¸Šä¸‹æ–‡ä¸­å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ç¨å¾®ä¸åŒäº`prepare_train_features`æ¥é‡æ–°å¤„ç†éªŒè¯é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚\n",
    "    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ\n",
    "    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†`prepare_validation_features`åº”ç”¨åˆ°æ•´ä¸ªéªŒè¯é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbc85bdcee9457891a7cc94d54fcd8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trainer`ä¼šéšè—æ¨¡å‹ä¸ä½¿ç”¨çš„åˆ—ï¼ˆåœ¨è¿™é‡Œæ˜¯`example_id`å’Œ`offset_mapping`ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬è¿›è¡Œåå¤„ç†ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬é‡æ–°è®¾ç½®å›æ¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹è¿›ä¹‹å‰çš„æµ‹è¯•ï¼š\n",
    "\n",
    "ç”±äºåœ¨åç§»æ˜ å°„ä¸­ï¼Œå½“å®ƒå¯¹åº”äºé—®é¢˜çš„ä¸€éƒ¨åˆ†æ—¶ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºNoneï¼Œå› æ­¤å¯ä»¥è½»æ¾æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å®Œå…¨åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä»è€ƒè™‘ä¸­æ’é™¤éå¸¸é•¿çš„ç­”æ¡ˆï¼ˆå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ï¼‰ã€‚\n",
    "\n",
    "å±•å¼€è¯´ä¸‹å…·ä½“å®ç°ï¼š\n",
    "- é¦–å…ˆä»æ¨¡å‹è¾“å‡ºä¸­è·å–èµ·å§‹å’Œç»“æŸçš„é€»è¾‘å€¼ï¼ˆlogitsï¼‰ï¼Œè¿™äº›å€¼è¡¨æ˜ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­å¯èƒ½å¼€å§‹å’Œç»“æŸçš„ä½ç½®ã€‚\n",
    "- ç„¶åï¼Œå®ƒä½¿ç”¨åç§»æ˜ å°„ï¼ˆoffset_mappingï¼‰æ¥æ‰¾åˆ°è¿™äº›é€»è¾‘å€¼åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å…·ä½“ä½ç½®ã€‚\n",
    "- æ¥ä¸‹æ¥ï¼Œä»£ç éå†å¯èƒ½çš„å¼€å§‹å’Œç»“æŸç´¢å¼•ç»„åˆï¼Œæ’é™¤é‚£äº›ä¸åœ¨ä¸Šä¸‹æ–‡èŒƒå›´å†…æˆ–é•¿åº¦ä¸åˆé€‚çš„ç­”æ¡ˆã€‚\n",
    "- å¯¹äºæœ‰æ•ˆçš„ç­”æ¡ˆï¼Œå®ƒè®¡ç®—å‡ºä¸€ä¸ªåˆ†æ•°ï¼ˆåŸºäºå¼€å§‹å’Œç»“æŸé€»è¾‘å€¼çš„å’Œï¼‰ï¼Œå¹¶å°†ç­”æ¡ˆåŠå…¶åˆ†æ•°å­˜å‚¨èµ·æ¥ã€‚\n",
    "- æœ€åï¼Œå®ƒæ ¹æ®åˆ†æ•°å¯¹ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œå¹¶è¿”å›å¾—åˆ†æœ€é«˜çš„å‡ ä¸ªç­”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 15.986347, 'text': 'Denver Broncos'},\n",
       " {'score': 14.585561,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 13.152991, 'text': 'Carolina Panthers'},\n",
       " {'score': 12.38233, 'text': 'Broncos'},\n",
       " {'score': 10.981544,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 10.852013,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 10.635618,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 10.283654, 'text': 'Denver'},\n",
       " {'score': 9.451225,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 9.234833,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 8.7582445,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 8.187819,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 8.134832, 'text': 'Panthers'},\n",
       " {'score': 8.092252,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 7.7162285,\n",
       "  'text': 'the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 7.595868,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10'},\n",
       " {'score': 7.382572,\n",
       "  'text': 'National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 7.320059,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 6.755249, 'text': 'Carolina'},\n",
       " {'score': 6.728976, 'text': 'champion Denver Broncos'}]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°æ¯”è¾ƒæ¨¡å‹è¾“å‡ºå’Œæ ‡å‡†ç­”æ¡ˆï¼ˆGround-truthï¼‰æ˜¯å¦ä¸€è‡´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ¨¡å‹æœ€é«˜æ¦‚ç‡çš„è¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´**\n",
    "\n",
    "æ­£å¦‚ä¸Šé¢çš„ä»£ç æ‰€ç¤ºï¼Œè¿™åœ¨ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šå¾ˆå®¹æ˜“ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å®ƒæ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚\n",
    "\n",
    "å¯¹äºå…¶ä»–ç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªç¤ºä¾‹ä¸å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„å…³ç³»ã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œç”±äºä¸€ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å°†ç”±ç»™å®šç¤ºä¾‹ç”Ÿæˆçš„æ‰€æœ‰ç‰¹å¾ä¸­çš„æ‰€æœ‰ç­”æ¡ˆæ±‡é›†åœ¨ä¸€èµ·ï¼Œç„¶åé€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç æ„å»ºäº†ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•åˆ°å…¶å¯¹åº”ç‰¹å¾ç´¢å¼•çš„æ˜ å°„å…³ç³»ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“`squad_v2 = True`æ—¶ï¼Œæœ‰ä¸€å®šæ¦‚ç‡å‡ºç°ä¸å¯èƒ½çš„ç­”æ¡ˆï¼ˆimpossible answer)ã€‚\n",
    "\n",
    "ä¸Šé¢çš„ä»£ç ä»…ä¿ç•™åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬è¿˜éœ€è¦è·å–ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°ï¼ˆå…¶èµ·å§‹å’Œç»“æŸç´¢å¼•å¯¹åº”äºCLSæ ‡è®°çš„ç´¢å¼•ï¼‰ã€‚\n",
    "\n",
    "å½“ä¸€ä¸ªç¤ºä¾‹ç”Ÿæˆå¤šä¸ªç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ‰€æœ‰ç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆéƒ½é¢„æµ‹å‡ºç°ä¸å¯èƒ½ç­”æ¡ˆæ—¶ï¼ˆå› ä¸ºä¸€ä¸ªç‰¹å¾å¯èƒ½ä¹‹æ‰€ä»¥èƒ½å¤Ÿé¢„æµ‹å‡ºä¸å¯èƒ½ç­”æ¡ˆï¼Œæ˜¯å› ä¸ºç­”æ¡ˆä¸åœ¨å®ƒå¯ä»¥è®¿é—®çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼‰ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸€ä¸ªç¤ºä¾‹ä¸­ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°æ˜¯è¯¥ç¤ºä¾‹ç”Ÿæˆçš„æ¯ä¸ªç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°çš„æœ€å°å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # æ—¥å¿—è®°å½•ã€‚\n",
    "    print(f\"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚\")\n",
    "\n",
    "    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚\n",
    "        for feature_index in feature_indices:\n",
    "            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨åŸå§‹ç»“æœä¸Šåº”ç”¨åå¤„ç†é—®ç­”ç»“æœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9fcc5ebf7d246008c7ec68f5806182c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ `datasets.load_metric` ä¸­åŠ è½½ `SQuAD v2` çš„è¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20254/2330875496.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")\n",
      "/root/miniconda3/lib/python3.11/site-packages/datasets/load.py:752: FutureWarning: The repository for squad contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.16.1/metrics/squad/squad.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c528a0f9cb2c4787b5c6ee2342fa1519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f681da0b712486fad8521f4a7d532a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°è¿›è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "åªéœ€ç¨å¾®è°ƒæ•´ä¸€ä¸‹é¢„æµ‹å’Œæ ‡ç­¾çš„æ ¼å¼ï¼Œå› ä¸ºå®ƒæœŸæœ›çš„æ˜¯ä¸€ç³»åˆ—å­—å…¸è€Œä¸æ˜¯ä¸€ä¸ªå¤§å­—å…¸ã€‚\n",
    "\n",
    "åœ¨ä½¿ç”¨`squad_v2`æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¾ç½®`no_answer_probability`å‚æ•°ï¼ˆæˆ‘ä»¬åœ¨è¿™é‡Œå°†å…¶è®¾ç½®ä¸º0.0ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬é€‰æ‹©äº†ç­”æ¡ˆï¼Œæˆ‘ä»¬å·²ç»å°†ç­”æ¡ˆè®¾ç½®ä¸ºç©ºï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 74.88174077578051, 'f1': 83.6359321422016}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homeworkï¼šåŠ è½½æœ¬åœ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¿›è¡Œè¯„ä¼°å’Œå†è®­ç»ƒæ›´é«˜çš„ F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 4.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question Answering on SQUAD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
