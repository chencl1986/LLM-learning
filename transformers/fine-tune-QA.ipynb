{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers å¾®è°ƒè¯­è¨€æ¨¡å‹-é—®ç­”ä»»åŠ¡\n",
    "\n",
    "æˆ‘ä»¬å·²ç»å­¦ä¼šä½¿ç”¨ Pipeline åŠ è½½æ”¯æŒé—®ç­”ä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œæœ¬æ•™ç¨‹ä»£ç å°†å±•ç¤ºå¦‚ä½•å¾®è°ƒè®­ç»ƒä¸€ä¸ªæ”¯æŒé—®ç­”ä»»åŠ¡çš„æ¨¡å‹ã€‚\n",
    "\n",
    "**æ³¨æ„ï¼šå¾®è°ƒåçš„æ¨¡å‹ä»ç„¶æ˜¯é€šè¿‡æå–ä¸Šä¸‹æ–‡çš„å­ä¸²æ¥å›ç­”é—®é¢˜çš„ï¼Œè€Œä¸æ˜¯ç”Ÿæˆæ–°çš„æ–‡æœ¬ã€‚**\n",
    "\n",
    "### æ¨¡å‹æ‰§è¡Œé—®ç­”æ•ˆæœç¤ºä¾‹\n",
    "\n",
    "![Widget inference representing the QA task](docs/images/question_answering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "# æ ¹æ®ä½ ä½¿ç”¨çš„æ¨¡å‹å’ŒGPUèµ„æºæƒ…å†µï¼Œè°ƒæ•´ä»¥ä¸‹å…³é”®å‚æ•°\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## ä¸‹è½½æ•°æ®é›†\n",
    "\n",
    "åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuADï¼‰](https://rajpurkar.github.io/SQuAD-explorer/)ã€‚\n",
    "\n",
    "### SQuAD æ•°æ®é›†\n",
    "\n",
    "**æ–¯å¦ç¦é—®ç­”æ•°æ®é›†(SQuAD)** æ˜¯ä¸€ä¸ªé˜…è¯»ç†è§£æ•°æ®é›†ï¼Œç”±ä¼—åŒ…å·¥ä½œè€…åœ¨ä¸€ç³»åˆ—ç»´åŸºç™¾ç§‘æ–‡ç« ä¸Šæå‡ºé—®é¢˜ç»„æˆã€‚æ¯ä¸ªé—®é¢˜çš„ç­”æ¡ˆéƒ½æ˜¯ç›¸åº”é˜…è¯»æ®µè½ä¸­çš„æ–‡æœ¬ç‰‡æ®µæˆ–èŒƒå›´ï¼Œæˆ–è€…è¯¥é—®é¢˜å¯èƒ½æ— æ³•å›ç­”ã€‚\n",
    "\n",
    "SQuAD2.0å°†SQuAD1.1ä¸­çš„10ä¸‡ä¸ªé—®é¢˜ä¸ç”±ä¼—åŒ…å·¥ä½œè€…å¯¹æŠ—æ€§åœ°æ’°å†™çš„5ä¸‡å¤šä¸ªæ— æ³•å›ç­”çš„é—®é¢˜ç›¸ç»“åˆï¼Œä½¿å…¶çœ‹èµ·æ¥ä¸å¯å›ç­”çš„é—®é¢˜ç±»ä¼¼ã€‚è¦åœ¨SQuAD2.0ä¸Šè¡¨ç°è‰¯å¥½ï¼Œç³»ç»Ÿä¸ä»…å¿…é¡»åœ¨å¯èƒ½æ—¶å›ç­”é—®é¢˜ï¼Œè¿˜å¿…é¡»ç¡®å®šæ®µè½ä¸­æ²¡æœ‰æ”¯æŒä»»ä½•ç­”æ¡ˆï¼Œå¹¶æ”¾å¼ƒå›ç­”ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a22392259b3422282cb387d26800370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e369a7bfc8d541eeaad5a9cbf46cefaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544572b98e14405c94d7cc62f7739fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f4e629cf85422a9f86aa2d246e876a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e5e192efa04d4abbc59db258adae83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¯¹æ¯”æ•°æ®é›†\n",
    "\n",
    "ç›¸æ¯”å¿«é€Ÿå…¥é—¨ä½¿ç”¨çš„ Yelp è¯„è®ºæ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ° SQuAD è®­ç»ƒå’Œæµ‹è¯•é›†éƒ½æ–°å¢äº†ç”¨äºä¸Šä¸‹æ–‡ã€é—®é¢˜ä»¥åŠé—®é¢˜ç­”æ¡ˆçš„åˆ—ï¼š\n",
    "\n",
    "**YelpReviewFull Datasetï¼š**\n",
    "\n",
    "```json\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 650000\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 50000\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56d443ef2ccc5a1400d830db',\n",
       " 'title': 'BeyoncÃ©',\n",
       " 'context': 'BeyoncÃ© attended St. Mary\\'s Elementary School in Fredericksburg, Texas, where she enrolled in dance classes. Her singing talent was discovered when dance instructor Darlette Johnson began humming a song and she finished it, able to hit the high-pitched notes. BeyoncÃ©\\'s interest in music and performing continued after winning a school talent show at age seven, singing John Lennon\\'s \"Imagine\" to beat 15/16-year-olds. In fall of 1990, BeyoncÃ© enrolled in Parker Elementary School, a music magnet school in Houston, where she would perform with the school\\'s choir. She also attended the High School for the Performing and Visual Arts and later Alief Elsik High School. BeyoncÃ© was also a member of the choir at St. John\\'s United Methodist Church as a soloist for two years.',\n",
       " 'question': \"What city was BeyoncÃ©'s elementary school located in?\",\n",
       " 'answers': {'text': ['Fredericksburg'], 'answer_start': [49]}}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä»ä¸Šä¸‹æ–‡ä¸­ç»„ç»‡å›å¤å†…å®¹\n",
    "\n",
    "æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ç­”æ¡ˆæ˜¯é€šè¿‡å®ƒä»¬åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®ï¼ˆè¿™é‡Œæ˜¯ç¬¬515ä¸ªå­—ç¬¦ï¼‰ä»¥åŠå®ƒä»¬çš„å®Œæ•´æ–‡æœ¬è¡¨ç¤ºçš„ï¼Œè¿™æ˜¯ä¸Šé¢æåˆ°çš„ä¸Šä¸‹æ–‡çš„å­å­—ç¬¦ä¸²ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5731ab21b9d445190005e44f</td>\n",
       "      <td>Religion_in_ancient_Rome</td>\n",
       "      <td>The meaning and origin of many archaic festivals baffled even Rome's intellectual elite, but the more obscure they were, the greater the opportunity for reinvention and reinterpretation â€” a fact lost neither on Augustus in his program of religious reform, which often cloaked autocratic innovation, nor on his only rival as mythmaker of the era, Ovid. In his Fasti, a long-form poem covering Roman holidays from January to June, Ovid presents a unique look at Roman antiquarian lore, popular customs, and religious practice that is by turns imaginative, entertaining, high-minded, and scurrilous; not a priestly account, despite the speaker's pose as a vates or inspired poet-prophet, but a work of description, imagination and poetic etymology that reflects the broad humor and burlesque spirit of such venerable festivals as the Saturnalia, Consualia, and feast of Anna Perenna on the Ides of March, where Ovid treats the assassination of the newly deified Julius Caesar as utterly incidental to the festivities among the Roman people. But official calendars preserved from different times and places also show a flexibility in omitting or expanding events, indicating that there was no single static and authoritative calendar of required observances. In the later Empire under Christian rule, the new Christian festivals were incorporated into the existing framework of the Roman calendar, alongside at least some of the traditional festivals.</td>\n",
       "      <td>What poet wrote a long poem describing Roman religious holidays?</td>\n",
       "      <td>{'text': ['Ovid'], 'answer_start': [346]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56e08b457aa994140058e5e3</td>\n",
       "      <td>Hydrogen</td>\n",
       "      <td>Hydrogen forms a vast array of compounds with carbon called the hydrocarbons, and an even vaster array with heteroatoms that, because of their general association with living things, are called organic compounds. The study of their properties is known as organic chemistry and their study in the context of living organisms is known as biochemistry. By some definitions, \"organic\" compounds are only required to contain carbon. However, most of them also contain hydrogen, and because it is the carbon-hydrogen bond which gives this class of compounds most of its particular chemical characteristics, carbon-hydrogen bonds are required in some definitions of the word \"organic\" in chemistry. Millions of hydrocarbons are known, and they are usually formed by complicated synthetic pathways, which seldom involve elementary hydrogen.</td>\n",
       "      <td>What is the form of hydrogen and carbon called?</td>\n",
       "      <td>{'text': ['hydrocarbons'], 'answer_start': [64]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56cef65baab44d1400b88d36</td>\n",
       "      <td>Spectre_(2015_film)</td>\n",
       "      <td>Christopher Orr, writing in The Atlantic, also criticised the film, saying that Spectre \"backslides on virtually every [aspect]\". Lawrence Toppman of The Charlotte Observer called Craig's performance \"Bored, James Bored.\" Alyssa Rosenberg, writing for The Washington Post, stated that the film turned into \"a disappointingly conventional Bond film.\"</td>\n",
       "      <td>What adjective did Lawrence Toppman use to describe Craig's portrayal of James Bond?</td>\n",
       "      <td>{'text': ['Bored'], 'answer_start': [201]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>571a30bb10f8ca1400304f53</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>King County Metro provides frequent stop bus service within the city and surrounding county, as well as a South Lake Union Streetcar line between the South Lake Union neighborhood and Westlake Center in downtown. Seattle is one of the few cities in North America whose bus fleet includes electric trolleybuses. Sound Transit currently provides an express bus service within the metropolitan area; two Sounder commuter rail lines between the suburbs and downtown; its Central Link light rail line, which opened in 2009, between downtown and Sea-Tac Airport gives the city its first rapid transit line that has intermediate stops within the city limits. Washington State Ferries, which manages the largest network of ferries in the United States and third largest in the world, connects Seattle to Bainbridge and Vashon Islands in Puget Sound and to Bremerton and Southworth on the Kitsap Peninsula.</td>\n",
       "      <td>To what two islands does the ferry service connect?</td>\n",
       "      <td>{'text': ['Bainbridge and Vashon'], 'answer_start': [796]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570d2cb4fed7b91900d45cb5</td>\n",
       "      <td>Macintosh</td>\n",
       "      <td>In 1998, after the return of Steve Jobs, Apple consolidated its multiple consumer-level desktop models into the all-in-one iMac G3, which became a commercial success and revitalized the brand. Since their transition to Intel processors in 2006, the complete lineup is entirely based on said processors and associated systems. Its current lineup comprises three desktops (the all-in-one iMac, entry-level Mac mini, and the Mac Pro tower graphics workstation), and four laptops (the MacBook, MacBook Air, MacBook Pro, and MacBook Pro with Retina display). Its Xserve server was discontinued in 2011 in favor of the Mac Mini and Mac Pro.</td>\n",
       "      <td>What took the place of Mac's Xserve server?</td>\n",
       "      <td>{'text': ['Mac Mini and Mac Pro'], 'answer_start': [613]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570af6876b8089140040f646</td>\n",
       "      <td>Videoconferencing</td>\n",
       "      <td>Technological developments by videoconferencing developers in the 2010s have extended the capabilities of video conferencing systems beyond the boardroom for use with hand-held mobile devices that combine the use of video, audio and on-screen drawing capabilities broadcasting in real-time over secure networks, independent of location. Mobile collaboration systems now allow multiple people in previously unreachable locations, such as workers on an off-shore oil rig, the ability to view and discuss issues with colleagues thousands of miles away. Traditional videoconferencing system manufacturers have begun providing mobile applications as well, such as those that allow for live and still image streaming.</td>\n",
       "      <td>What is one example of an application that videoconferencing manufacturers have begun to offer?</td>\n",
       "      <td>{'text': ['still image streaming'], 'answer_start': [689]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56e82d0100c9c71400d775eb</td>\n",
       "      <td>Dialect</td>\n",
       "      <td>Italy is home to a vast array of native regional minority languages, most of which are Romance-based and have their own local variants. These regional languages are often referred to colloquially or in non-linguistic circles as Italian \"dialects,\" or dialetti (standard Italian for \"dialects\"). However, the majority of the regional languages in Italy are in fact not actually \"dialects\" of standard Italian in the strict linguistic sense, as they are not derived from modern standard Italian but instead evolved locally from Vulgar Latin independent of standard Italian, with little to no influence from what is now known as \"standard Italian.\" They are therefore better classified as individual languages rather than \"dialects.\"</td>\n",
       "      <td>What are Italian dialects termed in the Italian language?</td>\n",
       "      <td>{'text': ['dialetti'], 'answer_start': [251]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56e147e6cd28a01900c6772b</td>\n",
       "      <td>Universal_Studios</td>\n",
       "      <td>The Universal Film Manufacturing Company was incorporated in New York on April 30, 1912. Laemmle, who emerged as president in July 1912, was the primary figure in the partnership with Dintenfass, Baumann, Kessel, Powers, Swanson, Horsley, and Brulatour. Eventually all would be bought out by Laemmle. The new Universal studio was a vertically integrated company, with movie production, distribution and exhibition venues all linked in the same corporate entity, the central element of the Studio system era.</td>\n",
       "      <td>Along with exhibition and distribution, what business did the Universal Film Manufacturing Company engage in?</td>\n",
       "      <td>{'text': ['movie production'], 'answer_start': [368]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5731933a05b4da19006bd2d0</td>\n",
       "      <td>Steven_Spielberg</td>\n",
       "      <td>Spielberg's next film, Schindler's List, was based on the true story of Oskar Schindler, a man who risked his life to save 1,100 Jews from the Holocaust. Schindler's List earned Spielberg his first Academy Award for Best Director (it also won Best Picture). With the film a huge success at the box office, Spielberg used the profits to set up the Shoah Foundation, a non-profit organization that archives filmed testimony of Holocaust survivors. In 1997, the American Film Institute listed it among the 10 Greatest American Films ever Made (#9) which moved up to (#8) when the list was remade in 2007.</td>\n",
       "      <td>Whose life was 'Schindler's List' based on?</td>\n",
       "      <td>{'text': ['Oskar Schindler'], 'answer_start': [72]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56de93f94396321400ee2a36</td>\n",
       "      <td>Arnold_Schwarzenegger</td>\n",
       "      <td>In 1985, Schwarzenegger appeared in \"Stop the Madness\", an anti-drug music video sponsored by the Reagan administration. He first came to wide public notice as a Republican during the 1988 presidential election, accompanying then-Vice President George H.W. Bush at a campaign rally.</td>\n",
       "      <td>In what presidential election year did Schwarzenegger make a name for himself as a prominent Republican?</td>\n",
       "      <td>{'text': ['1988'], 'answer_start': [184]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## é¢„å¤„ç†æ•°æ®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizerï¼ˆåˆ†è¯å™¨ï¼‰å°±åƒä¸€ä½ã€Œè¯­è¨€æ‹†è§£ä¸“å®¶ã€**ï¼Œä¸“é—¨å¸®è®¡ç®—æœºç†è§£äººç±»æ–‡å­—ã€‚å®ƒçš„æ ¸å¿ƒä½œç”¨å¯ä»¥ç”¨ä¸‰æ­¥è¯´æ¸…æ¥šï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ **æ‹†è§£æ–‡æœ¬**  \n",
    "æŠŠå¥å­æ‹†æˆ **æ¨¡å‹è®¤è¯†çš„ç‰‡æ®µ**ï¼ˆè¯æˆ–å­è¯ï¼‰ã€‚  \n",
    "ä¾‹å¦‚ï¼š  \n",
    "`\"æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†\"` â†’ `[\"æˆ‘\", \"çˆ±\", \"è‡ªç„¶\", \"è¯­è¨€\", \"å¤„ç†\"]`  \n",
    "ï¼ˆè‹±æ–‡å¦‚ `\"Hugging Face\"` â†’ `[\"Hug\", \"##ging\", \"Face\"]`ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ **æ·»åŠ ã€Œæš—å·ã€**  \n",
    "æ’å…¥æ¨¡å‹éœ€è¦çš„**ç‰¹æ®Šæ ‡è®°**ï¼Œæ¯”å¦‚ï¼š  \n",
    "- **`[CLS]`**ï¼šå¼€å¤´æ ‡è®°ï¼ˆBERTç”¨ï¼‰  \n",
    "- **`[SEP]`**ï¼šåˆ†éš”æ ‡è®°ï¼ˆåŒºåˆ†å¥å­ï¼‰  \n",
    "```python\n",
    "\"ä½ å¥½å—ï¼Ÿ\" â†’ [\"[CLS]\", \"ä½ \", \"å¥½\", \"å—\", \"ï¼Ÿ\", \"[SEP]\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ **è½¬æˆå¯†ç æ•°å­—**  \n",
    "æŠŠæ¯ä¸ªè¯æ¢æˆ**æ¨¡å‹è¯æ±‡è¡¨é‡Œçš„IDå·**ï¼Œç±»ä¼¼å¯†ç æœ¬ï¼š  \n",
    "```python\n",
    "[\"[CLS]\", \"ä½ \", \"å¥½\", \"å—\"] â†’ [101, 872, 1962, 3221, 102]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒ° **å®é™…æ•ˆæœç¤ºä¾‹**  \n",
    "ä½ è¾“å…¥ï¼š`\"ä»Šå¤©å¦é—¨å¤©æ°”å¦‚ä½•ï¼Ÿ\"`  \n",
    "Tokenizerå¤„ç†åè¾“å‡ºï¼š  \n",
    "```python\n",
    "{\n",
    "  \"input_ids\": [101, 791, 1921, 1762, 1377, 1442, 3221, 102],\n",
    "  \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1]  # æ ‡è®°å“ªäº›æ˜¯æœ‰æ•ˆå†…å®¹\n",
    "}\n",
    "```\n",
    "æ¨¡å‹çœ‹åˆ°è¿™äº›æ•°å­—å°±èƒ½åˆ†æè¯­ä¹‰ï¼Œç”Ÿæˆå›ç­”å•¦ï¼\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤– **ä¸åŒæ¨¡å‹çš„å·®å¼‚**  \n",
    "- **BERTç±»**ï¼šæ‹†è¯è¾ƒç»†ï¼ŒåŠ `[CLS]`/`[SEP]`  \n",
    "- **GPTç±»**ï¼šæŒ‰å­—èŠ‚æ‹†åˆ†ï¼ŒåŠ `<|endoftext|>`  \n",
    "- **å¤šè¯­è¨€æ¨¡å‹**ï¼šæ”¯æŒä¸­/è‹±/æ—¥ç­‰æ··åˆæ‹†åˆ†  \n",
    "\n",
    "ä¸€å¥è¯æ€»ç»“ï¼š**Tokenizerå°±æ˜¯æŠŠäººç±»è¯­è¨€ã€Œç¿»è¯‘ã€æˆAIèƒ½æ‡‚çš„æ•°å­—å¯†ç ï¼** ğŸ˜Š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AutoTokenizer å°±åƒã€Œä¸‡èƒ½é€‚é…å™¨ã€**  \n",
    "â€”â€”ä½ åªéœ€è¦å‘Šè¯‰å®ƒç”¨å“ªä¸ªAIæ¨¡å‹ï¼ˆæ¯”å¦‚BERTã€GPT-3ï¼‰ï¼Œå®ƒå°±ä¼šè‡ªåŠ¨åŒ¹é…å¯¹åº”çš„æ–‡å­—ç¿»è¯‘è§„åˆ™ã€‚\n",
    "\n",
    "ä¸¾ä¸ªæ —å­ğŸŒ°ï¼š  \n",
    "- ä½ æƒ³ç”¨ **BERT** æ¨¡å‹ â†’ å®ƒè‡ªåŠ¨åŠ è½½BERTçš„åˆ†è¯è§„åˆ™  \n",
    "  ```python\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "  ```\n",
    "- ä½ æƒ³ç”¨ **GPT** æ¨¡å‹ â†’ å®ƒè‡ªåŠ¨åˆ‡æ¢æˆGPTçš„åˆ†è¯æ–¹å¼  \n",
    "  ```python\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "  ```\n",
    "\n",
    "**å¥½å¤„**ï¼šä¸ç”¨è®°ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨åå­—ï¼ˆæ¯”å¦‚`BertTokenizer`ã€`GPT2Tokenizer`ï¼‰ï¼Œä¸€ä¸ª`AutoTokenizer`é€šåƒæ‰€æœ‰æ¨¡å‹ï¼Œå°±åƒä¸‡èƒ½å……ç”µå™¨ä¸€æ ·æ–¹ä¾¿ï¼\n",
    "\n",
    "---\n",
    "\n",
    "### å¯¹æ¯”ç¤ºä¾‹ï¼ˆæ‰‹åŠ¨ vs è‡ªåŠ¨ï¼‰\n",
    "| æ–¹å¼          | æ‰‹åŠ¨é€‰æ‹©åˆ†è¯å™¨                   | AutoTokenizer                  |\n",
    "|---------------|----------------------------------|---------------------------------|\n",
    "| **BERTæ¨¡å‹**  | `from transformers import BertTokenizer`<br>`tokenizer = BertTokenizer.from_pretrained(\"bert-base\")` | `AutoTokenizer.from_pretrained(\"bert-base\")` |\n",
    "| **GPTæ¨¡å‹**   | `from transformers import GPT2Tokenizer`<br>`tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")` | `AutoTokenizer.from_pretrained(\"gpt2\")` |\n",
    "\n",
    "---\n",
    "\n",
    "âš ï¸ **æ³¨æ„**ï¼šåå­—è¦å¯¹ï¼ˆæ¯”å¦‚`bert-base-chinese`ä¸èƒ½å†™æˆ`bert-chinese`ï¼‰ï¼Œå¦åˆ™è¿™ä¸ªä¸‡èƒ½å……ç”µå™¨ä¹Ÿä¼šæ‰¾ä¸åˆ°æ’å£~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "ä»¥ä¸‹æ–­è¨€ç¡®ä¿æˆ‘ä»¬çš„ Tokenizers ä½¿ç”¨çš„æ˜¯ FastTokenizerï¼ˆRust å®ç°ï¼Œé€Ÿåº¦å’ŒåŠŸèƒ½æ€§ä¸Šæœ‰ä¸€å®šä¼˜åŠ¿ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ˜¯å¦æ˜¯å¿«é€Ÿç‰ˆåˆ†è¯å™¨: True\n",
      "âœ… Tokenizer æ˜¯å¿«é€Ÿç‰ˆ (PreTrainedTokenizerFast)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "# ç›´æ¥æ‰“å°åˆ¤æ–­ç»“æœ\n",
    "print(\"æ˜¯å¦æ˜¯å¿«é€Ÿç‰ˆåˆ†è¯å™¨:\", isinstance(tokenizer, transformers.PreTrainedTokenizerFast))\n",
    "\n",
    "# æˆ–æ›´è¯¦ç»†çš„è¾“å‡º\n",
    "if isinstance(tokenizer, transformers.PreTrainedTokenizerFast):\n",
    "    print(\"âœ… Tokenizer æ˜¯å¿«é€Ÿç‰ˆ (PreTrainedTokenizerFast)\")\n",
    "else:\n",
    "    print(\"âŒ Tokenizer æ˜¯æ™®é€šç‰ˆ (PreTrainedTokenizer)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PreTrainedTokenizer å°±åƒä¸ªã€Œæ–‡å­—ç¿»è¯‘å®˜ã€**ï¼Œä¸“é—¨å¸® AI æ¨¡å‹å’Œäººç±»æ–‡å­—æ‰“äº¤é“ã€‚  \n",
    "\n",
    "ä¸¾ä¸ªæ —å­ğŸŒ°ï¼š  \n",
    "ä½ æƒ³é—® AI \"å¦é—¨ä»Šå¤©çƒ­å—ï¼Ÿ\"  \n",
    "â¡ï¸ **ç¿»è¯‘å®˜çš„å·¥ä½œ**ï¼š  \n",
    "1. æŠŠè¿™å¥è¯åˆ‡æˆå°å—ï¼š`[\"å¦é—¨\", \"ä»Šå¤©\", \"çƒ­\", \"å—\"]`  \n",
    "2. å·å·åŠ æš—å·ï¼š`[å¼€å¤´æš—å·] å¦é—¨ ä»Šå¤© çƒ­ å— [ç»“å°¾æš—å·]`  \n",
    "3. è½¬æˆå¯†ç æ•°å­—ï¼š`[101, 2345, 567, 8910, 102]`  \n",
    "\n",
    "ç„¶å AI å°±èƒ½çœ‹æ‡‚è¿™äº›æ•°å­—å¯†ç ï¼Œç»™å‡ºå›ç­”å•¦ï¼  \n",
    "ï¼ˆåè¿‡æ¥ä¹Ÿä¼šæŠŠ AI çš„æ•°å­—å¯†ç ç¿»è¯‘æˆäººç±»æ–‡å­—ç»™ä½ çœ‹ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)`\n",
    "\n",
    "è¿™ä¸ªæ­¥éª¤æ˜¯**å¯é€‰çš„å®‰å…¨æ£€æŸ¥**ï¼Œä¸»è¦ä¸ºäº†ç¡®ä¿ä½ åŠ è½½çš„æ˜¯**å¿«é€Ÿç‰ˆåˆ†è¯å™¨ï¼ˆPreTrainedTokenizerFastï¼‰**ï¼Œè€Œä¸æ˜¯æ—§ç‰ˆçš„æ…¢é€Ÿåˆ†è¯å™¨ï¼ˆPreTrainedTokenizerï¼‰ã€‚ä¸æ£€æŸ¥ä¹Ÿèƒ½è¿è¡Œï¼Œä½†å¯èƒ½ä¼šé‡åˆ°ä»¥ä¸‹é—®é¢˜ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¤” **ä¸ºä»€ä¹ˆè¦åŒºåˆ† Fast å’Œæ™®é€šç‰ˆï¼Ÿ**\n",
    "| ç‰¹æ€§                | PreTrainedTokenizerFastï¼ˆå¿«é€Ÿç‰ˆï¼‰          | PreTrainedTokenizerï¼ˆæ™®é€šç‰ˆï¼‰       |\n",
    "|---------------------|--------------------------------------------|-------------------------------------|\n",
    "| **åº•å±‚å®ç°**         | Rustè¯­è¨€ç¼–å†™ï¼ˆé€Ÿåº¦å¿«ï¼‰                     | Pythonå®ç°ï¼ˆé€Ÿåº¦æ…¢ï¼‰                 |\n",
    "| **æ‰¹å¤„ç†æ”¯æŒ**       | âœ… åŸç”Ÿæ”¯æŒï¼ˆå¦‚`batch_encode_plus`ï¼‰        | âŒ éœ€æ‰‹åŠ¨å¾ªç¯å¤„ç†                    |\n",
    "| **ç‰¹æ®Šæ ‡è®°å¤„ç†**     | è‡ªåŠ¨ç®¡ç†ï¼ˆå¦‚å¡«å……ã€æˆªæ–­ï¼‰                   | éœ€æ‰‹åŠ¨é…ç½®                          |\n",
    "| **å…¸å‹åœºæ™¯**         | ç”Ÿäº§ç¯å¢ƒã€å¤§æ•°æ®å¤„ç†                        | æ•™å­¦æˆ–å…¼å®¹æ—§ä»£ç                      |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¥ **ä¸æ£€æŸ¥å¯èƒ½å¸¦æ¥çš„é—®é¢˜**\n",
    "1. **æ€§èƒ½ä¸‹é™**ï¼šå¤„ç†1000æ¡æ–‡æœ¬æ—¶ï¼Œå¿«é€Ÿç‰ˆå¯èƒ½æ¯”æ™®é€šç‰ˆå¿«**5-10å€**ã€‚\n",
    "2. **åŠŸèƒ½ç¼ºå¤±**ï¼šæ™®é€šç‰ˆå¯èƒ½ç¼ºå°‘æŸäº›APIï¼ˆå¦‚`decode`çš„`skip_special_tokens`å‚æ•°ï¼‰ã€‚\n",
    "3. **æ„å¤–é”™è¯¯**ï¼šæŸäº›åº“ï¼ˆå¦‚Datasetsï¼‰é»˜è®¤è¦æ±‚å¿«é€Ÿç‰ˆåˆ†è¯å™¨ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸŒ° **å®é™…æ¡ˆä¾‹**\n",
    "å‡è®¾ä½ çš„`model_checkpoint`æ„å¤–æŒ‡å‘äº†ä¸€ä¸ªæ²¡æœ‰å¿«é€Ÿç‰ˆçš„æ¨¡å‹ï¼š\n",
    "```python\n",
    "model_checkpoint = \"some-old-model\"  # å‡è®¾è¯¥æ¨¡å‹åªæœ‰æ™®é€šç‰ˆåˆ†è¯å™¨\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# æ­¤æ—¶ tokenizer æ˜¯ PreTrainedTokenizer è€Œé Fast ç‰ˆ\n",
    "# åç»­è°ƒç”¨ batch_encode_plus å¯èƒ½æŠ¥é”™ï¼\n",
    "```\n",
    "\n",
    "é€šè¿‡`assert`æ£€æŸ¥ï¼Œå¯ä»¥**æå‰å‘ç°é—®é¢˜**ï¼Œé¿å…åç»­ä»£ç å´©æºƒã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”§ **æ›¿ä»£æ–¹æ¡ˆï¼ˆå¦‚æœä¸åšæ–­è¨€ï¼‰**\n",
    "1. **ç›´æ¥ä½¿ç”¨**ï¼šå¦‚æœç¡®å®šæ¨¡å‹æœ‰å¿«é€Ÿç‰ˆï¼Œå¯ä»¥è·³è¿‡æ£€æŸ¥ã€‚\n",
    "2. **é™çº§å¤„ç†**ï¼šæ•è·å¼‚å¸¸å¹¶æ”¹ç”¨æ™®é€šç‰ˆé€»è¾‘ï¼š\n",
    "```python\n",
    "if not isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "    print(\"è­¦å‘Šï¼šä½¿ç”¨æ…¢é€Ÿåˆ†è¯å™¨ï¼Œæ€§èƒ½å¯èƒ½å—å½±å“ï¼\")\n",
    "    # æ‰‹åŠ¨å¤„ç†æ™®é€šç‰ˆçš„é™åˆ¶\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "æ€»ç»“ï¼šè¿™ä¸ªæ–­è¨€æ˜¯**é˜²å¾¡æ€§ç¼–ç¨‹**çš„ä½“ç°ï¼Œç¡®ä¿ä»£ç åœ¨æ€§èƒ½å’ŒåŠŸèƒ½ä¸ŠæŒ‰é¢„æœŸè¿è¡Œã€‚å¯¹äºå…³é”®é¡¹ç›®å»ºè®®ä¿ç•™ï¼Œä¸ªäººå®éªŒå¯è·³è¿‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‚¨å¯ä»¥åœ¨å¤§æ¨¡å‹è¡¨ä¸ŠæŸ¥çœ‹å“ªç§ç±»å‹çš„æ¨¡å‹å…·æœ‰å¯ç”¨çš„å¿«é€Ÿæ ‡è®°å™¨ï¼Œå“ªç§ç±»å‹æ²¡æœ‰ã€‚\n",
    "\n",
    "æ‚¨å¯ä»¥ç›´æ¥åœ¨ä¸¤ä¸ªå¥å­ä¸Šè°ƒç”¨æ­¤æ ‡è®°å™¨ï¼ˆä¸€ä¸ªç”¨äºç­”æ¡ˆï¼Œä¸€ä¸ªç”¨äºä¸Šä¸‹æ–‡ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2129, 2024, 2017, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenizer(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer è¿›é˜¶æ“ä½œ\n",
    "\n",
    "åœ¨é—®ç­”é¢„å¤„ç†ä¸­çš„ä¸€ä¸ªç‰¹å®šé—®é¢˜æ˜¯å¦‚ä½•å¤„ç†éå¸¸é•¿çš„æ–‡æ¡£ã€‚\n",
    "\n",
    "åœ¨å…¶ä»–ä»»åŠ¡ä¸­ï¼Œå½“æ–‡æ¡£çš„é•¿åº¦è¶…è¿‡æ¨¡å‹æœ€å¤§å¥å­é•¿åº¦æ—¶ï¼Œæˆ‘ä»¬é€šå¸¸ä¼šæˆªæ–­å®ƒä»¬ï¼Œä½†åœ¨è¿™é‡Œï¼Œåˆ é™¤ä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†å¯èƒ½ä¼šå¯¼è‡´æˆ‘ä»¬ä¸¢å¤±æ­£åœ¨å¯»æ‰¾çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å…è®¸æ•°æ®é›†ä¸­çš„ä¸€ä¸ªï¼ˆé•¿ï¼‰ç¤ºä¾‹ç”Ÿæˆå¤šä¸ªè¾“å…¥ç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„é•¿åº¦éƒ½å°äºæ¨¡å‹çš„æœ€å¤§é•¿åº¦ï¼ˆæˆ–æˆ‘ä»¬è®¾ç½®çš„è¶…å‚æ•°ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **ä¸ºä½•è®¾ç½® `max_length=384`ï¼Ÿ**\n",
    "1. **æ¨¡å‹é™åˆ¶**  \n",
    "   BERTç­‰æ¨¡å‹æœ€å¤§æ”¯æŒ **512 tokens**ï¼Œéœ€ä¸ºä»¥ä¸‹å†…å®¹ç•™ç©ºé—´ï¼š  \n",
    "   - **é—®é¢˜æœ¬èº«**ï¼ˆçº¦20-30 tokensï¼‰  \n",
    "   - **ç‰¹æ®Šæ ‡è®°**ï¼ˆå¦‚ `[CLS]`ã€`[SEP]`ï¼Œå 3-5 tokensï¼‰  \n",
    "   - **ç­”æ¡ˆä½ç½®**ï¼ˆé¿å…è¢«æˆªæ–­ï¼‰\n",
    "\n",
    "2. **ç»éªŒæ¯”ä¾‹**  \n",
    "   å¯ç”¨ä¸Šä¸‹æ–‡é•¿åº¦ â‰ˆ æ€»é•¿çš„ **75%**ï¼ˆ512Ã—0.75â‰ˆ384ï¼‰ï¼Œå¹³è¡¡è¦†ç›–ç‡å’Œè®¡ç®—æ•ˆç‡ã€‚\n",
    "\n",
    "3. **åˆ†å—ä¼˜åŒ–**  \n",
    "   ç»“åˆ `doc_stride=128`ï¼ˆé‡å é‡ï¼‰ï¼Œç¡®ä¿ç­”æ¡ˆåœ¨è‡³å°‘ä¸€ä¸ªåˆ†å—ä¸­å®Œæ•´å‡ºç°ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **å®é™…æ¡ˆä¾‹**  \n",
    "- **è¾“å…¥**ï¼šé—®é¢˜ï¼ˆ20 tokensï¼‰+ ä¸Šä¸‹æ–‡ï¼ˆ500 tokensï¼‰  \n",
    "- **å¤„ç†**ï¼š  \n",
    "  1. åˆ†å—1ï¼šé—®é¢˜ + ä¸Šä¸‹æ–‡0-363  \n",
    "  2. åˆ†å—2ï¼šé—®é¢˜ + ä¸Šä¸‹æ–‡236-500ï¼ˆä¸åˆ†å—1é‡å 128 tokensï¼‰  \n",
    "- **ç»“æœ**ï¼šå³ä½¿ç­”æ¡ˆåœ¨360-400åŒºé—´ï¼Œä¹Ÿèƒ½è¢«åˆ†å—2è¦†ç›–ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **è°ƒæ•´å»ºè®®**\n",
    "- **çŸ­æ–‡æœ¬ä»»åŠ¡**ï¼šç›´æ¥è®¾ä¸º512  \n",
    "- **è¶…é•¿æ–‡æ¡£**ï¼šå¯é™ä½åˆ°256ï¼ˆéœ€æ›´å¤šåˆ†å—ï¼‰  \n",
    "- **æ”¯æŒæ›´é•¿æ¨¡å‹**ï¼šå¦‚æ”¯æŒ1024ï¼Œå¯è®¾ä¸º768  \n",
    "\n",
    "ä¸€å¥è¯æ€»ç»“ï¼š**384æ˜¯å¹³è¡¡æ¨¡å‹é™åˆ¶ã€ç­”æ¡ˆå®Œæ•´æ€§å’Œè®¡ç®—æ•ˆç‡çš„ç»éªŒå€¼ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_stride=128` çš„åŸç†ä¸ `max_length=384` ç±»ä¼¼ï¼Œä½†å…³æ³¨ç‚¹ä¸åŒã€‚ä»¥ä¸‹æ˜¯ç®€æ´æ¸…æ™°çš„è§£é‡Šï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### **ä¸ºä½•è®¾ç½® `doc_stride=128`ï¼Ÿ**\n",
    "1. **æ ¸å¿ƒç›®çš„**  \n",
    "   **é¿å…ç­”æ¡ˆè¢«åˆ‡å‰²åœ¨åˆ†å—è¾¹ç•Œ**ã€‚é€šè¿‡è®¾ç½®åˆ†å—é—´çš„é‡å åŒºåŸŸï¼Œç¡®ä¿å³ä½¿ç­”æ¡ˆä½äºåˆ†å—è¾¹ç¼˜ï¼Œä¹Ÿèƒ½è¢«è‡³å°‘ä¸€ä¸ªå®Œæ•´åˆ†å—è¦†ç›–ã€‚\n",
    "\n",
    "2. **ç»éªŒå…¬å¼**  \n",
    "   `doc_stride` â‰ˆ `max_length` çš„ **1/3~1/4**ï¼ˆå¦‚ `384/3â‰ˆ128`ï¼‰ï¼Œå¹³è¡¡ï¼š\n",
    "   - **è®¡ç®—æ•ˆç‡**ï¼ˆåˆ†å—è¶Šå°‘è¶Šå¥½ï¼‰\n",
    "   - **ç­”æ¡ˆè¦†ç›–ç‡**ï¼ˆé‡å è¶Šå¤šè¶Šå®‰å…¨ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **åˆ†å—é€»è¾‘ç¤ºä¾‹**\n",
    "- **å‚æ•°**ï¼š\n",
    "  - `max_length=384`ï¼ˆæ€»é•¿åº¦ï¼‰\n",
    "  - é—®é¢˜é•¿åº¦ = 20 tokens\n",
    "  - å¯ç”¨ä¸Šä¸‹æ–‡é•¿åº¦ = `384 - 20 - 3ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰â‰ˆ 361 tokens`\n",
    "  - `doc_stride=128`\n",
    "- **åˆ†å—æ­¥é•¿** = `361 - 128 = 233 tokens`\n",
    "\n",
    "| åˆ†å— | èµ·å§‹ä½ç½® | ç»“æŸä½ç½® | è¦†ç›–çš„ä¸Šä¸‹æ–‡èŒƒå›´ |\n",
    "|------|----------|----------|------------------|\n",
    "| 1    | 0        | 360      | tokens 0-360     |\n",
    "| 2    | 233      | 593      | tokens 233-593   |\n",
    "| 3    | 466      | 826      | tokens 466-826   |\n",
    "\n",
    "- **å‡è®¾ç­”æ¡ˆåœ¨ tokens 350-370**ï¼š\n",
    "  - åˆ†å—1ï¼šè¦†ç›–åˆ°360 â†’ ç­”æ¡ˆéƒ¨åˆ†æˆªæ–­ï¼ˆ350-360ä¿ç•™ï¼‰\n",
    "  - åˆ†å—2ï¼šä»233å¼€å§‹ â†’ å®Œæ•´è¦†ç›–ç­”æ¡ˆï¼ˆ350-370ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **å…³é”®å½±å“**\n",
    "| `doc_stride` å€¼ | ä¼˜ç‚¹               | ç¼ºç‚¹                 |\n",
    "|-----------------|--------------------|----------------------|\n",
    "| **è¾ƒå°ï¼ˆå¦‚64ï¼‰** | ç­”æ¡ˆè¦†ç›–ç‡â†‘        | åˆ†å—æ•°é‡â†‘ï¼Œè®¡ç®—é‡â†‘   |\n",
    "| **è¾ƒå¤§ï¼ˆå¦‚192ï¼‰**| åˆ†å—æ•°é‡â†“ï¼Œé€Ÿåº¦â†‘   | æ¼ç­”é£é™©â†‘            |\n",
    "\n",
    "---\n",
    "\n",
    "### **è°ƒæ•´å»ºè®®**\n",
    "- **çŸ­ç­”æ¡ˆä»»åŠ¡**ï¼ˆå¦‚å®ä½“æŠ½å–ï¼‰ï¼š`doc_stride=64~128`\n",
    "- **é•¿ç­”æ¡ˆä»»åŠ¡**ï¼ˆå¦‚æ®µè½æ€»ç»“ï¼‰ï¼š`doc_stride=128~256`\n",
    "\n",
    "ä¸€å¥è¯æ€»ç»“ï¼š**`doc_stride=128` æ˜¯ç»éªŒæ€§å‚æ•°ï¼Œé€šè¿‡åˆ†å—é‡å å¹³è¡¡æ•ˆç‡ä¸ç­”æ¡ˆå®Œæ•´æ€§ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‡è®¾æˆ‘ä»¬æœ‰ä»¥ä¸‹å‚æ•°ï¼š\n",
    "- **`max_length = 10`**ï¼ˆæ¯ä¸ªç‰‡æ®µæœ€å¤šåŒ…å«10ä¸ªå­—ç¬¦ï¼‰\n",
    "- **`doc_stride = 4`**ï¼ˆç›¸é‚»ç‰‡æ®µé‡å 4ä¸ªå­—ç¬¦ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **åˆ‡å‰²è¿‡ç¨‹**\n",
    "åŸå§‹æ–‡æœ¬ï¼š`ABCDEFGHIJKLMN`ï¼ˆå‡è®¾æ¯ä¸ªå­—æ¯ä»£è¡¨ä¸€ä¸ªtokenï¼‰\n",
    "\n",
    "1. **ç¬¬ä¸€ä¸ªç‰‡æ®µ**ï¼š  \n",
    "   - å–å‰10ä¸ªå­—ç¬¦ â†’ `ABCDEFGHIJ`ï¼ˆAåˆ°Jï¼‰\n",
    "   - ç»“æŸä½ç½®ï¼šç¬¬10ä¸ªå­—ç¬¦ï¼ˆJï¼‰\n",
    "\n",
    "2. **ç¬¬äºŒä¸ªç‰‡æ®µ**ï¼š  \n",
    "   - èµ·å§‹ä½ç½® = å‰ç‰‡æ®µçš„èµ·å§‹ä½ç½® + (`max_length - doc_stride`) = 0 + (10 - 4) = 6  \n",
    "     ï¼ˆå³ä»ç¬¬7ä¸ªå­—ç¬¦å¼€å§‹ï¼Œå¯¹åº”å­—æ¯ `G`ï¼‰\n",
    "   - å®é™…å­—ç¬¦ï¼š`GHIJKLMN`ï¼ˆGåˆ°Nï¼Œå…±8ä¸ªå­—ç¬¦ï¼Œä¸è¶³10ä¸ªåˆ™ä¿ç•™ï¼‰\n",
    "   - é‡å éƒ¨åˆ†ï¼š`GHIJ`ï¼ˆä¸å‰ä¸€ç‰‡æ®µçš„å4ä¸ªå­—ç¬¦é‡å ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **å›¾ç¤ºåˆ‡å‰²æ•ˆæœ**\n",
    "```\n",
    "åŸå§‹æ–‡æœ¬ï¼š A B C D E F G H I J K L M N\n",
    "ç‰‡æ®µ1ï¼š    [A B C D E F G H I J]          â†’ é•¿åº¦10\n",
    "ç‰‡æ®µ2ï¼š            [G H I J K L M N]      â†’ èµ·å§‹ä½ç½®6ï¼Œé‡å 4ä¸ªå­—ç¬¦\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **ä¸ºä»€ä¹ˆéœ€è¦é‡å ï¼Ÿ**\n",
    "å‡è®¾ç­”æ¡ˆåœ¨ `H I J K` åŒºåŸŸï¼š\n",
    "- **æ— é‡å **ï¼šå¯èƒ½è¢«æˆªæ–­åœ¨ç‰‡æ®µ1æœ«å°¾æˆ–ç‰‡æ®µ2å¼€å¤´\n",
    "- **æœ‰é‡å **ï¼šç¡®ä¿ç­”æ¡ˆå®Œæ•´åŒ…å«åœ¨è‡³å°‘ä¸€ä¸ªç‰‡æ®µä¸­\n",
    "\n",
    "---\n",
    "\n",
    "### **å®é™…é—®ç­”ä¸­çš„å‚æ•°**\n",
    "å½“ `max_length=384` ä¸” `doc_stride=128` æ—¶ï¼Œé€»è¾‘å®Œå…¨ä¸€è‡´ï¼Œåªæ˜¯æ•°å€¼æ›´å¤§ã€‚è¿™ç§æ»‘åŠ¨çª—å£åˆ‡å‰²æ˜¯å¤„ç†é•¿æ–‡æœ¬é—®ç­”çš„å¸¸ç”¨ç­–ç•¥ï¼ ğŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### è¶…å‡ºæœ€å¤§é•¿åº¦çš„æ–‡æœ¬æ•°æ®å¤„ç†\n",
    "\n",
    "ä¸‹é¢ï¼Œæˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­æ‰¾å‡ºä¸€ä¸ªè¶…è¿‡æœ€å¤§é•¿åº¦ï¼ˆ384ï¼‰çš„æ–‡æœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "# æŒ‘é€‰å‡ºæ¥è¶…è¿‡384ï¼ˆæœ€å¤§é•¿åº¦ï¼‰çš„æ•°æ®æ ·ä¾‹\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### æˆªæ–­ä¸Šä¸‹æ–‡ä¸ä¿ç•™è¶…å‡ºéƒ¨åˆ†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"],\n",
    "              example[\"context\"],\n",
    "              max_length=max_length,\n",
    "              truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…³äºæˆªæ–­çš„ç­–ç•¥\n",
    "\n",
    "- ç›´æ¥æˆªæ–­è¶…å‡ºéƒ¨åˆ†: truncation=`only_second`\n",
    "- ä»…æˆªæ–­ä¸Šä¸‹æ–‡ï¼ˆcontextï¼‰ï¼Œä¿ç•™é—®é¢˜ï¼ˆquestionï¼‰ï¼š`return_overflowing_tokens=True` & è®¾ç½®`stride`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨æ­¤ç­–ç•¥æˆªæ–­åï¼ŒTokenizer å°†è¿”å›å¤šä¸ª `input_ids` åˆ—è¡¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 157]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è§£ç ä¸¤ä¸ªè¾“å…¥ç‰¹å¾ï¼Œå¯ä»¥çœ‹åˆ°é‡å çš„éƒ¨åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 â€“ 2010 season. the team is coached by mike brey, who, as of the 2014 â€“ 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 â€“ 11 team concluded its regular season ranked number seven in the country, with a record of 25 â€“ 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ä½¿ç”¨ offsets_mapping è·å–åŸå§‹çš„ input_ids\n",
    "\n",
    "è®¾ç½® `return_offsets_mapping=True`ï¼Œå°†ä½¿å¾—æˆªæ–­åˆ†å‰²ç”Ÿæˆçš„å¤šä¸ª input_ids åˆ—è¡¨ä¸­çš„ tokenï¼Œé€šè¿‡æ˜ å°„ä¿ç•™åŸå§‹æ–‡æœ¬çš„ input_idsã€‚\n",
    "\n",
    "å¦‚ä¸‹æ‰€ç¤ºï¼šç¬¬ä¸€ä¸ªæ ‡è®°ï¼ˆ[CLS]ï¼‰çš„èµ·å§‹å’Œç»“æŸå­—ç¬¦éƒ½æ˜¯ï¼ˆ0, 0ï¼‰ï¼Œå› ä¸ºå®ƒä¸å¯¹åº”é—®é¢˜/ç­”æ¡ˆçš„ä»»ä½•éƒ¨åˆ†ï¼Œç„¶åç¬¬äºŒä¸ªæ ‡è®°ä¸é—®é¢˜(question)çš„å­—ç¬¦0åˆ°3ç›¸åŒ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],            # ç¬¬ä¸€ä¸ªå‚æ•°ï¼šé—®é¢˜æ–‡æœ¬\n",
    "    example[\"context\"],             # ç¬¬äºŒä¸ªå‚æ•°ï¼šä¸Šä¸‹æ–‡æ–‡æœ¬\n",
    "    max_length=max_length,          # æœ€å¤§è¾“å…¥é•¿åº¦ï¼ˆå¦‚384ï¼‰\n",
    "    truncation=\"only_second\",       # å…³é”®å‚æ•°1ï¼šæˆªæ–­ç­–ç•¥\n",
    "    return_overflowing_tokens=True, # å…³é”®å‚æ•°2ï¼šè¿”å›åˆ†å—ç»“æœ\n",
    "    return_offsets_mapping=True,    # å…³é”®å‚æ•°3ï¼šè¿”å›å­—ç¬¦çº§ä½ç½®æ˜ å°„\n",
    "    return_token_type_ids=True,     # æ˜¾å¼è¦æ±‚è¿”å› token_type_ids\n",
    "    stride=doc_stride               # åˆ†å—æ»‘åŠ¨æ­¥é•¿ï¼ˆå¦‚128ï¼‰\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **å‚æ•°è¯¦è§£**\n",
    "#### 1. `truncation=\"only_second\"`\n",
    "- **ä½œç”¨**ï¼š**åªæˆªæ–­ç¬¬äºŒä¸ªå‚æ•°ï¼ˆä¸Šä¸‹æ–‡ï¼‰**ï¼Œä¿æŒç¬¬ä¸€ä¸ªå‚æ•°ï¼ˆé—®é¢˜ï¼‰å®Œæ•´\n",
    "- **åœºæ™¯**ï¼šå½“ `é—®é¢˜+ä¸Šä¸‹æ–‡` æ€»é•¿åº¦è¶…è¿‡ `max_length` æ—¶ï¼Œä¼˜å…ˆä¿ç•™é—®é¢˜å®Œæ•´æ€§\n",
    "- **ç¤ºä¾‹**ï¼š\n",
    "  ```python\n",
    "  # è¾“å…¥ï¼šé—®é¢˜é•¿åº¦20ï¼Œä¸Šä¸‹æ–‡é•¿åº¦400 â†’ æ€»é•¿åº¦420 > 384\n",
    "  # å¤„ç†ï¼šæˆªæ–­ä¸Šä¸‹æ–‡ä¸º 384-20-3ï¼ˆç‰¹æ®Šæ ‡è®°ï¼‰= 361 tokens\n",
    "  ```\n",
    "\n",
    "#### 2. `return_overflowing_tokens=True`\n",
    "- **ä½œç”¨**ï¼š**è¿”å›åˆ†å—åçš„å¤šä¸ªè¾“å…¥ç‰¹å¾**ï¼ˆå½“è¾“å…¥è¿‡é•¿æ—¶è‡ªåŠ¨åˆ†å‰²ï¼‰\n",
    "- **è¾“å‡ºå­—æ®µ**ï¼š`overflow_to_sample_mapping`ï¼ˆåˆ†å—å¯¹åº”åŸå§‹æ ·æœ¬çš„ç´¢å¼•ï¼‰\n",
    "- **åˆ†å—é€»è¾‘**ï¼š\n",
    "  - å°†é•¿ä¸Šä¸‹æ–‡æŒ‰ `max_length - é—®é¢˜é•¿åº¦` åˆ‡å‰²\n",
    "  - ç›¸é‚»åˆ†å—é‡å  `stride` tokensï¼ˆç¡®ä¿ç­”æ¡ˆä¸è¢«åˆ‡å‰²ï¼‰\n",
    "\n",
    "#### 3. `return_offsets_mapping=True`\n",
    "- **ä½œç”¨**ï¼š**è¿”å›æ¯ä¸ª token åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å­—ç¬¦ä½ç½®**ï¼ˆèµ·å§‹å’Œç»“æŸç´¢å¼•ï¼‰\n",
    "- **è¾“å‡ºå­—æ®µ**ï¼š`offset_mapping`ï¼ˆåˆ—è¡¨çš„åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ `(start, end)` å…ƒç»„ï¼‰\n",
    "- **å…³é”®ç”¨é€”**ï¼šå°†æ¨¡å‹é¢„æµ‹çš„ token ä½ç½®æ˜ å°„å›åŸå§‹æ–‡æœ¬ï¼ˆå¦‚å®šä½ç­”æ¡ˆï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **`offset_mapping` ç¤ºä¾‹è§£æ**\n",
    "```python\n",
    "# å‡è®¾æ‰“å°ç»“æœå‰5ä¸ªå…ƒç´ ï¼š\n",
    "[(0, 0), (0, 3), (4, 7), (8, 11), (12, 15), ...]\n",
    "\n",
    "# å¯¹åº”å«ä¹‰ï¼š\n",
    "# [CLS]  What    is    your   name?  [SEP] ...\n",
    "# (0,0) (0,3) (4,7) (8,11) (12,15)   ...\n",
    "```\n",
    "- **ç‰¹æ®Šæ ‡è®°**ï¼š`[CLS]`ã€`[SEP]` ç­‰æ— å¯¹åº”æ–‡æœ¬ â†’ `(0, 0)`\n",
    "- **é—®é¢˜éƒ¨åˆ†**ï¼šå­—ç¬¦ç´¢å¼•ä»é—®é¢˜æ–‡æœ¬çš„èµ·å§‹ä½ç½®è®¡ç®—\n",
    "- **ä¸Šä¸‹æ–‡éƒ¨åˆ†**ï¼šå­—ç¬¦ç´¢å¼•ä»ä¸Šä¸‹æ–‡æ–‡æœ¬çš„èµ·å§‹ä½ç½®è®¡ç®—ï¼ˆéœ€æ³¨æ„é—®é¢˜æ–‡æœ¬é•¿åº¦ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **å‚æ•°ååŒä½œç”¨**\n",
    "| å‚æ•°ç»„åˆ                        | å®é™…æ•ˆæœ                                                                 |\n",
    "|---------------------------------|------------------------------------------------------------------------|\n",
    "| `truncation=\"only_second\"` + `return_overflowing_tokens=True` | å°†é•¿ä¸Šä¸‹æ–‡åˆ‡å‰²ä¸ºå¤šä¸ªåˆ†å—ï¼Œæ¯ä¸ªåˆ†å—åŒ…å«å®Œæ•´é—®é¢˜å’Œéƒ¨åˆ†ä¸Šä¸‹æ–‡ |\n",
    "| `return_offsets_mapping=True`   | æä¾›åˆ†å—ä¸­æ¯ä¸ª token åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„ä½ç½®ï¼Œç”¨äºç­”æ¡ˆä½ç½®æ˜ å°„               |\n",
    "\n",
    "---\n",
    "\n",
    "### **åº”ç”¨åœºæ™¯**\n",
    "1. **è®­ç»ƒé˜¶æ®µ**ï¼šå°†ç­”æ¡ˆçš„å­—ç¬¦ä½ç½®è½¬æ¢ä¸ºåˆ†å—å†…çš„ token ä½ç½®\n",
    "2. **æ¨ç†é˜¶æ®µ**ï¼šå°†æ¨¡å‹é¢„æµ‹çš„ token ä½ç½®åå‘æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡\n",
    "3. **æ•°æ®éªŒè¯**ï¼šæ£€æŸ¥åˆ†å—æ˜¯å¦è¦†ç›–æ­£ç¡®ç­”æ¡ˆçš„åŸå§‹ä½ç½®\n",
    "\n",
    "---\n",
    "\n",
    "é€šè¿‡è¿™ä¸‰ä¸ªå‚æ•°ï¼Œå®ç°äº†é•¿æ–‡æœ¬é—®ç­”ä»»åŠ¡ä¸­ **è¾“å…¥åˆ†å—å¤„ç†** å’Œ **ä½ç½®ç²¾ç¡®æ˜ å°„** çš„æ ¸å¿ƒéœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªæ˜ å°„æ¥æ‰¾åˆ°ç­”æ¡ˆåœ¨ç»™å®šç‰¹å¾ä¸­çš„èµ·å§‹å’Œç»“æŸæ ‡è®°çš„ä½ç½®ã€‚\n",
    "\n",
    "æˆ‘ä»¬åªéœ€åŒºåˆ†åç§»çš„å“ªäº›éƒ¨åˆ†å¯¹åº”äºé—®é¢˜ï¼Œå“ªäº›éƒ¨åˆ†å¯¹åº”äºä¸Šä¸‹æ–‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129\n",
      "(0, 3)\n",
      "how How\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(first_token_id)\n",
    "print(offsets)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many many\n"
     ]
    }
   ],
   "source": [
    "second_token_id = tokenized_example[\"input_ids\"][0][2]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][2]\n",
    "print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8214\n",
      "(29, 33)\n",
      "dame Dame\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][7]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][7]\n",
    "print(first_token_id)\n",
    "print(offsets)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055\n",
      "(38, 39)\n",
      "s 0\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][10]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][10]\n",
    "print(first_token_id)\n",
    "print(offsets)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"context\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== åˆ†å— 0 ===\n",
      "0 0\n",
      "Token 0: [CLS] â†’ \n",
      "0 3\n",
      "Token 1: how â†’ How\n",
      "4 8\n",
      "Token 2: many â†’ many\n",
      "9 13\n",
      "Token 3: wins â†’ wins\n",
      "14 18\n",
      "Token 4: does â†’ does\n",
      "19 22\n",
      "Token 5: the â†’ the\n",
      "23 28\n",
      "Token 6: notre â†’ Notre\n",
      "29 33\n",
      "Token 7: dame â†’ Dame\n",
      "34 37\n",
      "Token 8: men â†’ men\n",
      "37 38\n",
      "Token 9: ' â†’ '\n",
      "38 39\n",
      "Token 10: s â†’ s\n",
      "40 50\n",
      "Token 11: basketball â†’ basketball\n",
      "51 55\n",
      "Token 12: team â†’ team\n",
      "56 60\n",
      "Token 13: have â†’ have\n",
      "60 61\n",
      "Token 14: ? â†’ ?\n",
      "0 0\n",
      "Token 15: [SEP] â†’ \n",
      "0 3\n",
      "Token 16: the â†’ The\n",
      "4 7\n",
      "Token 17: men â†’ men\n",
      "7 8\n",
      "Token 18: ' â†’ '\n",
      "8 9\n",
      "Token 19: s â†’ s\n",
      "10 20\n",
      "Token 20: basketball â†’ basketball\n",
      "21 25\n",
      "Token 21: team â†’ team\n",
      "26 29\n",
      "Token 22: has â†’ has\n",
      "30 34\n",
      "Token 23: over â†’ over\n",
      "35 36\n",
      "Token 24: 1 â†’ 1\n",
      "36 37\n",
      "Token 25: , â†’ ,\n",
      "37 40\n",
      "Token 26: 600 â†’ 600\n",
      "41 45\n",
      "Token 27: wins â†’ wins\n",
      "45 46\n",
      "Token 28: , â†’ ,\n",
      "47 50\n",
      "Token 29: one â†’ one\n",
      "51 53\n",
      "Token 30: of â†’ of\n",
      "54 58\n",
      "Token 31: only â†’ only\n",
      "59 61\n",
      "Token 32: 12 â†’ 12\n",
      "62 69\n",
      "Token 33: schools â†’ schools\n",
      "70 73\n",
      "Token 34: who â†’ who\n",
      "74 78\n",
      "Token 35: have â†’ have\n",
      "79 86\n",
      "Token 36: reached â†’ reached\n",
      "87 91\n",
      "Token 37: that â†’ that\n",
      "92 96\n",
      "Token 38: mark â†’ mark\n",
      "96 97\n",
      "Token 39: , â†’ ,\n",
      "98 101\n",
      "Token 40: and â†’ and\n",
      "102 106\n",
      "Token 41: have â†’ have\n",
      "107 115\n",
      "Token 42: appeared â†’ appeared\n",
      "116 118\n",
      "Token 43: in â†’ in\n",
      "119 121\n",
      "Token 44: 28 â†’ 28\n",
      "122 126\n",
      "Token 45: ncaa â†’ NCAA\n",
      "127 138\n",
      "Token 46: tournaments â†’ tournaments\n",
      "138 139\n",
      "Token 47: . â†’ .\n",
      "140 146\n",
      "Token 48: former â†’ Former\n",
      "147 153\n",
      "Token 49: player â†’ player\n",
      "154 160\n",
      "Token 50: austin â†’ Austin\n",
      "161 165\n",
      "Token 51: carr â†’ Carr\n",
      "166 171\n",
      "Token 52: holds â†’ holds\n",
      "172 175\n",
      "Token 53: the â†’ the\n",
      "176 182\n",
      "Token 54: record â†’ record\n",
      "183 186\n",
      "Token 55: for â†’ for\n",
      "187 191\n",
      "Token 56: most â†’ most\n",
      "192 198\n",
      "Token 57: points â†’ points\n",
      "199 205\n",
      "Token 58: scored â†’ scored\n",
      "206 208\n",
      "Token 59: in â†’ in\n",
      "209 210\n",
      "Token 60: a â†’ a\n",
      "211 217\n",
      "Token 61: single â†’ single\n",
      "218 222\n",
      "Token 62: game â†’ game\n",
      "223 225\n",
      "Token 63: of â†’ of\n",
      "226 229\n",
      "Token 64: the â†’ the\n",
      "230 240\n",
      "Token 65: tournament â†’ tournament\n",
      "241 245\n",
      "Token 66: with â†’ with\n",
      "246 248\n",
      "Token 67: 61 â†’ 61\n",
      "248 249\n",
      "Token 68: . â†’ .\n",
      "250 258\n",
      "Token 69: although â†’ Although\n",
      "259 262\n",
      "Token 70: the â†’ the\n",
      "263 267\n",
      "Token 71: team â†’ team\n",
      "268 271\n",
      "Token 72: has â†’ has\n",
      "272 277\n",
      "Token 73: never â†’ never\n",
      "278 281\n",
      "Token 74: won â†’ won\n",
      "282 285\n",
      "Token 75: the â†’ the\n",
      "286 290\n",
      "Token 76: ncaa â†’ NCAA\n",
      "291 301\n",
      "Token 77: tournament â†’ Tournament\n",
      "301 302\n",
      "Token 78: , â†’ ,\n",
      "303 307\n",
      "Token 79: they â†’ they\n",
      "308 312\n",
      "Token 80: were â†’ were\n",
      "313 318\n",
      "Token 81: named â†’ named\n",
      "319 321\n",
      "Token 82: by â†’ by\n",
      "322 325\n",
      "Token 83: the â†’ the\n",
      "326 330\n",
      "Token 84: helm â†’ Helm\n",
      "330 331\n",
      "Token 85: ##s â†’ s\n",
      "332 340\n",
      "Token 86: athletic â†’ Athletic\n",
      "341 351\n",
      "Token 87: foundation â†’ Foundation\n",
      "352 354\n",
      "Token 88: as â†’ as\n",
      "355 363\n",
      "Token 89: national â†’ national\n",
      "364 373\n",
      "Token 90: champions â†’ champions\n",
      "374 379\n",
      "Token 91: twice â†’ twice\n",
      "379 380\n",
      "Token 92: . â†’ .\n",
      "381 384\n",
      "Token 93: the â†’ The\n",
      "385 389\n",
      "Token 94: team â†’ team\n",
      "390 393\n",
      "Token 95: has â†’ has\n",
      "394 406\n",
      "Token 96: orchestrated â†’ orchestrated\n",
      "407 408\n",
      "Token 97: a â†’ a\n",
      "409 415\n",
      "Token 98: number â†’ number\n",
      "416 418\n",
      "Token 99: of â†’ of\n",
      "419 424\n",
      "Token 100: upset â†’ upset\n",
      "424 425\n",
      "Token 101: ##s â†’ s\n",
      "426 428\n",
      "Token 102: of â†’ of\n",
      "429 435\n",
      "Token 103: number â†’ number\n",
      "436 439\n",
      "Token 104: one â†’ one\n",
      "440 446\n",
      "Token 105: ranked â†’ ranked\n",
      "447 452\n",
      "Token 106: teams â†’ teams\n",
      "452 453\n",
      "Token 107: , â†’ ,\n",
      "454 457\n",
      "Token 108: the â†’ the\n",
      "458 462\n",
      "Token 109: most â†’ most\n",
      "463 470\n",
      "Token 110: notable â†’ notable\n",
      "471 473\n",
      "Token 111: of â†’ of\n",
      "474 479\n",
      "Token 112: which â†’ which\n",
      "480 483\n",
      "Token 113: was â†’ was\n",
      "484 490\n",
      "Token 114: ending â†’ ending\n",
      "491 495\n",
      "Token 115: ucla â†’ UCLA\n",
      "495 496\n",
      "Token 116: ' â†’ '\n",
      "496 497\n",
      "Token 117: s â†’ s\n",
      "498 504\n",
      "Token 118: record â†’ record\n",
      "505 507\n",
      "Token 119: 88 â†’ 88\n",
      "507 508\n",
      "Token 120: - â†’ -\n",
      "508 512\n",
      "Token 121: game â†’ game\n",
      "513 520\n",
      "Token 122: winning â†’ winning\n",
      "521 527\n",
      "Token 123: streak â†’ streak\n",
      "528 530\n",
      "Token 124: in â†’ in\n",
      "531 535\n",
      "Token 125: 1974 â†’ 1974\n",
      "535 536\n",
      "Token 126: . â†’ .\n",
      "537 540\n",
      "Token 127: the â†’ The\n",
      "541 545\n",
      "Token 128: team â†’ team\n",
      "546 549\n",
      "Token 129: has â†’ has\n",
      "550 556\n",
      "Token 130: beaten â†’ beaten\n",
      "557 559\n",
      "Token 131: an â†’ an\n",
      "560 570\n",
      "Token 132: additional â†’ additional\n",
      "571 576\n",
      "Token 133: eight â†’ eight\n",
      "577 583\n",
      "Token 134: number â†’ number\n",
      "583 584\n",
      "Token 135: - â†’ -\n",
      "584 587\n",
      "Token 136: one â†’ one\n",
      "588 593\n",
      "Token 137: teams â†’ teams\n",
      "593 594\n",
      "Token 138: , â†’ ,\n",
      "595 598\n",
      "Token 139: and â†’ and\n",
      "599 604\n",
      "Token 140: those â†’ those\n",
      "605 609\n",
      "Token 141: nine â†’ nine\n",
      "610 614\n",
      "Token 142: wins â†’ wins\n",
      "615 619\n",
      "Token 143: rank â†’ rank\n",
      "620 626\n",
      "Token 144: second â†’ second\n",
      "626 627\n",
      "Token 145: , â†’ ,\n",
      "628 630\n",
      "Token 146: to â†’ to\n",
      "631 635\n",
      "Token 147: ucla â†’ UCLA\n",
      "635 636\n",
      "Token 148: ' â†’ '\n",
      "636 637\n",
      "Token 149: s â†’ s\n",
      "638 640\n",
      "Token 150: 10 â†’ 10\n",
      "640 641\n",
      "Token 151: , â†’ ,\n",
      "642 645\n",
      "Token 152: all â†’ all\n",
      "645 646\n",
      "Token 153: - â†’ -\n",
      "646 650\n",
      "Token 154: time â†’ time\n",
      "651 653\n",
      "Token 155: in â†’ in\n",
      "654 658\n",
      "Token 156: wins â†’ wins\n",
      "659 666\n",
      "Token 157: against â†’ against\n",
      "667 670\n",
      "Token 158: the â†’ the\n",
      "671 674\n",
      "Token 159: top â†’ top\n",
      "675 679\n",
      "Token 160: team â†’ team\n",
      "679 680\n",
      "Token 161: . â†’ .\n",
      "681 684\n",
      "Token 162: the â†’ The\n",
      "685 689\n",
      "Token 163: team â†’ team\n",
      "690 695\n",
      "Token 164: plays â†’ plays\n",
      "696 698\n",
      "Token 165: in â†’ in\n",
      "699 704\n",
      "Token 166: newly â†’ newly\n",
      "705 714\n",
      "Token 167: renovated â†’ renovated\n",
      "715 722\n",
      "Token 168: purcell â†’ Purcell\n",
      "723 731\n",
      "Token 169: pavilion â†’ Pavilion\n",
      "732 733\n",
      "Token 170: ( â†’ (\n",
      "733 739\n",
      "Token 171: within â†’ within\n",
      "740 743\n",
      "Token 172: the â†’ the\n",
      "744 750\n",
      "Token 173: edmund â†’ Edmund\n",
      "751 752\n",
      "Token 174: p â†’ P\n",
      "752 753\n",
      "Token 175: . â†’ .\n",
      "754 759\n",
      "Token 176: joyce â†’ Joyce\n",
      "760 766\n",
      "Token 177: center â†’ Center\n",
      "766 767\n",
      "Token 178: ) â†’ )\n",
      "767 768\n",
      "Token 179: , â†’ ,\n",
      "769 774\n",
      "Token 180: which â†’ which\n",
      "775 783\n",
      "Token 181: reopened â†’ reopened\n",
      "784 787\n",
      "Token 182: for â†’ for\n",
      "788 791\n",
      "Token 183: the â†’ the\n",
      "792 801\n",
      "Token 184: beginning â†’ beginning\n",
      "802 804\n",
      "Token 185: of â†’ of\n",
      "805 808\n",
      "Token 186: the â†’ the\n",
      "809 813\n",
      "Token 187: 2009 â†’ 2009\n",
      "813 814\n",
      "Token 188: â€“ â†’ â€“\n",
      "814 818\n",
      "Token 189: 2010 â†’ 2010\n",
      "819 825\n",
      "Token 190: season â†’ season\n",
      "825 826\n",
      "Token 191: . â†’ .\n",
      "827 830\n",
      "Token 192: the â†’ The\n",
      "831 835\n",
      "Token 193: team â†’ team\n",
      "836 838\n",
      "Token 194: is â†’ is\n",
      "839 846\n",
      "Token 195: coached â†’ coached\n",
      "847 849\n",
      "Token 196: by â†’ by\n",
      "850 854\n",
      "Token 197: mike â†’ Mike\n",
      "855 857\n",
      "Token 198: br â†’ Br\n",
      "857 859\n",
      "Token 199: ##ey â†’ ey\n",
      "859 860\n",
      "Token 200: , â†’ ,\n",
      "861 864\n",
      "Token 201: who â†’ who\n",
      "864 865\n",
      "Token 202: , â†’ ,\n",
      "866 868\n",
      "Token 203: as â†’ as\n",
      "869 871\n",
      "Token 204: of â†’ of\n",
      "872 875\n",
      "Token 205: the â†’ the\n",
      "876 880\n",
      "Token 206: 2014 â†’ 2014\n",
      "880 881\n",
      "Token 207: â€“ â†’ â€“\n",
      "881 883\n",
      "Token 208: 15 â†’ 15\n",
      "884 890\n",
      "Token 209: season â†’ season\n",
      "890 891\n",
      "Token 210: , â†’ ,\n",
      "892 895\n",
      "Token 211: his â†’ his\n",
      "896 905\n",
      "Token 212: fifteenth â†’ fifteenth\n",
      "906 908\n",
      "Token 213: at â†’ at\n",
      "909 914\n",
      "Token 214: notre â†’ Notre\n",
      "915 919\n",
      "Token 215: dame â†’ Dame\n",
      "919 920\n",
      "Token 216: , â†’ ,\n",
      "921 924\n",
      "Token 217: has â†’ has\n",
      "925 933\n",
      "Token 218: achieved â†’ achieved\n",
      "934 935\n",
      "Token 219: a â†’ a\n",
      "936 939\n",
      "Token 220: 332 â†’ 332\n",
      "939 940\n",
      "Token 221: - â†’ -\n",
      "940 943\n",
      "Token 222: 165 â†’ 165\n",
      "944 950\n",
      "Token 223: record â†’ record\n",
      "950 951\n",
      "Token 224: . â†’ .\n",
      "952 954\n",
      "Token 225: in â†’ In\n",
      "955 959\n",
      "Token 226: 2009 â†’ 2009\n",
      "960 964\n",
      "Token 227: they â†’ they\n",
      "965 969\n",
      "Token 228: were â†’ were\n",
      "970 977\n",
      "Token 229: invited â†’ invited\n",
      "978 980\n",
      "Token 230: to â†’ to\n",
      "981 984\n",
      "Token 231: the â†’ the\n",
      "985 987\n",
      "Token 232: ni â†’ NI\n",
      "987 988\n",
      "Token 233: ##t â†’ T\n",
      "988 989\n",
      "Token 234: , â†’ ,\n",
      "990 995\n",
      "Token 235: where â†’ where\n",
      "996 1000\n",
      "Token 236: they â†’ they\n",
      "1001 1009\n",
      "Token 237: advanced â†’ advanced\n",
      "1010 1012\n",
      "Token 238: to â†’ to\n",
      "1013 1016\n",
      "Token 239: the â†’ the\n",
      "1017 1027\n",
      "Token 240: semifinals â†’ semifinals\n",
      "1028 1031\n",
      "Token 241: but â†’ but\n",
      "1032 1036\n",
      "Token 242: were â†’ were\n",
      "1037 1043\n",
      "Token 243: beaten â†’ beaten\n",
      "1044 1046\n",
      "Token 244: by â†’ by\n",
      "1047 1051\n",
      "Token 245: penn â†’ Penn\n",
      "1052 1057\n",
      "Token 246: state â†’ State\n",
      "1058 1061\n",
      "Token 247: who â†’ who\n",
      "1062 1066\n",
      "Token 248: went â†’ went\n",
      "1067 1069\n",
      "Token 249: on â†’ on\n",
      "1070 1073\n",
      "Token 250: and â†’ and\n",
      "1074 1078\n",
      "Token 251: beat â†’ beat\n",
      "1079 1085\n",
      "Token 252: baylor â†’ Baylor\n",
      "1086 1088\n",
      "Token 253: in â†’ in\n",
      "1089 1092\n",
      "Token 254: the â†’ the\n",
      "1093 1105\n",
      "Token 255: championship â†’ championship\n",
      "1105 1106\n",
      "Token 256: . â†’ .\n",
      "1107 1110\n",
      "Token 257: the â†’ The\n",
      "1111 1115\n",
      "Token 258: 2010 â†’ 2010\n",
      "1115 1116\n",
      "Token 259: â€“ â†’ â€“\n",
      "1116 1118\n",
      "Token 260: 11 â†’ 11\n",
      "1119 1123\n",
      "Token 261: team â†’ team\n",
      "1124 1133\n",
      "Token 262: concluded â†’ concluded\n",
      "1134 1137\n",
      "Token 263: its â†’ its\n",
      "1138 1145\n",
      "Token 264: regular â†’ regular\n",
      "1146 1152\n",
      "Token 265: season â†’ season\n",
      "1153 1159\n",
      "Token 266: ranked â†’ ranked\n",
      "1160 1166\n",
      "Token 267: number â†’ number\n",
      "1167 1172\n",
      "Token 268: seven â†’ seven\n",
      "1173 1175\n",
      "Token 269: in â†’ in\n",
      "1176 1179\n",
      "Token 270: the â†’ the\n",
      "1180 1187\n",
      "Token 271: country â†’ country\n",
      "1187 1188\n",
      "Token 272: , â†’ ,\n",
      "1189 1193\n",
      "Token 273: with â†’ with\n",
      "1194 1195\n",
      "Token 274: a â†’ a\n",
      "1196 1202\n",
      "Token 275: record â†’ record\n",
      "1203 1205\n",
      "Token 276: of â†’ of\n",
      "1206 1208\n",
      "Token 277: 25 â†’ 25\n",
      "1208 1209\n",
      "Token 278: â€“ â†’ â€“\n",
      "1209 1210\n",
      "Token 279: 5 â†’ 5\n",
      "1210 1211\n",
      "Token 280: , â†’ ,\n",
      "1212 1214\n",
      "Token 281: br â†’ Br\n",
      "1214 1216\n",
      "Token 282: ##ey â†’ ey\n",
      "1216 1217\n",
      "Token 283: ' â†’ '\n",
      "1217 1218\n",
      "Token 284: s â†’ s\n",
      "1219 1224\n",
      "Token 285: fifth â†’ fifth\n",
      "1225 1233\n",
      "Token 286: straight â†’ straight\n",
      "1234 1236\n",
      "Token 287: 20 â†’ 20\n",
      "1236 1237\n",
      "Token 288: - â†’ -\n",
      "1237 1240\n",
      "Token 289: win â†’ win\n",
      "1241 1247\n",
      "Token 290: season â†’ season\n",
      "1247 1248\n",
      "Token 291: , â†’ ,\n",
      "1249 1252\n",
      "Token 292: and â†’ and\n",
      "1253 1254\n",
      "Token 293: a â†’ a\n",
      "1255 1261\n",
      "Token 294: second â†’ second\n",
      "1261 1262\n",
      "Token 295: - â†’ -\n",
      "1262 1267\n",
      "Token 296: place â†’ place\n",
      "1268 1274\n",
      "Token 297: finish â†’ finish\n",
      "1275 1277\n",
      "Token 298: in â†’ in\n",
      "1278 1281\n",
      "Token 299: the â†’ the\n",
      "1282 1285\n",
      "Token 300: big â†’ Big\n",
      "1286 1290\n",
      "Token 301: east â†’ East\n",
      "1290 1291\n",
      "Token 302: . â†’ .\n",
      "1292 1298\n",
      "Token 303: during â†’ During\n",
      "1299 1302\n",
      "Token 304: the â†’ the\n",
      "1303 1307\n",
      "Token 305: 2014 â†’ 2014\n",
      "1307 1308\n",
      "Token 306: - â†’ -\n",
      "1308 1310\n",
      "Token 307: 15 â†’ 15\n",
      "1311 1317\n",
      "Token 308: season â†’ season\n",
      "1317 1318\n",
      "Token 309: , â†’ ,\n",
      "1319 1322\n",
      "Token 310: the â†’ the\n",
      "1323 1327\n",
      "Token 311: team â†’ team\n",
      "1328 1332\n",
      "Token 312: went â†’ went\n",
      "1333 1335\n",
      "Token 313: 32 â†’ 32\n",
      "1335 1336\n",
      "Token 314: - â†’ -\n",
      "1336 1337\n",
      "Token 315: 6 â†’ 6\n",
      "1338 1341\n",
      "Token 316: and â†’ and\n",
      "1342 1345\n",
      "Token 317: won â†’ won\n",
      "1346 1349\n",
      "Token 318: the â†’ the\n",
      "1350 1353\n",
      "Token 319: acc â†’ ACC\n",
      "1354 1364\n",
      "Token 320: conference â†’ conference\n",
      "1365 1375\n",
      "Token 321: tournament â†’ tournament\n",
      "1375 1376\n",
      "Token 322: , â†’ ,\n",
      "1377 1382\n",
      "Token 323: later â†’ later\n",
      "1383 1392\n",
      "Token 324: advancing â†’ advancing\n",
      "1393 1395\n",
      "Token 325: to â†’ to\n",
      "1396 1399\n",
      "Token 326: the â†’ the\n",
      "1400 1405\n",
      "Token 327: elite â†’ Elite\n",
      "1406 1407\n",
      "Token 328: 8 â†’ 8\n",
      "1407 1408\n",
      "Token 329: , â†’ ,\n",
      "1409 1414\n",
      "Token 330: where â†’ where\n",
      "1415 1418\n",
      "Token 331: the â†’ the\n",
      "1419 1427\n",
      "Token 332: fighting â†’ Fighting\n",
      "1428 1433\n",
      "Token 333: irish â†’ Irish\n",
      "1434 1438\n",
      "Token 334: lost â†’ lost\n",
      "1439 1441\n",
      "Token 335: on â†’ on\n",
      "1442 1443\n",
      "Token 336: a â†’ a\n",
      "1444 1450\n",
      "Token 337: missed â†’ missed\n",
      "1451 1455\n",
      "Token 338: buzz â†’ buzz\n",
      "1455 1457\n",
      "Token 339: ##er â†’ er\n",
      "1457 1458\n",
      "Token 340: - â†’ -\n",
      "1458 1462\n",
      "Token 341: beat â†’ beat\n",
      "1462 1464\n",
      "Token 342: ##er â†’ er\n",
      "1465 1472\n",
      "Token 343: against â†’ against\n",
      "1473 1477\n",
      "Token 344: then â†’ then\n",
      "1478 1488\n",
      "Token 345: undefeated â†’ undefeated\n",
      "1489 1497\n",
      "Token 346: kentucky â†’ Kentucky\n",
      "1497 1498\n",
      "Token 347: . â†’ .\n",
      "1499 1502\n",
      "Token 348: led â†’ Led\n",
      "1503 1505\n",
      "Token 349: by â†’ by\n",
      "1506 1509\n",
      "Token 350: nba â†’ NBA\n",
      "1510 1515\n",
      "Token 351: draft â†’ draft\n",
      "1516 1521\n",
      "Token 352: picks â†’ picks\n",
      "1522 1524\n",
      "Token 353: je â†’ Je\n",
      "1524 1528\n",
      "Token 354: ##rian â†’ rian\n",
      "1529 1534\n",
      "Token 355: grant â†’ Grant\n",
      "1535 1538\n",
      "Token 356: and â†’ and\n",
      "1539 1542\n",
      "Token 357: pat â†’ Pat\n",
      "1543 1546\n",
      "Token 358: con â†’ Con\n",
      "1546 1548\n",
      "Token 359: ##na â†’ na\n",
      "1548 1552\n",
      "Token 360: ##ught â†’ ught\n",
      "1552 1554\n",
      "Token 361: ##on â†’ on\n",
      "1554 1555\n",
      "Token 362: , â†’ ,\n",
      "1556 1559\n",
      "Token 363: the â†’ the\n",
      "1560 1568\n",
      "Token 364: fighting â†’ Fighting\n",
      "1569 1574\n",
      "Token 365: irish â†’ Irish\n",
      "1575 1579\n",
      "Token 366: beat â†’ beat\n",
      "1580 1583\n",
      "Token 367: the â†’ the\n",
      "1584 1592\n",
      "Token 368: eventual â†’ eventual\n",
      "1593 1601\n",
      "Token 369: national â†’ national\n",
      "1602 1610\n",
      "Token 370: champion â†’ champion\n",
      "1611 1615\n",
      "Token 371: duke â†’ Duke\n",
      "1616 1620\n",
      "Token 372: blue â†’ Blue\n",
      "1621 1627\n",
      "Token 373: devils â†’ Devils\n",
      "1628 1633\n",
      "Token 374: twice â†’ twice\n",
      "1634 1640\n",
      "Token 375: during â†’ during\n",
      "1641 1644\n",
      "Token 376: the â†’ the\n",
      "1645 1651\n",
      "Token 377: season â†’ season\n",
      "1651 1652\n",
      "Token 378: . â†’ .\n",
      "1653 1656\n",
      "Token 379: the â†’ The\n",
      "1657 1659\n",
      "Token 380: 32 â†’ 32\n",
      "1660 1664\n",
      "Token 381: wins â†’ wins\n",
      "1665 1669\n",
      "Token 382: were â†’ were\n",
      "0 0\n",
      "Token 383: [SEP] â†’ \n",
      "\n",
      "=== åˆ†å— 1 ===\n",
      "0 0\n",
      "Token 0: [CLS] â†’ \n",
      "0 3\n",
      "Token 1: how â†’ How\n",
      "4 8\n",
      "Token 2: many â†’ many\n",
      "9 13\n",
      "Token 3: wins â†’ wins\n",
      "14 18\n",
      "Token 4: does â†’ does\n",
      "19 22\n",
      "Token 5: the â†’ the\n",
      "23 28\n",
      "Token 6: notre â†’ Notre\n",
      "29 33\n",
      "Token 7: dame â†’ Dame\n",
      "34 37\n",
      "Token 8: men â†’ men\n",
      "37 38\n",
      "Token 9: ' â†’ '\n",
      "38 39\n",
      "Token 10: s â†’ s\n",
      "40 50\n",
      "Token 11: basketball â†’ basketball\n",
      "51 55\n",
      "Token 12: team â†’ team\n",
      "56 60\n",
      "Token 13: have â†’ have\n",
      "60 61\n",
      "Token 14: ? â†’ ?\n",
      "0 0\n",
      "Token 15: [SEP] â†’ \n",
      "1093 1105\n",
      "Token 16: championship â†’ championship\n",
      "1105 1106\n",
      "Token 17: . â†’ .\n",
      "1107 1110\n",
      "Token 18: the â†’ The\n",
      "1111 1115\n",
      "Token 19: 2010 â†’ 2010\n",
      "1115 1116\n",
      "Token 20: â€“ â†’ â€“\n",
      "1116 1118\n",
      "Token 21: 11 â†’ 11\n",
      "1119 1123\n",
      "Token 22: team â†’ team\n",
      "1124 1133\n",
      "Token 23: concluded â†’ concluded\n",
      "1134 1137\n",
      "Token 24: its â†’ its\n",
      "1138 1145\n",
      "Token 25: regular â†’ regular\n",
      "1146 1152\n",
      "Token 26: season â†’ season\n",
      "1153 1159\n",
      "Token 27: ranked â†’ ranked\n",
      "1160 1166\n",
      "Token 28: number â†’ number\n",
      "1167 1172\n",
      "Token 29: seven â†’ seven\n",
      "1173 1175\n",
      "Token 30: in â†’ in\n",
      "1176 1179\n",
      "Token 31: the â†’ the\n",
      "1180 1187\n",
      "Token 32: country â†’ country\n",
      "1187 1188\n",
      "Token 33: , â†’ ,\n",
      "1189 1193\n",
      "Token 34: with â†’ with\n",
      "1194 1195\n",
      "Token 35: a â†’ a\n",
      "1196 1202\n",
      "Token 36: record â†’ record\n",
      "1203 1205\n",
      "Token 37: of â†’ of\n",
      "1206 1208\n",
      "Token 38: 25 â†’ 25\n",
      "1208 1209\n",
      "Token 39: â€“ â†’ â€“\n",
      "1209 1210\n",
      "Token 40: 5 â†’ 5\n",
      "1210 1211\n",
      "Token 41: , â†’ ,\n",
      "1212 1214\n",
      "Token 42: br â†’ Br\n",
      "1214 1216\n",
      "Token 43: ##ey â†’ ey\n",
      "1216 1217\n",
      "Token 44: ' â†’ '\n",
      "1217 1218\n",
      "Token 45: s â†’ s\n",
      "1219 1224\n",
      "Token 46: fifth â†’ fifth\n",
      "1225 1233\n",
      "Token 47: straight â†’ straight\n",
      "1234 1236\n",
      "Token 48: 20 â†’ 20\n",
      "1236 1237\n",
      "Token 49: - â†’ -\n",
      "1237 1240\n",
      "Token 50: win â†’ win\n",
      "1241 1247\n",
      "Token 51: season â†’ season\n",
      "1247 1248\n",
      "Token 52: , â†’ ,\n",
      "1249 1252\n",
      "Token 53: and â†’ and\n",
      "1253 1254\n",
      "Token 54: a â†’ a\n",
      "1255 1261\n",
      "Token 55: second â†’ second\n",
      "1261 1262\n",
      "Token 56: - â†’ -\n",
      "1262 1267\n",
      "Token 57: place â†’ place\n",
      "1268 1274\n",
      "Token 58: finish â†’ finish\n",
      "1275 1277\n",
      "Token 59: in â†’ in\n",
      "1278 1281\n",
      "Token 60: the â†’ the\n",
      "1282 1285\n",
      "Token 61: big â†’ Big\n",
      "1286 1290\n",
      "Token 62: east â†’ East\n",
      "1290 1291\n",
      "Token 63: . â†’ .\n",
      "1292 1298\n",
      "Token 64: during â†’ During\n",
      "1299 1302\n",
      "Token 65: the â†’ the\n",
      "1303 1307\n",
      "Token 66: 2014 â†’ 2014\n",
      "1307 1308\n",
      "Token 67: - â†’ -\n",
      "1308 1310\n",
      "Token 68: 15 â†’ 15\n",
      "1311 1317\n",
      "Token 69: season â†’ season\n",
      "1317 1318\n",
      "Token 70: , â†’ ,\n",
      "1319 1322\n",
      "Token 71: the â†’ the\n",
      "1323 1327\n",
      "Token 72: team â†’ team\n",
      "1328 1332\n",
      "Token 73: went â†’ went\n",
      "1333 1335\n",
      "Token 74: 32 â†’ 32\n",
      "1335 1336\n",
      "Token 75: - â†’ -\n",
      "1336 1337\n",
      "Token 76: 6 â†’ 6\n",
      "1338 1341\n",
      "Token 77: and â†’ and\n",
      "1342 1345\n",
      "Token 78: won â†’ won\n",
      "1346 1349\n",
      "Token 79: the â†’ the\n",
      "1350 1353\n",
      "Token 80: acc â†’ ACC\n",
      "1354 1364\n",
      "Token 81: conference â†’ conference\n",
      "1365 1375\n",
      "Token 82: tournament â†’ tournament\n",
      "1375 1376\n",
      "Token 83: , â†’ ,\n",
      "1377 1382\n",
      "Token 84: later â†’ later\n",
      "1383 1392\n",
      "Token 85: advancing â†’ advancing\n",
      "1393 1395\n",
      "Token 86: to â†’ to\n",
      "1396 1399\n",
      "Token 87: the â†’ the\n",
      "1400 1405\n",
      "Token 88: elite â†’ Elite\n",
      "1406 1407\n",
      "Token 89: 8 â†’ 8\n",
      "1407 1408\n",
      "Token 90: , â†’ ,\n",
      "1409 1414\n",
      "Token 91: where â†’ where\n",
      "1415 1418\n",
      "Token 92: the â†’ the\n",
      "1419 1427\n",
      "Token 93: fighting â†’ Fighting\n",
      "1428 1433\n",
      "Token 94: irish â†’ Irish\n",
      "1434 1438\n",
      "Token 95: lost â†’ lost\n",
      "1439 1441\n",
      "Token 96: on â†’ on\n",
      "1442 1443\n",
      "Token 97: a â†’ a\n",
      "1444 1450\n",
      "Token 98: missed â†’ missed\n",
      "1451 1455\n",
      "Token 99: buzz â†’ buzz\n",
      "1455 1457\n",
      "Token 100: ##er â†’ er\n",
      "1457 1458\n",
      "Token 101: - â†’ -\n",
      "1458 1462\n",
      "Token 102: beat â†’ beat\n",
      "1462 1464\n",
      "Token 103: ##er â†’ er\n",
      "1465 1472\n",
      "Token 104: against â†’ against\n",
      "1473 1477\n",
      "Token 105: then â†’ then\n",
      "1478 1488\n",
      "Token 106: undefeated â†’ undefeated\n",
      "1489 1497\n",
      "Token 107: kentucky â†’ Kentucky\n",
      "1497 1498\n",
      "Token 108: . â†’ .\n",
      "1499 1502\n",
      "Token 109: led â†’ Led\n",
      "1503 1505\n",
      "Token 110: by â†’ by\n",
      "1506 1509\n",
      "Token 111: nba â†’ NBA\n",
      "1510 1515\n",
      "Token 112: draft â†’ draft\n",
      "1516 1521\n",
      "Token 113: picks â†’ picks\n",
      "1522 1524\n",
      "Token 114: je â†’ Je\n",
      "1524 1528\n",
      "Token 115: ##rian â†’ rian\n",
      "1529 1534\n",
      "Token 116: grant â†’ Grant\n",
      "1535 1538\n",
      "Token 117: and â†’ and\n",
      "1539 1542\n",
      "Token 118: pat â†’ Pat\n",
      "1543 1546\n",
      "Token 119: con â†’ Con\n",
      "1546 1548\n",
      "Token 120: ##na â†’ na\n",
      "1548 1552\n",
      "Token 121: ##ught â†’ ught\n",
      "1552 1554\n",
      "Token 122: ##on â†’ on\n",
      "1554 1555\n",
      "Token 123: , â†’ ,\n",
      "1556 1559\n",
      "Token 124: the â†’ the\n",
      "1560 1568\n",
      "Token 125: fighting â†’ Fighting\n",
      "1569 1574\n",
      "Token 126: irish â†’ Irish\n",
      "1575 1579\n",
      "Token 127: beat â†’ beat\n",
      "1580 1583\n",
      "Token 128: the â†’ the\n",
      "1584 1592\n",
      "Token 129: eventual â†’ eventual\n",
      "1593 1601\n",
      "Token 130: national â†’ national\n",
      "1602 1610\n",
      "Token 131: champion â†’ champion\n",
      "1611 1615\n",
      "Token 132: duke â†’ Duke\n",
      "1616 1620\n",
      "Token 133: blue â†’ Blue\n",
      "1621 1627\n",
      "Token 134: devils â†’ Devils\n",
      "1628 1633\n",
      "Token 135: twice â†’ twice\n",
      "1634 1640\n",
      "Token 136: during â†’ during\n",
      "1641 1644\n",
      "Token 137: the â†’ the\n",
      "1645 1651\n",
      "Token 138: season â†’ season\n",
      "1651 1652\n",
      "Token 139: . â†’ .\n",
      "1653 1656\n",
      "Token 140: the â†’ The\n",
      "1657 1659\n",
      "Token 141: 32 â†’ 32\n",
      "1660 1664\n",
      "Token 142: wins â†’ wins\n",
      "1665 1669\n",
      "Token 143: were â†’ were\n",
      "1670 1673\n",
      "Token 144: the â†’ the\n",
      "1674 1678\n",
      "Token 145: most â†’ most\n",
      "1679 1681\n",
      "Token 146: by â†’ by\n",
      "1682 1685\n",
      "Token 147: the â†’ the\n",
      "1686 1694\n",
      "Token 148: fighting â†’ Fighting\n",
      "1695 1700\n",
      "Token 149: irish â†’ Irish\n",
      "1701 1705\n",
      "Token 150: team â†’ team\n",
      "1706 1711\n",
      "Token 151: since â†’ since\n",
      "1712 1716\n",
      "Token 152: 1908 â†’ 1908\n",
      "1716 1717\n",
      "Token 153: - â†’ -\n",
      "1717 1719\n",
      "Token 154: 09 â†’ 09\n",
      "1719 1720\n",
      "Token 155: . â†’ .\n",
      "0 0\n",
      "Token 156: [SEP] â†’ \n"
     ]
    }
   ],
   "source": [
    "# éå†æ¯ä¸ªåˆ†å—\n",
    "for chunk_idx in range(len(tokenized_example[\"input_ids\"])):\n",
    "    print(f\"\\n=== åˆ†å— {chunk_idx} ===\")\n",
    "    \n",
    "    # è·å–å½“å‰åˆ†å—çš„æ•°æ®\n",
    "    input_ids = tokenized_example[\"input_ids\"][chunk_idx]\n",
    "    offset_mapping = tokenized_example[\"offset_mapping\"][chunk_idx]\n",
    "    token_type_ids = tokenized_example[\"token_type_ids\"][chunk_idx]\n",
    "\n",
    "    # éå†åˆ†å—å†…çš„æ¯ä¸ª token\n",
    "    for token_idx, (token_id, offset, token_type) in enumerate(zip(input_ids, offset_mapping, token_type_ids)):\n",
    "        # æ ¹æ® token_type é€‰æ‹©æ¥æºæ–‡æœ¬\n",
    "        if token_type == 0:\n",
    "            source_text = example[\"question\"]\n",
    "        else:\n",
    "            source_text = example[\"context\"]\n",
    "\n",
    "        # å…³é”®ä¿®å¤ç‚¹ï¼šåˆ†è§£ offset å…ƒç»„ä¸º start å’Œ end\n",
    "        start = offset[0]  # èµ·å§‹å­—ç¬¦ä½ç½®\n",
    "        end = offset[1]    # ç»“æŸå­—ç¬¦ä½ç½®\n",
    "        print(start, end)\n",
    "        original_text = source_text[start:end]\n",
    "        \n",
    "        # è½¬æ¢ token_id ä¸ºå¯è¯»æ–‡æœ¬\n",
    "        token_str = tokenizer.convert_ids_to_tokens([token_id])[0]  # å–åˆ—è¡¨ç¬¬ä¸€ä¸ªå…ƒç´ \n",
    "        \n",
    "        # æ‰“å°ç»“æœ\n",
    "        print(f\"Token {token_idx}: {token_str} â†’ {original_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç”¨æœ€ç®€å•çš„æ¯”å–»è§£é‡Šè¿™æ®µä»£ç ï¼š\n",
    "\n",
    "**1. åˆ†å—ï¼ˆåˆ‡ä¹¦ï¼‰**  \n",
    "- å°±åƒä¸€æœ¬åšä¹¦æ‹†æˆå‡ æœ¬å°å†Œå­ï¼Œæ¯æœ¬æœ€å¤š512é¡µï¼ˆæ¨¡å‹ä¸€æ¬¡è¯»ä¸å®Œé•¿æ–‡æœ¬ï¼‰\n",
    "\n",
    "**2. æ–‡å­—å˜æ•°å­—ï¼ˆåŠ å¯†ï¼‰**  \n",
    "- æŠŠæ¯ä¸ªå­—å˜æˆæ•°å­—å¯†ç ï¼Œæ¯”å¦‚ \"è´\"â†’100ï¼Œ\"çˆ·\"â†’101  \n",
    "- `input_ids` å°±æ˜¯è¿™äº›å¯†ç ç»„æˆçš„åˆ—è¡¨ï¼š[100, 101, ...]\n",
    "\n",
    "**3. è®°ä½ç½®ï¼ˆä¹¦ç­¾ï¼‰**  \n",
    "- `offset_mapping` è®°å½•æ¯ä¸ªå¯†ç åœ¨åŸæ–‡çš„ä½ç½®ï¼Œæ¯”å¦‚ (0,2) è¡¨ç¤ºå‰ä¸¤ä¸ªå­—\n",
    "\n",
    "**4. åŒºåˆ†é—®é¢˜å’Œç­”æ¡ˆï¼ˆè´´æ ‡ç­¾ï¼‰**  \n",
    "- `token_type_ids=0` è¡¨ç¤ºæ–‡å­—æ¥è‡ªé—®é¢˜ï¼ˆå¦‚ \"è´çˆ·å“ªå¹´ç»“å©šï¼Ÿ\"ï¼‰  \n",
    "- `token_type_ids=1` è¡¨ç¤ºæ–‡å­—æ¥è‡ªç­”æ¡ˆï¼ˆå¦‚ \"2000å¹´...\"ï¼‰\n",
    "\n",
    "**5. æ‰¾å¯¹åº”æ–‡å­—ï¼ˆè§£å¯†ï¼‰**  \n",
    "- ç”¨å¯†ç æœ¬æŠŠæ•°å­—è½¬å›æ–‡å­—  \n",
    "- æ ¹æ®ä½ç½®æ ‡ç­¾ï¼Œä»é—®é¢˜æˆ–ç­”æ¡ˆæ–‡æœ¬æˆªå–å¯¹åº”æ–‡å­—\n",
    "\n",
    "**å°±åƒè¿™æ ·ï¼š**  \n",
    "å¯†ç  `100` â†’ æŸ¥å¯†ç æœ¬ â†’ æ˜¯\"è´\" â†’ åœ¨é—®é¢˜ç¬¬0-2ä¸ªä½ç½® â†’ æˆªå–\"è´çˆ·\"\n",
    "\n",
    "æ•´ä¸ªè¿‡ç¨‹è®©è®¡ç®—æœºåƒäººç±»ä¸€æ ·ï¼šå…ˆçœ‹é—®é¢˜ï¼Œå†å¿«é€Ÿç¿»ä¹¦æ‰¾ç­”æ¡ˆä½ç½®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How many wins does the Notre Dame men's basketball team have?\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009â€“2010 season. The team is coached by Mike Brey, who, as of the 2014â€“15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010â€“11 team concluded its regular season ranked number seven in the country, with a record of 25â€“5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\""
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å€ŸåŠ©`tokenized_example`çš„`sequence_ids`æ–¹æ³•ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿çš„åŒºåˆ†tokençš„æ¥æºç¼–å·ï¼š\n",
    "\n",
    "- å¯¹äºç‰¹æ®Šæ ‡è®°ï¼šè¿”å›Noneï¼Œ\n",
    "- å¯¹äºæ­£æ–‡Tokenï¼šè¿”å›å¥å­ç¼–å·ï¼ˆä»0å¼€å§‹ç¼–å·ï¼‰ã€‚\n",
    "\n",
    "ç»¼ä¸Šï¼Œç°åœ¨æˆ‘ä»¬å¯ä»¥å¾ˆæ–¹ä¾¿çš„åœ¨ä¸€ä¸ªè¾“å…¥ç‰¹å¾ä¸­æ‰¾åˆ°ç­”æ¡ˆçš„èµ·å§‹å’Œç»“æŸ Tokenã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **`sequence_ids`**ï¼š\n",
    "\n",
    "---\n",
    "\n",
    "### **ç±»æ¯”åœºæ™¯**\n",
    "æƒ³è±¡ä½ åœ¨ç©ä¸€ä¸ª**åŒè‰²è§å…‰ç¬”æ ‡è®°**çš„æ¸¸æˆï¼š\n",
    "- **é»„è‰²**ï¼šæ ‡è®°é—®é¢˜ï¼ˆæ¯”å¦‚ï¼š\"è´çˆ·å“ªå¹´ç»“å©šï¼Ÿ\"ï¼‰\n",
    "- **è“è‰²**ï¼šæ ‡è®°ä¹¦ä¸­çš„ç­”æ¡ˆæ®µè½ï¼ˆæ¯”å¦‚ä¹¦é‡Œå†™ï¼š\"è´çˆ·2008å¹´ç»“å©š...\"ï¼‰\n",
    "- **çº¢è‰²**ï¼šæ ‡è®°ç‰¹æ®Šç¬¦å·ï¼ˆæ¯”å¦‚ä¹¦çš„å°é¢ã€ç« èŠ‚åˆ†éš”é¡µï¼‰\n",
    "\n",
    "`sequence_ids` å°±æ˜¯ä¸€ä¸ª**é¢œè‰²ç¼–å·åˆ—è¡¨**ï¼Œå‘Šè¯‰ä½ æ¯ä¸ªå­—å±äºå“ªéƒ¨åˆ†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **ä¸‰ç§æ ‡è®°è§„åˆ™**\n",
    "1. **`None` â†’ çº¢è‰²æ ‡è®°**  \n",
    "   - å¯¹åº”ç‰¹æ®Šç¬¦å·ï¼š`[CLS]`ï¼ˆå¼€å¤´æ ‡å¿—ï¼‰ã€`[SEP]`ï¼ˆåˆ†éš”ç¬¦ï¼‰\n",
    "   - ä¾‹ï¼š`[CLS]` â†’ `None`\n",
    "\n",
    "2. **`0` â†’ é»„è‰²æ ‡è®°**  \n",
    "   - æ‰€æœ‰æ¥è‡ª**é—®é¢˜**çš„æ–‡å­—  \n",
    "   - ä¾‹ï¼š\"è´çˆ·\"ã€\"å“ªå¹´\" â†’ `0`\n",
    "\n",
    "3. **`1` â†’ è“è‰²æ ‡è®°**  \n",
    "   - æ‰€æœ‰æ¥è‡ª**ä¹¦æœ¬æ–‡æ¡£**çš„æ–‡å­—  \n",
    "   - ä¾‹ï¼š\"2008å¹´\"ã€\"ç»“å©š\" â†’ `1`\n",
    "\n",
    "---\n",
    "\n",
    "### **å®é™…æ•ˆæœç¤ºä¾‹**\n",
    "å‡è®¾é—®é¢˜å’Œæ–‡æ¡£ç»„åˆåï¼š\n",
    "```\n",
    "[CLS] è´çˆ·å“ªå¹´ç»“å©šï¼Ÿ [SEP] è´çˆ·2008å¹´ä¸Jay-Zç»“å©š... [SEP]\n",
    "```\n",
    "\n",
    "å¯¹åº”çš„ `sequence_ids` å°±åƒè¿™æ ·ï¼š\n",
    "```\n",
    "[ None, 0,0,0,0, None, 1,1,1,1,1, None ]\n",
    "```\n",
    "å¯è§†åŒ–æ ‡è®°ï¼š\n",
    "```\n",
    "çº¢è‰² [CLS] â†’ é»„é»„é»„é»„ â†’ çº¢è‰² [SEP] â†’ è“è“è“è“è“ â†’ çº¢è‰² [SEP]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **æ ¸å¿ƒç”¨é€”**\n",
    "1. **å¿«é€Ÿå®šä½ç­”æ¡ˆèŒƒå›´**  \n",
    "   ```python\n",
    "   # æ‰¾åˆ°æ–‡æ¡£éƒ¨åˆ†çš„èµ·æ­¢ä½ç½®\n",
    "   start = sequence_ids.index(1)                 # ç¬¬ä¸€ä¸ªè“è‰²æ ‡è®°çš„ä½ç½®\n",
    "   end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1  # æœ€åä¸€ä¸ªè“è‰²æ ‡è®°\n",
    "   ```\n",
    "\n",
    "2. **è¿‡æ»¤æ— æ•ˆå†…å®¹**  \n",
    "   ```python\n",
    "   # åªå¤„ç†æ–‡æ¡£éƒ¨åˆ†çš„æ–‡å­—\n",
    "   if sequence_ids[i] == 1:\n",
    "       print(\"è¿™æ˜¯ä¹¦é‡Œçš„å†…å®¹ï¼\")\n",
    "   ```\n",
    "\n",
    "3. **å¤„ç†é•¿æ–‡æœ¬åˆ†å—**  \n",
    "   - å½“æ–‡æ¡£å¤ªé•¿æ—¶ï¼Œè‡ªåŠ¨åˆ†æˆå¤šå—ï¼Œæ¯å—éƒ½æœ‰è‡ªå·±çš„ `sequence_ids`\n",
    "   - ä¾‹ï¼šåˆ†å—1çš„è“è‰²æ ‡è®°å¯¹åº”æ–‡æ¡£å‰åŠéƒ¨åˆ†ï¼Œåˆ†å—2å¯¹åº”ååŠéƒ¨åˆ†\n",
    "\n",
    "---\n",
    "\n",
    "### **ä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ**\n",
    "å°±åƒè¯»ä¹¦æ—¶ç”¨è§å…‰ç¬”åˆ’é‡ç‚¹ï¼š\n",
    "- **é»„è‰²**ï¼šæ˜ç¡®é—®é¢˜ï¼ˆçŸ¥é“è¦æ‰¾ä»€ä¹ˆï¼‰\n",
    "- **è“è‰²**ï¼šå¿«é€Ÿé”å®šç­”æ¡ˆåŒºåŸŸï¼ˆä¸ç”¨è¯»å®Œæ•´æœ¬ä¹¦ï¼‰\n",
    "- **çº¢è‰²**ï¼šå¿½ç•¥æ— å…³çš„å°é¢/åˆ†éš”é¡µ\n",
    "\n",
    "è¿™è®©æ¨¡å‹åƒäººç±»ä¸€æ ·ï¼šå…ˆçœ‹é—®é¢˜ï¼Œå†å¿«é€Ÿç¿»ä¹¦æ‰¾ç­”æ¡ˆä½ç½®ï¼Œè€Œä¸æ˜¯å‚»å‚»é€šè¯»å…¨æ–‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç”Ÿæˆåˆ†å—æ•°: 2\n",
      "\n",
      "=== åˆ†å— 0 ===\n",
      "Tokenæ•°é‡: 384\n",
      "sequence_idsç»“æ„: [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1]...\n",
      "é—®é¢˜éƒ¨åˆ†è¦†ç›–çš„tokenä½ç½®: [1, 2, 3, 4, 5]...\n",
      "\n",
      "=== åˆ†å— 1 ===\n",
      "Tokenæ•°é‡: 157\n",
      "sequence_idsç»“æ„: [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1]...\n",
      "é—®é¢˜éƒ¨åˆ†è¦†ç›–çš„tokenä½ç½®: [1, 2, 3, 4, 5]...\n"
     ]
    }
   ],
   "source": [
    "# æ£€æŸ¥åˆ†å—æ•°é‡\n",
    "num_chunks = len(tokenized_example[\"input_ids\"])\n",
    "print(f\"ç”Ÿæˆåˆ†å—æ•°: {num_chunks}\")\n",
    "\n",
    "# éå†æ¯ä¸ªåˆ†å—\n",
    "for chunk_idx in range(num_chunks):\n",
    "    print(f\"\\n=== åˆ†å— {chunk_idx} ===\")\n",
    "    \n",
    "    # æ­£ç¡®è·å–å½“å‰åˆ†å—çš„æ•°æ®\n",
    "    chunk_input_ids = tokenized_example[\"input_ids\"][chunk_idx]\n",
    "    chunk_sequence_ids = tokenized_example.sequence_ids(chunk_idx)  # å…³é”®ä¿®å¤ç‚¹\n",
    "    \n",
    "    # æ‰“å°å…³é”®ä¿¡æ¯\n",
    "    print(f\"Tokenæ•°é‡: {len(chunk_input_ids)}\")\n",
    "    print(f\"sequence_idsç»“æ„: {chunk_sequence_ids[:20]}...\")  # æ‰“å°å‰20ä¸ªå…ƒç´ \n",
    "    \n",
    "    # æ£€æŸ¥é—®é¢˜éƒ¨åˆ†æ˜¯å¦å®Œæ•´\n",
    "    question_segment = [i for i, sid in enumerate(chunk_sequence_ids) if sid == 0]\n",
    "    print(f\"é—®é¢˜éƒ¨åˆ†è¦†ç›–çš„tokenä½ç½®: {question_segment[:5]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['over 1,600'], 'answer_start': [30]}\n",
      "30\n",
      "40\n",
      "23 26\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "print(answers)\n",
    "print(start_char)\n",
    "print(end_char)\n",
    "# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹æ ‡è®°ç´¢å¼•ã€‚\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# å½“å‰spanåœ¨æ–‡æœ¬ä¸­çš„ç»“æŸæ ‡è®°ç´¢å¼•ã€‚\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºspanèŒƒå›´ï¼ˆå¦‚æœè¶…å‡ºèŒƒå›´ï¼Œè¯¥ç‰¹å¾å°†ä»¥CLSæ ‡è®°ç´¢å¼•æ ‡è®°ï¼‰ã€‚\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # å°†token_start_indexå’Œtoken_end_indexç§»åŠ¨åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚\n",
    "    # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼Œæˆ‘ä»¬å¯ä»¥ç§»åˆ°æœ€åä¸€ä¸ªæ ‡è®°ä¹‹åï¼ˆè¾¹ç•Œæƒ…å†µï¼‰ã€‚\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"ç­”æ¡ˆä¸åœ¨æ­¤ç‰¹å¾ä¸­ã€‚\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°æ£€æŸ¥æ˜¯å¦å‡†ç¡®æ‰¾åˆ°äº†èµ·å§‹ä½ç½®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 1, 600\n",
      "over 1,600\n"
     ]
    }
   ],
   "source": [
    "# é€šè¿‡æŸ¥æ‰¾ offset mapping ä½ç½®ï¼Œè§£ç  context ä¸­çš„ç­”æ¡ˆ \n",
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "# ç›´æ¥æ‰“å° æ•°æ®é›†ä¸­çš„æ ‡å‡†ç­”æ¡ˆï¼ˆanswer[\"text\"])\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å…³äºå¡«å……çš„ç­–ç•¥\n",
    "\n",
    "- å¯¹äºæ²¡æœ‰è¶…è¿‡æœ€å¤§é•¿åº¦çš„æ–‡æœ¬ï¼Œå¡«å……è¡¥é½é•¿åº¦ã€‚\n",
    "- å¯¹äºéœ€è¦å·¦ä¾§å¡«å……çš„æ¨¡å‹ï¼Œäº¤æ¢ question å’Œ context é¡ºåº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•´åˆä»¥ä¸Šæ‰€æœ‰é¢„å¤„ç†æ­¥éª¤\n",
    "\n",
    "è®©æˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹æ•´åˆåˆ°ä¸€ä¸ªå‡½æ•°ä¸­ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°è®­ç»ƒé›†ã€‚\n",
    "\n",
    "é’ˆå¯¹ä¸å¯å›ç­”çš„æƒ…å†µï¼ˆä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œç­”æ¡ˆåœ¨å¦ä¸€ä¸ªç‰¹å¾ä¸­ï¼‰ï¼Œæˆ‘ä»¬ä¸ºå¼€å§‹å’Œç»“æŸä½ç½®éƒ½è®¾ç½®äº†clsç´¢å¼•ã€‚\n",
    "\n",
    "å¦‚æœallow_impossible_answersæ ‡å¿—ä¸ºFalseï¼Œæˆ‘ä»¬è¿˜å¯ä»¥ç®€å•åœ°ä»è®­ç»ƒé›†ä¸­ä¸¢å¼ƒè¿™äº›ç¤ºä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§å¯èƒ½æœ‰å¾ˆå¤šç©ºç™½å­—ç¬¦ï¼Œè¿™å¯¹æˆ‘ä»¬æ²¡æœ‰ç”¨ï¼Œè€Œä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡çš„æˆªæ–­å¤±è´¥\n",
    "    # ï¼ˆæ ‡è®°åŒ–çš„é—®é¢˜å°†å ç”¨å¤§é‡ç©ºé—´ï¼‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åˆ é™¤å·¦ä¾§çš„ç©ºç™½å­—ç¬¦ã€‚\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œæ ‡è®°åŒ–ï¼Œä½†ä¿ç•™æº¢å‡ºéƒ¨åˆ†ï¼Œä½¿ç”¨æ­¥å¹…ï¼ˆstrideï¼‰ã€‚\n",
    "    # å½“ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶ï¼Œè¿™ä¼šå¯¼è‡´ä¸€ä¸ªç¤ºä¾‹å¯èƒ½æä¾›å¤šä¸ªç‰¹å¾ï¼Œå…¶ä¸­æ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡æœ‰ä¸€äº›é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹å¯èƒ½ç»™æˆ‘ä»¬æä¾›å¤šä¸ªç‰¹å¾ï¼ˆå¦‚æœå®ƒå…·æœ‰å¾ˆé•¿çš„ä¸Šä¸‹æ–‡ï¼‰ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æä¾›äº†è¿™ä¸ªæ˜ å°„å…³ç³»ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # åç§»æ˜ å°„å°†ä¸ºæˆ‘ä»¬æä¾›ä»ä»¤ç‰Œåˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚è¿™å°†å¸®åŠ©æˆ‘ä»¬è®¡ç®—å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®ã€‚\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # è®©æˆ‘ä»¬ä¸ºè¿™äº›ç¤ºä¾‹è¿›è¡Œæ ‡è®°ï¼\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # æˆ‘ä»¬å°†ä½¿ç”¨ CLS ç‰¹æ®Š token çš„ç´¢å¼•æ¥æ ‡è®°ä¸å¯èƒ½çš„ç­”æ¡ˆã€‚\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£ä¸Šä¸‹æ–‡å’Œé—®é¢˜æ˜¯ä»€ä¹ˆï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥æä¾›å¤šä¸ªè·¨åº¦ï¼Œè¿™æ˜¯åŒ…å«æ­¤æ–‡æœ¬è·¨åº¦çš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # å¦‚æœæ²¡æœ‰ç»™å‡ºç­”æ¡ˆï¼Œåˆ™å°†cls_indexè®¾ç½®ä¸ºç­”æ¡ˆã€‚\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹å’Œç»“æŸå­—ç¬¦ç´¢å¼•ã€‚\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„å¼€å§‹ä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # å½“å‰è·¨åº¦åœ¨æ–‡æœ¬ä¸­çš„ç»“æŸä»¤ç‰Œç´¢å¼•ã€‚\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # æ£€æµ‹ç­”æ¡ˆæ˜¯å¦è¶…å‡ºè·¨åº¦ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè¯¥ç‰¹å¾çš„æ ‡ç­¾å°†ä½¿ç”¨CLSç´¢å¼•ï¼‰ã€‚\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # å¦åˆ™ï¼Œå°†token_start_indexå’Œtoken_end_indexç§»åˆ°ç­”æ¡ˆçš„ä¸¤ç«¯ã€‚\n",
    "                # æ³¨æ„ï¼šå¦‚æœç­”æ¡ˆæ˜¯æœ€åä¸€ä¸ªå•è¯ï¼ˆè¾¹ç¼˜æƒ…å†µï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨æœ€åä¸€ä¸ªåç§»ä¹‹åç»§ç»­ã€‚\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **åŠŸèƒ½ç›®æ ‡**\n",
    "è¿™ä¸ªå‡½æ•°å°±åƒä¸€ä½ **æ•°æ®åŠ å·¥å‚çš„æµæ°´çº¿å·¥äºº**ï¼Œè´Ÿè´£æŠŠåŸå§‹é—®ç­”æ•°æ®æ”¹é€ æˆé€‚åˆæ¨¡å‹ç†è§£çš„æ ¼å¼ã€‚ä¸»è¦è§£å†³ä¸¤ä¸ªé—®é¢˜ï¼š\n",
    "1. **é•¿æ–‡æœ¬åˆ‡å‰²**ï¼šå½“ç­”æ¡ˆæ–‡ç« å¤ªé•¿æ—¶ï¼Œåˆ‡æˆå¤šä¸ªçŸ­å—ï¼ˆç±»ä¼¼å°†é•¿è§†é¢‘åˆ†æ®µï¼‰\n",
    "2. **ç­”æ¡ˆå®šä½**ï¼šåœ¨æ¯ä¸ªçŸ­å—ä¸­æ ‡æ³¨ç­”æ¡ˆçš„ä½ç½®ï¼ˆç±»ä¼¼è§†é¢‘å‰ªè¾‘æ—¶æ ‡è®°ç²¾å½©ç‰‡æ®µçš„èµ·æ­¢æ—¶é—´ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **æ ¸å¿ƒå¤„ç†æ­¥éª¤**\n",
    "\n",
    "#### **1. æ¸…ç†é—®é¢˜æ–‡å­—ï¼ˆå»å·¦ç©ºæ ¼ï¼‰**\n",
    "- **é—®é¢˜**ï¼šç”¨æˆ·æé—®å¯èƒ½åŒ…å«å¤šä½™ç©ºæ ¼ï¼Œä¾‹å¦‚ `\"Â Â  Beyonceå“ªå¹´ç»“å©šï¼Ÿ\"`\n",
    "- **å¤„ç†**ï¼šå»æ‰å·¦è¾¹çš„ç©ºæ ¼ â†’ `\"Beyonceå“ªå¹´ç»“å©šï¼Ÿ\"`\n",
    "- **åŸå› **ï¼šé˜²æ­¢ç©ºæ ¼å ç”¨åˆ†è¯åé¢ï¼Œå¯¼è‡´æ­£æ–‡è¢«è¿‡åº¦æˆªæ–­\n",
    "\n",
    "#### **2. æ–‡æœ¬åˆ†å—å¤„ç†**\n",
    "- **æ“ä½œ**ï¼šå°†é•¿æ–‡ç« åˆ‡æˆå¤šä¸ªå°å—ï¼ˆæ¯å—æœ€é•¿ `max_length`ï¼Œå—é—´é‡å  `stride`ï¼‰\n",
    "- **ç¤ºä¾‹**ï¼š\n",
    "  ```\n",
    "  åŸæ–‡ç« ï¼šæ®µè½1...æ®µè½2...æ®µè½3...ï¼ˆæ€»é•¿è¶…è¿‡max_lengthï¼‰\n",
    "  åˆ†å—1ï¼šæ®µè½1...æ®µè½2ï¼ˆå‰åŠï¼‰\n",
    "  åˆ†å—2ï¼šæ®µè½2ï¼ˆååŠï¼‰...æ®µè½3\n",
    "  ```\n",
    "\n",
    "#### **3. è®°å½•åˆ†å—å…³ç³»**\n",
    "- **overflow_to_sample_mapping**ï¼šè®°å½•æ¯ä¸ªåˆ†å—å±äºå“ªä¸ªåŸå§‹æ ·æœ¬  \n",
    "  ï¼ˆç±»ä¼¼å¿«é€’åˆ†ç®±æ—¶åœ¨æ¯ç®±è´´åŸè®¢å•å·ï¼‰\n",
    "- **offset_mapping**ï¼šè®°å½•æ¯ä¸ªåˆ†è¯å¯¹åº”çš„åŸå§‹å­—ç¬¦ä½ç½®  \n",
    "  ï¼ˆç±»ä¼¼æ¯å—ç§¯æœ¨å¯¹åº”åŸå›¾çº¸çš„ä½ç½®ï¼‰\n",
    "\n",
    "#### **4. å¤„ç†æ— ç­”æ¡ˆæƒ…å†µ**\n",
    "- **åœºæ™¯**ï¼šå½“ç­”æ¡ˆä¸åœ¨å½“å‰åˆ†å—ä¸­ï¼ˆä¾‹å¦‚ç­”æ¡ˆåœ¨å¦ä¸€ä¸ªåˆ†å—é‡Œï¼‰\n",
    "- **æ ‡è®°**ï¼šå°†ç­”æ¡ˆä½ç½®è®¾ä¸º `[CLS]` çš„ä½ç½®ï¼ˆæ¨¡å‹çœ‹åˆ°è¿™ä¸ªå°±çŸ¥é“å½“å‰å—æ— ç­”æ¡ˆï¼‰\n",
    "\n",
    "#### **5. ç²¾ç¡®å®šä½ç­”æ¡ˆ**\n",
    "- **æ­¥éª¤**ï¼š\n",
    "  1. **ç¡®å®šç­”æ¡ˆå­—ç¬¦èŒƒå›´**ï¼š`start_char` åˆ° `end_char`\n",
    "  2. **æ‰¾åˆ°åˆ†å—çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†**ï¼ˆè·³è¿‡é—®é¢˜å’Œç‰¹æ®Šæ ‡è®°ï¼‰\n",
    "  3. **æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨æœ¬åˆ†å—**ï¼š\n",
    "     - æ˜¯ â†’ è°ƒæ•´åˆ°ç²¾ç¡®çš„åˆ†è¯ä½ç½®\n",
    "     - å¦ â†’ æ ‡è®°ä¸º `[CLS]`\n",
    "\n",
    "---\n",
    "\n",
    "### **å®é™…æ¡ˆä¾‹æ¼”ç¤º**\n",
    "**è¾“å…¥æ•°æ®**ï¼š\n",
    "```python\n",
    "{\n",
    "    \"question\": \"Beyonceå“ªå¹´ç»“å©šï¼Ÿ\",\n",
    "    \"context\": \"Beyonceäº2008å¹´ä¸Jay-Zç»“å©š...ï¼ˆé•¿æ–‡æœ¬ï¼‰\",\n",
    "    \"answers\": {\"text\": [\"2008å¹´\"], \"answer_start\": }\n",
    "}\n",
    "```\n",
    "\n",
    "**å¤„ç†è¿‡ç¨‹**ï¼š\n",
    "1. **åˆ†å—**ï¼šå°†é•¿ `context` åˆ†æˆä¸¤ä¸ªå—\n",
    "2. **å—1å¤„ç†**ï¼š\n",
    "   - å‘ç°ç­”æ¡ˆ `2008å¹´` åœ¨å—1ä¸­\n",
    "   - æ ‡æ³¨èµ·å§‹ä½ç½®ä¸º `token 6`ï¼Œç»“æŸä½ç½®ä¸º `token 7`\n",
    "3. **å—2å¤„ç†**ï¼š\n",
    "   - å—2ä¸åŒ…å«ç­”æ¡ˆ â†’ æ ‡æ³¨ä¸º `[CLS]`\n",
    "\n",
    "**è¾“å‡ºç‰¹å¾**ï¼š\n",
    "```python\n",
    "{\n",
    "    \"input_ids\": [101, 2345, 3456, ..., 102],  # åˆ†å—åçš„token\n",
    "    \"start_positions\": 6, \n",
    "    \"end_positions\": 7\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **å‚æ•°æ§åˆ¶è¡Œä¸º**\n",
    "| å‚æ•° | ä½œç”¨ | ç±»æ¯”è§£é‡Š |\n",
    "|------|------|----------|\n",
    "| `max_length=384` | æ¯å—æœ€å¤§é•¿åº¦ | æ¯æ®µè§†é¢‘æœ€é•¿5åˆ†é’Ÿ |\n",
    "| `stride=128` | åˆ†å—é—´é‡å é•¿åº¦ | ä¸¤æ®µè§†é¢‘é—´é‡å 30ç§’é˜²æ­¢æ¼å†…å®¹ |\n",
    "| `pad_on_right=True` | é—®é¢˜åœ¨å³/å·¦å¡«å…… | å­—å¹•åœ¨è§†é¢‘å·¦ä¸‹æ–¹è¿˜æ˜¯å³ä¸‹æ–¹ |\n",
    "\n",
    "---\n",
    "\n",
    "### **æ€»ç»“**\n",
    "è¿™ä¸ªå‡½æ•°å°±åƒä¸€ä½æ™ºèƒ½å‰ªè¾‘å¸ˆï¼š\n",
    "1. **åˆ‡åˆ†é•¿è§†é¢‘**ï¼ˆåˆ†å—å¤„ç†ï¼‰\n",
    "2. **æ ‡è®°å…³é”®ç‰‡æ®µ**ï¼ˆç­”æ¡ˆå®šä½ï¼‰\n",
    "3. **å¤„ç†ç‰¹æ®Šæƒ…å†µ**ï¼ˆæ— ç­”æ¡ˆæ—¶æ‰“æ ‡è®°ï¼‰\n",
    "\n",
    "æœ€ç»ˆè¾“å‡ºæ¨¡å‹å¯ä»¥ç›´æ¥å­¦ä¹ çš„æ ‡å‡†åŒ–æ•°æ®æ ¼å¼ï¼Œæ˜¯è®­ç»ƒé«˜è´¨é‡é—®ç­”æ¨¡å‹çš„å…³é”®é¢„å¤„ç†æ­¥éª¤ï¼ ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "#### datasets.map çš„è¿›é˜¶ä½¿ç”¨\n",
    "\n",
    "ä½¿ç”¨ `datasets.map` æ–¹æ³•å°† `prepare_train_features` åº”ç”¨äºæ‰€æœ‰è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®ï¼š\n",
    "\n",
    "- batched: æ‰¹é‡å¤„ç†æ•°æ®ã€‚\n",
    "- remove_columns: å› ä¸ºé¢„å¤„ç†æ›´æ”¹äº†æ ·æœ¬çš„æ•°é‡ï¼Œæ‰€ä»¥åœ¨åº”ç”¨å®ƒæ—¶éœ€è¦åˆ é™¤æ—§åˆ—ã€‚\n",
    "- load_from_cache_fileï¼šæ˜¯å¦ä½¿ç”¨datasetsåº“çš„è‡ªåŠ¨ç¼“å­˜\n",
    "\n",
    "datasets åº“é’ˆå¯¹å¤§è§„æ¨¡æ•°æ®ï¼Œå®ç°äº†é«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹ä¼ é€’ç»™ map çš„å‡½æ•°æ˜¯å¦å·²æ›´æ”¹ï¼ˆå› æ­¤éœ€è¦ä¸ä½¿ç”¨ç¼“å­˜æ•°æ®ï¼‰ã€‚å¦‚æœåœ¨è°ƒç”¨ map æ—¶è®¾ç½® `load_from_cache_file=False`ï¼Œå¯ä»¥å¼ºåˆ¶é‡æ–°åº”ç”¨é¢„å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **æ ¸å¿ƒæµç¨‹ç±»æ¯”**\n",
    "æƒ³è±¡ä½ ç»è¥ä¸€ä¸ª **å¤§å‹å¿«é€’åˆ†æ‹£ä¸­å¿ƒ**ï¼Œéœ€è¦å¤„ç†ä¸‰ç§åŒ…è£¹ï¼ˆè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ï¼‰ã€‚`datasets.map` å°±æ˜¯ä½ çš„ **è‡ªåŠ¨åŒ–åˆ†æ‹£æµæ°´çº¿**ï¼Œ`prepare_train_features` æ˜¯ä½ å®šåˆ¶çš„ **æ™ºèƒ½åˆ†æ‹£è§„åˆ™**ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "### **åˆ†æ‹£çº¿å‚æ•°è§£æ**\n",
    "```python\n",
    "tokenized_datasets = datasets.map(\n",
    "    prepare_train_features,  # ä½ çš„æ™ºèƒ½åˆ†æ‹£è§„åˆ™\n",
    "    batched=True,            # æ•´ç®±å¤„ç†ï¼ˆè€Œä¸æ˜¯å•ä»¶ï¼‰\n",
    "    remove_columns=åŸå§‹åŒ…è£¹æ ‡ç­¾  # æ’•æ‰æ—§æ ‡ç­¾\n",
    ")\n",
    "```\n",
    "\n",
    "#### 1. **`batched=True` â†’ æ•´ç®±å¤„ç†æ¨¡å¼**\n",
    "- **ä¼ ç»Ÿæ–¹å¼**ï¼šå·¥äººé€ä¸ªæ£€æŸ¥åŒ…è£¹ï¼ˆå•æ¡æ•°æ®å¤„ç†ï¼‰\n",
    "- **é«˜æ•ˆæ¨¡å¼**ï¼šæ•´ç®±å€’è¿›æœºå™¨ï¼ŒåŒæ—¶å¤„ç†æ•°ç™¾ä¸ªåŒ…è£¹ï¼ˆæ‰¹é‡å¤„ç†ï¼‰\n",
    "- **ä¼˜åŠ¿**ï¼šé€Ÿåº¦æå‡ 10-100 å€ï¼Œç‰¹åˆ«é€‚åˆ GPU å¹¶è¡Œè®¡ç®—\n",
    "\n",
    "#### 2. **`remove_columns` â†’ æ¸…é™¤æ—§æ ‡ç­¾**\n",
    "- **åŸå› **ï¼šç»è¿‡åˆ†æ‹£åï¼ŒåŒ…è£¹å½¢çŠ¶æ”¹å˜ï¼ˆæ•°æ®åˆ—å˜åŒ–ï¼‰\n",
    "- **æ“ä½œ**ï¼š\n",
    "  - åŸå§‹æ ‡ç­¾ï¼šå‘ä»¶äººã€æ”¶ä»¶äººï¼ˆ`question`, `context` ç­‰ï¼‰\n",
    "  - æ–°æ ‡ç­¾ï¼šç›®çš„åœ°ä»£ç ã€é‡é‡åˆ†çº§ï¼ˆ`input_ids`, `attention_mask` ç­‰ï¼‰\n",
    "- **ç¤ºä¾‹**ï¼šå°±åƒå¿«é€’é‡æ–°åŒ…è£…åï¼Œéœ€è¦å»æ‰æ—§é¢å•\n",
    "\n",
    "#### 3. **ç¼“å­˜æœºåˆ¶ â†’ æ™ºèƒ½æš‚å­˜åŒº**\n",
    "- **è‡ªåŠ¨æ£€æµ‹**ï¼šå¦‚æœåˆ†æ‹£è§„åˆ™æ²¡å˜ï¼Œç›´æ¥ä½¿ç”¨æš‚å­˜åŒºå¤„ç†å¥½çš„åŒ…è£¹\n",
    "- **å¼ºåˆ¶åˆ·æ–°**ï¼š`load_from_cache_file=False` å°±åƒè¦æ±‚ã€Œä¸ç®¡æœ‰æ²¡æœ‰æ—§åŒ…è£¹ï¼Œå…¨éƒ¨é‡æ–°åˆ†æ‹£ã€\n",
    "- **ä¼˜åŠ¿**ï¼šèŠ‚çœ 70% ä»¥ä¸Šæ—¶é—´ï¼Œé¿å…é‡å¤åŠ³åŠ¨\n",
    "\n",
    "---\n",
    "\n",
    "### **å®Œæ•´å·¥ä½œæµç¨‹**\n",
    "1. **æ”¶åŒ…è£¹**ï¼šä¸‰ç§ç±»å‹åŒ…è£¹è¿›å…¥æµæ°´çº¿ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ï¼‰\n",
    "2. **è§„åˆ™åº”ç”¨**ï¼š\n",
    "   - æ™ºèƒ½åˆ‡å‰²å¤§åŒ…è£¹ï¼ˆé•¿æ–‡æœ¬åˆ†å—ï¼‰\n",
    "   - è´´ä¸Šç²¾å‡†ç›®çš„åœ°æ ‡ç­¾ï¼ˆç­”æ¡ˆä½ç½®æ ‡è®°ï¼‰\n",
    "   - ä¸¢å¼ƒç ´æŸåŒ…è£¹ï¼ˆæ— æ•ˆæ ·æœ¬ï¼‰\n",
    "3. **è¾“å‡ºç»“æœ**ï¼š\n",
    "   - æ ‡å‡†åŒ–å¿«é€’ç®±ï¼ˆæ¨¡å‹å¯è¯»çš„ `input_ids` ç­‰ï¼‰\n",
    "   - ç²¾å‡†ç‰©æµæ ‡ç­¾ï¼ˆ`start_positions`, `end_positions`ï¼‰\n",
    "\n",
    "---\n",
    "\n",
    "### **æŠ€æœ¯ç»†èŠ‚å¯¹åº”**\n",
    "| å¿«é€’åœºæ™¯ | æ•°æ®å¤„ç† |\n",
    "|---------|----------|\n",
    "| åŒ…è£¹ç±»å‹åŒºåˆ† | ä¿æŒè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†ç»“æ„ |\n",
    "| åˆ†æ‹£æœºå™¨äºº | `prepare_train_features` å‡½æ•° |\n",
    "| æ•´ç®±å¤„ç† | æ‰¹é‡çŸ©é˜µè¿ç®— |\n",
    "| æš‚å­˜åŒº | Hugging Face çš„ç¼“å­˜æ–‡ä»¶ï¼ˆé€šå¸¸å­˜äº ~/.cache/huggingface/datasetsï¼‰|\n",
    "\n",
    "---\n",
    "\n",
    "### **ä¸ºä»€ä¹ˆéœ€è¦è¿™æ ·è®¾è®¡ï¼Ÿ**\n",
    "1. **æ•ˆç‡ä¼˜å…ˆ**ï¼šå¦‚åŒå¿«é€’è¡Œä¸šè¿½æ±‚æ¯æ—¥ç™¾ä¸‡ä»¶å¤„ç†é‡ï¼Œæ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒå°±æ˜¯ **å¤§è§„æ¨¡æ•°æ®åå**\n",
    "2. **èµ„æºç®¡ç†**ï¼šç¼“å­˜æœºåˆ¶åƒåŒåä¸€çš„é¢„å”®åŒ…è£…ï¼Œæå‰å®Œæˆéƒ¨åˆ†å·¥ä½œå‡è½»é«˜å³°å‹åŠ›\n",
    "3. **è´¨é‡ç®¡æ§**ï¼š`remove_columns` ç¡®ä¿ä¸ä¼šæŠŠç”Ÿé²œå’Œæ™®é€šåŒ…è£¹æ··æ·†ï¼ˆé˜²æ­¢æ•°æ®æ±¡æŸ“ï¼‰\n",
    "\n",
    "é€šè¿‡è¿™å¥—ç³»ç»Ÿï¼Œä½ çš„æ¨¡å‹å°±åƒé«˜æ•ˆçš„ç‰©æµç½‘ç»œï¼Œèƒ½å¿«é€Ÿå‡†ç¡®åœ°å°†ã€Œé—®é¢˜åŒ…è£¹ã€é€è¾¾ã€Œç­”æ¡ˆç›®çš„åœ°ã€ï¼ğŸššâœ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058a7843bd524904b48dacd181f13666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c35c3f2cee456dad6bdb82f1359e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## å¾®è°ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "ç°åœ¨æˆ‘ä»¬çš„æ•°æ®å·²ç»å‡†å¤‡å¥½ç”¨äºè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹å¹¶è¿›è¡Œå¾®è°ƒã€‚\n",
    "\n",
    "ç”±äºæˆ‘ä»¬çš„ä»»åŠ¡æ˜¯é—®ç­”ï¼Œæˆ‘ä»¬ä½¿ç”¨ `AutoModelForQuestionAnswering` ç±»ã€‚(å¯¹æ¯” Yelp è¯„è®ºæ‰“åˆ†ä½¿ç”¨çš„æ˜¯ `AutoModelForSequenceClassification` ç±»ï¼‰\n",
    "\n",
    "è­¦å‘Šé€šçŸ¥æˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆ`vocab_transform` å’Œ `vocab_layer_norm` å±‚ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡ï¼ˆ`pre_classifier` å’Œ `classifier` å±‚ï¼‰ã€‚åœ¨å¾®è°ƒæ¨¡å‹æƒ…å†µä¸‹æ˜¯ç»å¯¹æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ é™¤ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å¤´éƒ¨ï¼Œå¹¶ç”¨ä¸€ä¸ªæ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒï¼Œå¯¹äºè¿™ä¸ªæ–°å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“ä¼šè­¦å‘Šæˆ‘ä»¬åœ¨ç”¨å®ƒè¿›è¡Œæ¨ç†ä¹‹å‰åº”è¯¥å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ä¸€ã€ä»»åŠ¡ä¸æ¨¡å‹ç±»çš„å¯¹åº”å…³ç³»\n",
    "Hugging Face Transformers åº“ä¸ºä¸åŒä»»åŠ¡æä¾›äº†ä¸“ç”¨ç±»ï¼Œå°±åƒé€‰æ‹©ä¸åŒçš„å·¥å…·ï¼š\n",
    "\n",
    "| ä»»åŠ¡ç±»å‹                  | å¯¹åº”æ¨¡å‹ç±»                          | ç¤ºä¾‹åœºæ™¯                     |\n",
    "|--------------------------|-----------------------------------|----------------------------|\n",
    "| æ–‡æœ¬åˆ†ç±»                  | `AutoModelForSequenceClassification` | æƒ…æ„Ÿåˆ†æã€è¯„åˆ†é¢„æµ‹          |\n",
    "| é—®ç­”ä»»åŠ¡                  | `AutoModelForQuestionAnswering`     | SQuAD é—®ç­”ã€é˜…è¯»ç†è§£        |\n",
    "| æ–‡æœ¬ç”Ÿæˆ                  | `AutoModelForCausalLM`              | æ•…äº‹ç»­å†™ã€å¯¹è¯ç”Ÿæˆ          |\n",
    "| æ©ç è¯­è¨€å»ºæ¨¡              | `AutoModelForMaskedLM`              | BERT å¼å¡«ç©ºä»»åŠ¡             |\n",
    "| åºåˆ—åˆ°åºåˆ—                | `AutoModelForSeq2SeqLM`             | ç¿»è¯‘ã€æ‘˜è¦ç”Ÿæˆ              |\n",
    "| æ ‡è®°åˆ†ç±»                  | `AutoModelForTokenClassification`   | å‘½åå®ä½“è¯†åˆ«ã€è¯æ€§æ ‡æ³¨      |\n",
    "| å¤šé€‰ä»»åŠ¡                  | `AutoModelForMultipleChoice`        | å¤šé€‰é¢˜å›ç­”                  |\n",
    "\n",
    "---\n",
    "\n",
    "### äºŒã€Transformers åº“çš„ä¸»è¦æ¨¡å‹ç±»\n",
    "ä»¥ä¸‹æ˜¯å¸¸ç”¨çš„æ¨¡å‹ç±»ï¼ˆä»¥ **BERT** æ¶æ„ä¸ºä¾‹ï¼Œå…¶ä»–æ¨¡å‹ç±»ä¼¼ï¼‰ï¼š\n",
    "\n",
    "#### 1. åŸºç¡€æ¨¡å‹\n",
    "```python\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")  # é€šç”¨ç‰¹å¾æå–\n",
    "```\n",
    "\n",
    "#### 2. ä»»åŠ¡ä¸“ç”¨æ¨¡å‹\n",
    "```python\n",
    "# æ–‡æœ¬åˆ†ç±»ï¼ˆå¦‚æƒ…æ„Ÿåˆ†æï¼‰\n",
    "AutoModelForSequenceClassification.from_pretrained(...)\n",
    "\n",
    "# é—®ç­”ä»»åŠ¡ï¼ˆå¦‚SQuADï¼‰\n",
    "AutoModelForQuestionAnswering.from_pretrained(...)\n",
    "\n",
    "# æ–‡æœ¬ç”Ÿæˆï¼ˆå¦‚GPTé£æ ¼ï¼‰\n",
    "AutoModelForCausalLM.from_pretrained(...)\n",
    "\n",
    "# åºåˆ—åˆ°åºåˆ—ï¼ˆå¦‚BART/T5ï¼‰\n",
    "AutoModelForSeq2SeqLM.from_pretrained(...)\n",
    "\n",
    "# æ ‡è®°çº§åˆ†ç±»ï¼ˆå¦‚NERï¼‰\n",
    "AutoModelForTokenClassification.from_pretrained(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ä¸‰ã€å¤„ç†è‡ªå®šä¹‰ä»»åŠ¡çš„ä¸‰ç§æ–¹æ¡ˆ\n",
    "å¦‚æœä½ çš„ä»»åŠ¡æ²¡æœ‰ç°æˆç±»ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•è§£å†³ï¼š\n",
    "\n",
    "#### æ–¹æ¡ˆ 1ï¼šæ”¹é€ ç°æœ‰æ¨¡å‹ï¼ˆæ¨èï¼‰\n",
    "```python\n",
    "from transformers import AutoModel\n",
    "\n",
    "# åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# æ·»åŠ è‡ªå®šä¹‰å¤´éƒ¨\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = model\n",
    "        self.custom_head = nn.Linear(768, 3)  # å‡è®¾ä½ çš„ä»»åŠ¡éœ€è¦3ç±»è¾“å‡º\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        pooled = outputs.last_hidden_state[:,0]  # å–CLSæ ‡è®°\n",
    "        return self.custom_head(pooled)\n",
    "```\n",
    "\n",
    "#### æ–¹æ¡ˆ 2ï¼šç»§æ‰¿å¹¶æ‰©å±•\n",
    "```python\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "class MyCustomModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.my_layer = nn.Linear(config.hidden_size, 5)  # è‡ªå®šä¹‰è¾“å‡ºç»´åº¦\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        return self.my_layer(sequence_output[:,0])  # ä½¿ç”¨CLSæ ‡è®°\n",
    "```\n",
    "\n",
    "#### æ–¹æ¡ˆ 3ï¼šä½¿ç”¨ `AutoModelWithHeads`\n",
    "ï¼ˆéœ€å®‰è£… `adapters` åº“ï¼‰\n",
    "```python\n",
    "from transformers.adapters import AutoAdapterModel\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.add_classification_head(\"my_task\", num_labels=3)  # æ·»åŠ åˆ†ç±»å¤´\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### å››ã€å…³äºè­¦å‘Šä¿¡æ¯çš„è§£é‡Š\n",
    "å½“ä½ çœ‹åˆ°ç±»ä¼¼è¿™æ ·çš„è­¦å‘Šï¼š\n",
    "```\n",
    "Some weights were not used... (vocab_transform, vocab_layer_norm)\n",
    "You should probably TRAIN this model...\n",
    "```\n",
    "è¿™æ˜¯ **æ­£å¸¸ç°è±¡**ï¼å› ä¸ºï¼š\n",
    "1. é¢„è®­ç»ƒæ¨¡å‹çš„åŸå§‹å¤´éƒ¨ï¼ˆå¦‚MLMå¤´éƒ¨ï¼‰è¢«ç§»é™¤\n",
    "2. æ–°çš„ä»»åŠ¡å¤´éƒ¨ï¼ˆå¦‚åˆ†ç±»å™¨ï¼‰éœ€è¦é‡æ–°è®­ç»ƒ\n",
    "3. åº“åœ¨æé†’ä½ éœ€è¦å¾®è°ƒåæ‰èƒ½ç”¨äºæ¨ç†\n",
    "\n",
    "---\n",
    "\n",
    "### äº”ã€å­¦ä¹ èµ„æºæ¨è\n",
    "1. [å®˜æ–¹ä»»åŠ¡æŒ‡å—](https://huggingface.co/docs/transformers/task_summary)\n",
    "2. [è‡ªå®šä¹‰æ¨¡å‹æ•™ç¨‹](https://huggingface.co/docs/transformers/custom_models)\n",
    "3. [ç¤¾åŒºè®ºå›](https://discuss.huggingface.co/)ï¼ˆé‡åˆ°é—®é¢˜æ—¶ä¼˜å…ˆæœç´¢ï¼‰\n",
    "\n",
    "é€šè¿‡çµæ´»ç»„åˆè¿™äº›æ–¹æ³•ï¼Œä½ å¯ä»¥åº”å¯¹ä»»ä½•è‡ªå®šä¹‰ä»»åŠ¡éœ€æ±‚ï¼ğŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard ç‰ˆæœ¬: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import version\n",
    "print(\"TensorBoard ç‰ˆæœ¬:\", version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8001 (pid 1319472), started 17:47:30 ago. (Use '!kill 1319472' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6c031199972a8469\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6c031199972a8469\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8001;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "# æŒ‡å®šæ—¥å¿—ç›®å½•å’Œç«¯å£ï¼ˆæ³¨æ„è¿™é‡Œçš„ç«¯å£è¦ä¸æ£€æµ‹çš„8001ä¸€è‡´ï¼‰\n",
    "log_dir = \"your_logs_directory\"  # æ›¿æ¢ä¸ºå®é™…çš„æ—¥å¿—ç›®å½•è·¯å¾„\n",
    "%tensorboard --logdir $log_dir --port 8001 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "# åˆ›å»ºTCPå¥—æ¥å­—\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "# å°è¯•è¿æ¥æœ¬åœ°8001ç«¯å£ï¼ˆéé˜»å¡æ–¹å¼ï¼‰\n",
    "result = sock.connect_ex(('localhost', 8001))\n",
    "# æ–­è¨€éªŒè¯ï¼ˆ0è¡¨ç¤ºç«¯å£å¼€æ”¾ï¼‰\n",
    "assert result == 0, \"TensorBoard ç«¯å£ 8001 æœªå¼€å¯ï¼\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æœ‰é—®é¢˜ï¼Œæš‚æ—¶ä¸ç”¨ï¼Œå…ˆè®­ç»ƒ\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# # åŠ è½½F1æŒ‡æ ‡ï¼ˆæ”¯æŒåˆ†ç±»ä»»åŠ¡çš„micro/macro/weightedï¼‰\n",
    "# squad_metric = evaluate.load(\"/root/projects/LLM-learning/evaluate/squad_v2.py\" if squad_v2 else \"/root/projects/LLM-learning/evaluate/squad.py\")\n",
    "\n",
    "# # å®šä¹‰è®¡ç®—å‡½æ•°ï¼ˆå¤„ç†æ¨¡å‹è¾“å‡ºï¼‰\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)  # åˆ†ç±»ä»»åŠ¡å–æœ€å¤§æ¦‚ç‡ç±»åˆ«\n",
    "#     return squad_metric.compute(\n",
    "#         predictions=predictions, \n",
    "#         references=labels,\n",
    "#         average=\"macro\"  # \"micro\"ï¼ˆå…¨å±€ç»Ÿè®¡ï¼‰ã€\"macro\"ï¼ˆç±»åˆ«å¹³å‡ï¼‰ã€\"weighted\"ï¼ˆåŠ æƒå¹³å‡ï¼‰\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "#### è®­ç»ƒè¶…å‚æ•°ï¼ˆTrainingArgumentsï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,  # æ¨¡å‹/æ—¥å¿—ä¿å­˜è·¯å¾„\n",
    "    evaluation_strategy = \"epoch\",  # æ¯ä¸ªepochåè¯„ä¼°ï¼ˆå¯é€‰\"steps\"æŒ‰æ­¥è¯„ä¼°ï¼‰\n",
    "    learning_rate=2e-5,  # ç»å…¸å¾®è°ƒå­¦ä¹ ç‡ï¼ˆé¢„è®­ç»ƒæ¨¡å‹çš„å…¸å‹å­¦ä¹ ç‡èŒƒå›´ï¼š1e-5~5e-5ï¼‰\n",
    "    per_device_train_batch_size=batch_size,  # æ¯ä¸ªGPUçš„è®­ç»ƒæ‰¹æ¬¡ï¼ˆæ€»batch_size = è¯¥å€¼ * GPUæ•°é‡ï¼‰\n",
    "    per_device_eval_batch_size=batch_size,   # æ¯ä¸ªGPUçš„è¯„ä¼°æ‰¹æ¬¡ï¼ˆå¯å¤§äºè®­ç»ƒbatch_sizeï¼‰\n",
    "    num_train_epochs=3,  # è®­ç»ƒè½®æ¬¡ï¼ˆSQuADç­‰ä¸­å‹æ•°æ®é›†å¸¸ç”¨2-5è½®ï¼‰\n",
    "    weight_decay=0.01,  # L2æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œå¸¸ç”¨0.01-0.1ï¼‰\n",
    "    fp16=True,  # å¯ç”¨FP16æ··åˆç²¾åº¦\n",
    "    # save_strategy=\"epoch\",       # æ¯ä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹\n",
    "    # load_best_model_at_end=True, # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collatorï¼ˆæ•°æ®æ•´ç†å™¨ï¼‰\n",
    "\n",
    "æ•°æ®æ•´ç†å™¨å°†è®­ç»ƒæ•°æ®æ•´ç†ä¸ºæ‰¹æ¬¡æ•°æ®ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒæ—¶çš„æ‰¹æ¬¡å¤„ç†ã€‚æœ¬æ•™ç¨‹ä½¿ç”¨é»˜è®¤çš„ `default_data_collator`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "### å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼ˆTrainerï¼‰\n",
    "\n",
    "ä¸ºäº†å‡å°‘è®­ç»ƒæ—¶é—´ï¼ˆéœ€è¦å¤§é‡ç®—åŠ›æ”¯æŒï¼‰ï¼Œæˆ‘ä»¬ä¸åœ¨æœ¬æ•™ç¨‹çš„è®­ç»ƒæ¨¡å‹è¿‡ç¨‹ä¸­è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ã€‚\n",
    "\n",
    "è€Œæ˜¯è®­ç»ƒå®Œæˆåï¼Œå†ç‹¬ç«‹è¿›è¡Œæ¨¡å‹è¯„ä¼°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU ä½¿ç”¨æƒ…å†µ\n",
    "\n",
    "è®­ç»ƒæ•°æ®ä¸æ¨¡å‹é…ç½®ï¼š\n",
    "\n",
    "- SQUAD v1.1\n",
    "- model_checkpoint = \"distilbert-base-uncased\"\n",
    "- batch_size = 64\n",
    "\n",
    "NVIDIA GPU ä½¿ç”¨æƒ…å†µï¼š\n",
    "\n",
    "```shell\n",
    "Every 5.0s: nvidia-smi                                                                                                                                 deepseek-r1-t4-test: Wed Mar 12 17:52:30 2025\n",
    "\n",
    "Wed Mar 12 17:52:30 2025\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       On  | 00000000:00:07.0 Off |                    0 |\n",
    "| N/A   53C    P0              63W /  70W |  10945MiB / 15360MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A   1271234      C   /root/miniconda3/envs/peft/bin/python     10942MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4152/4152 49:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.498200</td>\n",
       "      <td>1.273643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.121800</td>\n",
       "      <td>1.185791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.985200</td>\n",
       "      <td>1.167942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4152, training_loss=1.3124258081807336, metrics={'train_runtime': 2959.0362, 'train_samples_per_second': 89.749, 'train_steps_per_second': 1.403, 'total_flos': 2.602335381127373e+16, 'train_loss': 1.3124258081807336, 'epoch': 3.0})"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒå®Œæˆåï¼Œç¬¬ä¸€æ—¶é—´ä¿å­˜æ¨¡å‹æƒé‡æ–‡ä»¶ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹è¯„ä¼°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è¯„ä¼°æ¨¡å‹è¾“å‡ºéœ€è¦ä¸€äº›é¢å¤–çš„å¤„ç†ï¼šå°†æ¨¡å‹çš„é¢„æµ‹æ˜ å°„å›ä¸Šä¸‹æ–‡çš„éƒ¨åˆ†ã€‚**\n",
    "\n",
    "æ¨¡å‹ç›´æ¥è¾“å‡ºçš„æ˜¯é¢„æµ‹ç­”æ¡ˆçš„`èµ·å§‹ä½ç½®`å’Œ`ç»“æŸä½ç½®`çš„**logits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨¡å‹çš„è¾“å‡ºæ˜¯ä¸€ä¸ªç±»ä¼¼å­—å…¸çš„å¯¹è±¡ï¼Œå…¶ä¸­åŒ…å«æŸå¤±ï¼ˆå› ä¸ºæˆ‘ä»¬æä¾›äº†æ ‡ç­¾ï¼‰ï¼Œä»¥åŠèµ·å§‹å’Œç»“æŸlogitsã€‚æˆ‘ä»¬ä¸éœ€è¦æŸå¤±æ¥è¿›è¡Œé¢„æµ‹ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹logitsï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  43, 118, 108,  72,  35, 108,  34,  73,  41,  80,  91,\n",
       "         156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  41,  35,  42,  77,\n",
       "          11,  44,  27, 133,  66,  40,  87,  44,  43,  83, 127,  26,  28,  33,\n",
       "          87, 127,  95,  25,  43, 132,  42,  29,  44,  46,  24,  44,  65,  58,\n",
       "          81,  14,  59,  72,  25,  36,  55,  43], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  44, 118, 109,  75,  37, 109,  36,  76,  42,  83,  94,\n",
       "         158,  35,  83,  94,  83,  60,  80,  31,  43,  54,  42,  35,  43,  80,\n",
       "          13,  45,  28, 133,  66,  41,  89,  45,  44,  85, 127,  27,  30,  34,\n",
       "          89, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,\n",
       "          81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### å¦‚ä½•ä»æ¨¡å‹è¾“å‡ºçš„ä½ç½® logit ç»„åˆæˆç­”æ¡ˆ\n",
    "\n",
    "æˆ‘ä»¬æœ‰æ¯ä¸ªç‰¹å¾å’Œæ¯ä¸ªæ ‡è®°çš„logitã€‚åœ¨æ¯ä¸ªç‰¹å¾ä¸­ä¸ºæ¯ä¸ªæ ‡è®°é¢„æµ‹ç­”æ¡ˆæœ€æ˜æ˜¾çš„æ–¹æ³•æ˜¯ï¼Œå°†èµ·å§‹logitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºèµ·å§‹ä½ç½®ï¼Œå°†ç»“æŸlogitsçš„æœ€å¤§ç´¢å¼•ä½œä¸ºç»“æŸä½ç½®ã€‚\n",
    "\n",
    "åœ¨è®¸å¤šæƒ…å†µä¸‹è¿™ç§æ–¹å¼æ•ˆæœå¾ˆå¥½ï¼Œä½†æ˜¯å¦‚æœæ­¤é¢„æµ‹ç»™å‡ºäº†ä¸å¯èƒ½çš„ç»“æœè¯¥æ€ä¹ˆåŠï¼Ÿæ¯”å¦‚ï¼šèµ·å§‹ä½ç½®å¯èƒ½å¤§äºç»“æŸä½ç½®ï¼Œæˆ–è€…æŒ‡å‘é—®é¢˜ä¸­çš„æ–‡æœ¬ç‰‡æ®µè€Œä¸æ˜¯ç­”æ¡ˆã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯èƒ½å¸Œæœ›æŸ¥çœ‹ç¬¬äºŒå¥½çš„é¢„æµ‹ï¼Œçœ‹å®ƒæ˜¯å¦ç»™å‡ºäº†ä¸€ä¸ªå¯èƒ½çš„ç­”æ¡ˆï¼Œå¹¶é€‰æ‹©å®ƒã€‚\n",
    "\n",
    "é€‰æ‹©ç¬¬äºŒå¥½çš„ç­”æ¡ˆå¹¶ä¸åƒé€‰æ‹©æœ€ä½³ç­”æ¡ˆé‚£ä¹ˆå®¹æ˜“ï¼š\n",
    "- å®ƒæ˜¯èµ·å§‹logitsä¸­ç¬¬äºŒä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­æœ€ä½³ç´¢å¼•å—ï¼Ÿ\n",
    "- è¿˜æ˜¯èµ·å§‹logitsä¸­æœ€ä½³ç´¢å¼•ä¸ç»“æŸlogitsä¸­ç¬¬äºŒä½³ç´¢å¼•ï¼Ÿ\n",
    "- å¦‚æœç¬¬äºŒå¥½çš„ç­”æ¡ˆä¹Ÿä¸å¯èƒ½ï¼Œé‚£ä¹ˆå¯¹äºç¬¬ä¸‰å¥½çš„ç­”æ¡ˆï¼Œæƒ…å†µä¼šæ›´åŠ æ£˜æ‰‹ã€‚\n",
    "\n",
    "ä¸ºäº†å¯¹ç­”æ¡ˆè¿›è¡Œåˆ†ç±»ï¼Œ\n",
    "1. å°†ä½¿ç”¨é€šè¿‡æ·»åŠ èµ·å§‹å’Œç»“æŸlogitsè·å¾—çš„åˆ†æ•°\n",
    "1. è®¾è®¡ä¸€ä¸ªåä¸º`n_best_size`çš„è¶…å‚æ•°ï¼Œé™åˆ¶ä¸å¯¹æ‰€æœ‰å¯èƒ½çš„ç­”æ¡ˆè¿›è¡Œæ’åºã€‚\n",
    "1. æˆ‘ä»¬å°†é€‰æ‹©èµ·å§‹å’Œç»“æŸlogitsä¸­çš„æœ€ä½³ç´¢å¼•ï¼Œå¹¶æ”¶é›†è¿™äº›é¢„æµ‹çš„æ‰€æœ‰ç­”æ¡ˆã€‚\n",
    "1. åœ¨æ£€æŸ¥æ¯ä¸€ä¸ªæ˜¯å¦æœ‰æ•ˆåï¼Œæˆ‘ä»¬å°†æŒ‰ç…§å…¶åˆ†æ•°å¯¹å®ƒä»¬è¿›è¡Œæ’åºï¼Œå¹¶ä¿ç•™æœ€ä½³çš„ç­”æ¡ˆã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯æˆ‘ä»¬å¦‚ä½•åœ¨æ‰¹æ¬¡ä¸­çš„ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šæ‰§è¡Œæ­¤æ“ä½œçš„ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®å®ƒä»¬çš„å¾—åˆ†å¯¹`valid_answers`è¿›è¡Œæ’åºï¼Œå¹¶ä»…ä¿ç•™æœ€ä½³ç­”æ¡ˆã€‚å”¯ä¸€å‰©ä¸‹çš„é—®é¢˜æ˜¯å¦‚ä½•æ£€æŸ¥ç»™å®šçš„è·¨åº¦æ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼ˆè€Œä¸æ˜¯é—®é¢˜ä¸­ï¼‰ï¼Œä»¥åŠå¦‚ä½•è·å–å…¶ä¸­çš„æ–‡æœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‘æˆ‘ä»¬çš„éªŒè¯ç‰¹å¾æ·»åŠ ä¸¤ä¸ªå†…å®¹ï¼š\n",
    "\n",
    "- ç”Ÿæˆè¯¥ç‰¹å¾çš„ç¤ºä¾‹çš„IDï¼ˆå› ä¸ºæ¯ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œå¦‚å‰æ‰€ç¤ºï¼‰ï¼›\n",
    "- åç§»æ˜ å°„ï¼Œå®ƒå°†ä¸ºæˆ‘ä»¬æä¾›ä»æ ‡è®°ç´¢å¼•åˆ°ä¸Šä¸‹æ–‡ä¸­å­—ç¬¦ä½ç½®çš„æ˜ å°„ã€‚\n",
    "\n",
    "è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ç¨å¾®ä¸åŒäº`prepare_train_features`æ¥é‡æ–°å¤„ç†éªŒè¯é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # ä¸€äº›é—®é¢˜çš„å·¦ä¾§æœ‰å¾ˆå¤šç©ºç™½ï¼Œè¿™äº›ç©ºç™½å¹¶ä¸æœ‰ç”¨ä¸”ä¼šå¯¼è‡´ä¸Šä¸‹æ–‡æˆªæ–­å¤±è´¥ï¼ˆåˆ†è¯åçš„é—®é¢˜ä¼šå ç”¨å¾ˆå¤šç©ºé—´ï¼‰ã€‚\n",
    "    # å› æ­¤æˆ‘ä»¬ç§»é™¤è¿™äº›å·¦ä¾§ç©ºç™½\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # ä½¿ç”¨æˆªæ–­å’Œå¯èƒ½çš„å¡«å……å¯¹æˆ‘ä»¬çš„ç¤ºä¾‹è¿›è¡Œåˆ†è¯ï¼Œä½†ä½¿ç”¨æ­¥é•¿ä¿ç•™æº¢å‡ºçš„ä»¤ç‰Œã€‚è¿™å¯¼è‡´ä¸€ä¸ªé•¿ä¸Šä¸‹æ–‡çš„ç¤ºä¾‹å¯èƒ½äº§ç”Ÿ\n",
    "    # å‡ ä¸ªç‰¹å¾ï¼Œæ¯ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡éƒ½ä¼šç¨å¾®ä¸å‰ä¸€ä¸ªç‰¹å¾çš„ä¸Šä¸‹æ–‡é‡å ã€‚\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # ç”±äºä¸€ä¸ªç¤ºä¾‹åœ¨ä¸Šä¸‹æ–‡å¾ˆé•¿æ—¶å¯èƒ½ä¼šäº§ç”Ÿå‡ ä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªä»ç‰¹å¾æ˜ å°„åˆ°å…¶å¯¹åº”ç¤ºä¾‹çš„æ˜ å°„ã€‚è¿™ä¸ªé”®å°±æ˜¯ä¸ºäº†è¿™ä¸ªç›®çš„ã€‚\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # æˆ‘ä»¬ä¿ç•™äº§ç”Ÿè¿™ä¸ªç‰¹å¾çš„ç¤ºä¾‹IDï¼Œå¹¶ä¸”ä¼šå­˜å‚¨åç§»æ˜ å°„ã€‚\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # è·å–ä¸è¯¥ç¤ºä¾‹å¯¹åº”çš„åºåˆ—ï¼ˆä»¥äº†è§£å“ªäº›æ˜¯ä¸Šä¸‹æ–‡ï¼Œå“ªäº›æ˜¯é—®é¢˜ï¼‰ã€‚\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # ä¸€ä¸ªç¤ºä¾‹å¯ä»¥äº§ç”Ÿå‡ ä¸ªæ–‡æœ¬æ®µï¼Œè¿™é‡Œæ˜¯åŒ…å«è¯¥æ–‡æœ¬æ®µçš„ç¤ºä¾‹çš„ç´¢å¼•ã€‚\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # å°†ä¸å±äºä¸Šä¸‹æ–‡çš„åç§»æ˜ å°„è®¾ç½®ä¸ºNoneï¼Œä»¥ä¾¿å®¹æ˜“ç¡®å®šä¸€ä¸ªä»¤ç‰Œä½ç½®æ˜¯å¦å±äºä¸Šä¸‹æ–‡ã€‚\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†`prepare_validation_features`åº”ç”¨åˆ°æ•´ä¸ªéªŒè¯é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d136b1633f240deb31acbb9d95b9b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trainer`ä¼šéšè—æ¨¡å‹ä¸ä½¿ç”¨çš„åˆ—ï¼ˆåœ¨è¿™é‡Œæ˜¯`example_id`å’Œ`offset_mapping`ï¼Œæˆ‘ä»¬éœ€è¦å®ƒä»¬è¿›è¡Œåå¤„ç†ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦å°†å®ƒä»¬é‡æ–°è®¾ç½®å›æ¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹è¿›ä¹‹å‰çš„æµ‹è¯•ï¼š\n",
    "\n",
    "ç”±äºåœ¨åç§»æ˜ å°„ä¸­ï¼Œå½“å®ƒå¯¹åº”äºé—®é¢˜çš„ä¸€éƒ¨åˆ†æ—¶ï¼Œæˆ‘ä»¬å°†å…¶è®¾ç½®ä¸ºNoneï¼Œå› æ­¤å¯ä»¥è½»æ¾æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦å®Œå…¨åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚æˆ‘ä»¬è¿˜å¯ä»¥ä»è€ƒè™‘ä¸­æ’é™¤éå¸¸é•¿çš„ç­”æ¡ˆï¼ˆå¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ï¼‰ã€‚\n",
    "\n",
    "å±•å¼€è¯´ä¸‹å…·ä½“å®ç°ï¼š\n",
    "- é¦–å…ˆä»æ¨¡å‹è¾“å‡ºä¸­è·å–èµ·å§‹å’Œç»“æŸçš„é€»è¾‘å€¼ï¼ˆlogitsï¼‰ï¼Œè¿™äº›å€¼è¡¨æ˜ç­”æ¡ˆåœ¨æ–‡æœ¬ä¸­å¯èƒ½å¼€å§‹å’Œç»“æŸçš„ä½ç½®ã€‚\n",
    "- ç„¶åï¼Œå®ƒä½¿ç”¨åç§»æ˜ å°„ï¼ˆoffset_mappingï¼‰æ¥æ‰¾åˆ°è¿™äº›é€»è¾‘å€¼åœ¨åŸå§‹æ–‡æœ¬ä¸­çš„å…·ä½“ä½ç½®ã€‚\n",
    "- æ¥ä¸‹æ¥ï¼Œä»£ç éå†å¯èƒ½çš„å¼€å§‹å’Œç»“æŸç´¢å¼•ç»„åˆï¼Œæ’é™¤é‚£äº›ä¸åœ¨ä¸Šä¸‹æ–‡èŒƒå›´å†…æˆ–é•¿åº¦ä¸åˆé€‚çš„ç­”æ¡ˆã€‚\n",
    "- å¯¹äºæœ‰æ•ˆçš„ç­”æ¡ˆï¼Œå®ƒè®¡ç®—å‡ºä¸€ä¸ªåˆ†æ•°ï¼ˆåŸºäºå¼€å§‹å’Œç»“æŸé€»è¾‘å€¼çš„å’Œï¼‰ï¼Œå¹¶å°†ç­”æ¡ˆåŠå…¶åˆ†æ•°å­˜å‚¨èµ·æ¥ã€‚\n",
    "- æœ€åï¼Œå®ƒæ ¹æ®åˆ†æ•°å¯¹ç­”æ¡ˆè¿›è¡Œæ’åºï¼Œå¹¶è¿”å›å¾—åˆ†æœ€é«˜çš„å‡ ä¸ªç­”æ¡ˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 15.2265625, 'text': 'Denver Broncos'},\n",
       " {'score': 13.082031,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 11.640625, 'text': 'Carolina Panthers'},\n",
       " {'score': 11.4296875, 'text': 'Broncos'},\n",
       " {'score': 11.277344,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 10.154297,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 10.123047, 'text': 'Denver'},\n",
       " {'score': 9.285156,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 9.1328125,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 8.009766,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 8.008057,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 7.694336,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 7.317871,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 7.251465,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title.'},\n",
       " {'score': 7.1833496,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 6.987793, 'text': 'AFC) champion Denver Broncos'},\n",
       " {'score': 6.864746, 'text': 'champion Denver Broncos'},\n",
       " {'score': 6.614258,\n",
       "  'text': 'National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 6.426758, 'text': 'Panthers'},\n",
       " {'score': 6.2529297, 'text': 'Carolina'}]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰“å°æ¯”è¾ƒæ¨¡å‹è¾“å‡ºå’Œæ ‡å‡†ç­”æ¡ˆï¼ˆGround-truthï¼‰æ˜¯å¦ä¸€è‡´:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**æ¨¡å‹æœ€é«˜æ¦‚ç‡çš„è¾“å‡ºä¸æ ‡å‡†ç­”æ¡ˆä¸€è‡´**\n",
    "\n",
    "æ­£å¦‚ä¸Šé¢çš„ä»£ç æ‰€ç¤ºï¼Œè¿™åœ¨ç¬¬ä¸€ä¸ªç‰¹å¾ä¸Šå¾ˆå®¹æ˜“ï¼Œå› ä¸ºæˆ‘ä»¬çŸ¥é“å®ƒæ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚\n",
    "\n",
    "å¯¹äºå…¶ä»–ç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å»ºç«‹ä¸€ä¸ªç¤ºä¾‹ä¸å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„å…³ç³»ã€‚\n",
    "\n",
    "æ­¤å¤–ï¼Œç”±äºä¸€ä¸ªç¤ºä¾‹å¯ä»¥ç”Ÿæˆå¤šä¸ªç‰¹å¾ï¼Œæˆ‘ä»¬éœ€è¦å°†ç”±ç»™å®šç¤ºä¾‹ç”Ÿæˆçš„æ‰€æœ‰ç‰¹å¾ä¸­çš„æ‰€æœ‰ç­”æ¡ˆæ±‡é›†åœ¨ä¸€èµ·ï¼Œç„¶åé€‰æ‹©æœ€ä½³ç­”æ¡ˆã€‚\n",
    "\n",
    "ä¸‹é¢çš„ä»£ç æ„å»ºäº†ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•åˆ°å…¶å¯¹åº”ç‰¹å¾ç´¢å¼•çš„æ˜ å°„å…³ç³»ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“`squad_v2 = True`æ—¶ï¼Œæœ‰ä¸€å®šæ¦‚ç‡å‡ºç°ä¸å¯èƒ½çš„ç­”æ¡ˆï¼ˆimpossible answer)ã€‚\n",
    "\n",
    "ä¸Šé¢çš„ä»£ç ä»…ä¿ç•™åœ¨ä¸Šä¸‹æ–‡ä¸­çš„ç­”æ¡ˆï¼Œæˆ‘ä»¬è¿˜éœ€è¦è·å–ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°ï¼ˆå…¶èµ·å§‹å’Œç»“æŸç´¢å¼•å¯¹åº”äºCLSæ ‡è®°çš„ç´¢å¼•ï¼‰ã€‚\n",
    "\n",
    "å½“ä¸€ä¸ªç¤ºä¾‹ç”Ÿæˆå¤šä¸ªç‰¹å¾æ—¶ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ‰€æœ‰ç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆéƒ½é¢„æµ‹å‡ºç°ä¸å¯èƒ½ç­”æ¡ˆæ—¶ï¼ˆå› ä¸ºä¸€ä¸ªç‰¹å¾å¯èƒ½ä¹‹æ‰€ä»¥èƒ½å¤Ÿé¢„æµ‹å‡ºä¸å¯èƒ½ç­”æ¡ˆï¼Œæ˜¯å› ä¸ºç­”æ¡ˆä¸åœ¨å®ƒå¯ä»¥è®¿é—®çš„ä¸Šä¸‹æ–‡éƒ¨åˆ†ï¼‰ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆä¸€ä¸ªç¤ºä¾‹ä¸­ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°æ˜¯è¯¥ç¤ºä¾‹ç”Ÿæˆçš„æ¯ä¸ªç‰¹å¾ä¸­çš„ä¸å¯èƒ½ç­”æ¡ˆçš„åˆ†æ•°çš„æœ€å°å€¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # æ„å»ºä¸€ä¸ªä»ç¤ºä¾‹åˆ°å…¶å¯¹åº”ç‰¹å¾çš„æ˜ å°„ã€‚\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # æˆ‘ä»¬éœ€è¦å¡«å……çš„å­—å…¸ã€‚\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # æ—¥å¿—è®°å½•ã€‚\n",
    "    print(f\"æ­£åœ¨åå¤„ç† {len(examples)} ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ {len(features)} ä¸ªç‰¹å¾ä¸­ã€‚\")\n",
    "\n",
    "    # éå†æ‰€æœ‰ç¤ºä¾‹ï¼\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # è¿™äº›æ˜¯ä¸å½“å‰ç¤ºä¾‹å…³è”çš„ç‰¹å¾çš„ç´¢å¼•ã€‚\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # ä»…åœ¨squad_v2ä¸ºTrueæ—¶ä½¿ç”¨ã€‚\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # éå†ä¸å½“å‰ç¤ºä¾‹å…³è”çš„æ‰€æœ‰ç‰¹å¾ã€‚\n",
    "        for feature_index in feature_indices:\n",
    "            # æˆ‘ä»¬è·å–æ¨¡å‹å¯¹è¿™ä¸ªç‰¹å¾çš„é¢„æµ‹ã€‚\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # è¿™å°†å…è®¸æˆ‘ä»¬å°†logitsä¸­çš„æŸäº›ä½ç½®æ˜ å°„åˆ°åŸå§‹ä¸Šä¸‹æ–‡ä¸­çš„æ–‡æœ¬è·¨åº¦ã€‚\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # æ›´æ–°æœ€å°ç©ºé¢„æµ‹ã€‚\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # æµè§ˆæ‰€æœ‰çš„æœ€ä½³å¼€å§‹å’Œç»“æŸlogitsï¼Œä¸º `n_best_size` ä¸ªæœ€ä½³é€‰æ‹©ã€‚\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # åœ¨æå°‘æ•°æƒ…å†µä¸‹æˆ‘ä»¬æ²¡æœ‰ä¸€ä¸ªéç©ºé¢„æµ‹ï¼Œæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªå‡é¢„æµ‹ä»¥é¿å…å¤±è´¥ã€‚\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # é€‰æ‹©æˆ‘ä»¬çš„æœ€ç»ˆç­”æ¡ˆï¼šæœ€ä½³ç­”æ¡ˆæˆ–ç©ºç­”æ¡ˆï¼ˆä»…é€‚ç”¨äºsquad_v2ï¼‰\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨åŸå§‹ç»“æœä¸Šåº”ç”¨åå¤„ç†é—®ç­”ç»“æœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca311ea914044afaa9a7b2b3ebf30363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ `datasets.load_metric` ä¸­åŠ è½½ `SQuAD v2` çš„è¯„ä¼°æŒ‡æ ‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e80d506dcc4b7d8960aacd2775c45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fe64f5e7a9471d989d18404be582a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å¯ä»¥è°ƒç”¨ä¸Šé¢å®šä¹‰çš„å‡½æ•°è¿›è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "åªéœ€ç¨å¾®è°ƒæ•´ä¸€ä¸‹é¢„æµ‹å’Œæ ‡ç­¾çš„æ ¼å¼ï¼Œå› ä¸ºå®ƒæœŸæœ›çš„æ˜¯ä¸€ç³»åˆ—å­—å…¸è€Œä¸æ˜¯ä¸€ä¸ªå¤§å­—å…¸ã€‚\n",
    "\n",
    "åœ¨ä½¿ç”¨`squad_v2`æ•°æ®é›†æ—¶ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è®¾ç½®`no_answer_probability`å‚æ•°ï¼ˆæˆ‘ä»¬åœ¨è¿™é‡Œå°†å…¶è®¾ç½®ä¸º0.0ï¼Œå› ä¸ºå¦‚æœæˆ‘ä»¬é€‰æ‹©äº†ç­”æ¡ˆï¼Œæˆ‘ä»¬å·²ç»å°†ç­”æ¡ˆè®¾ç½®ä¸ºç©ºï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 74.33301797540209, 'f1': 83.26051790761488}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è½½æœ¬åœ°ä¿å­˜çš„æ¨¡å‹ï¼Œè¿›è¡Œè¯„ä¼°å’Œå†è®­ç»ƒæ›´é«˜çš„ F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4152/4152 49:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.846900</td>\n",
       "      <td>1.212233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.713500</td>\n",
       "      <td>1.232366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>1.247295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-4000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4152, training_loss=0.7533242697890324, metrics={'train_runtime': 2980.1413, 'train_samples_per_second': 89.114, 'train_steps_per_second': 1.393, 'total_flos': 2.602335381127373e+16, 'train_loss': 0.7533242697890324, 'epoch': 3.0})"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save_2 = trained_trainer.save_model(f\"{model_dir}-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trained_trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trained_trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trained_trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  54, 118, 107,  72,  35, 107,  34,  73,  52,  80,  91,\n",
       "         156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  52,  35,  53,  77,\n",
       "          11,  44,  27, 133,  66,  40,  87,  44,  43,  41, 127,  26,  28,  33,\n",
       "          87, 127,  95,  25,  43, 132,  42,  29,  44,  46,  24,  44,  65,  58,\n",
       "          81,  14,  59,  72,  25,  36,  57,  43], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  44, 118, 110,  75,  37, 110,  36,  76,  42,  83,  94,\n",
       "         158,  35,  83,  94,  83,  60,  80,  31,  43,  54,  42,  35,  43,  91,\n",
       "          13,  45,  28, 133,  66,  41,  89,  45,  44,  42, 127,  27,  30,  34,\n",
       "          90, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,\n",
       "          81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# è·å–æœ€ä½³çš„èµ·å§‹å’Œç»“æŸä½ç½®çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# éå†èµ·å§‹ä½ç½®å’Œç»“æŸä½ç½®çš„ç´¢å¼•ç»„åˆ\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # éœ€è¦è¿›ä¸€æ­¥æµ‹è¯•ä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # æˆ‘ä»¬éœ€è¦æ‰¾åˆ°ä¸€ç§æ–¹æ³•æ¥è·å–ä¸ä¸Šä¸‹æ–‡ä¸­ç­”æ¡ˆå¯¹åº”çš„åŸå§‹å­å­—ç¬¦ä¸²\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trained_trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 18.515625, 'text': 'Denver Broncos'},\n",
       " {'score': 15.769531,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 14.910156, 'text': 'Broncos'},\n",
       " {'score': 13.9296875, 'text': 'Carolina Panthers'},\n",
       " {'score': 13.019531,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 12.859375,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 12.537109, 'text': 'Denver'},\n",
       " {'score': 12.1640625,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 10.9296875,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title.'},\n",
       " {'score': 10.2734375,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 10.113281,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 9.332031,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 9.089844,\n",
       "  'text': 'Carolina Panthers 24â€“10 to earn their third Super Bowl title.'},\n",
       " {'score': 8.664551, 'text': 'AFC) champion Denver Broncos'},\n",
       " {'score': 8.66333,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 8.26416,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 7.742676, 'text': 'Panthers'},\n",
       " {'score': 7.600586,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 7.4921875, 'text': 'Carolina'},\n",
       " {'score': 7.3242188,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title.'}]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# ç¬¬ä¸€ä¸ªç‰¹å¾æ¥è‡ªç¬¬ä¸€ä¸ªç¤ºä¾‹ã€‚å¯¹äºæ›´ä¸€èˆ¬çš„æƒ…å†µï¼Œæˆ‘ä»¬éœ€è¦å°†example_idåŒ¹é…åˆ°ä¸€ä¸ªç¤ºä¾‹ç´¢å¼•\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# æ”¶é›†æœ€ä½³å¼€å§‹/ç»“æŸé€»è¾‘çš„ç´¢å¼•ï¼š\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # ä¸è€ƒè™‘è¶…å‡ºèŒƒå›´çš„ç­”æ¡ˆï¼ŒåŸå› æ˜¯ç´¢å¼•è¶…å‡ºèŒƒå›´æˆ–å¯¹åº”äºè¾“å…¥IDçš„éƒ¨åˆ†ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ã€‚\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # ä¸è€ƒè™‘é•¿åº¦å°äº0æˆ–å¤§äºmax_answer_lengthçš„ç­”æ¡ˆã€‚\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # æˆ‘ä»¬éœ€è¦ç»†åŒ–è¿™ä¸ªæµ‹è¯•ï¼Œä»¥æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦åœ¨ä¸Šä¸‹æ–‡ä¸­\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨åå¤„ç† 10570 ä¸ªç¤ºä¾‹çš„é¢„æµ‹ï¼Œè¿™äº›é¢„æµ‹åˆ†æ•£åœ¨ 10784 ä¸ªç‰¹å¾ä¸­ã€‚\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cafe784ba2c40ec9c95307728b6fb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 74.91958372753075, 'f1': 83.83155050300782}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question Answering on SQUAD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
