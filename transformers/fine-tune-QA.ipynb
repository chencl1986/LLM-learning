{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face Transformers 微调语言模型-问答任务\n",
    "\n",
    "我们已经学会使用 Pipeline 加载支持问答任务的预训练模型，本教程代码将展示如何微调训练一个支持问答任务的模型。\n",
    "\n",
    "**注意：微调后的模型仍然是通过提取上下文的子串来回答问题的，而不是生成新的文本。**\n",
    "\n",
    "### 模型执行问答效果示例\n",
    "\n",
    "![Widget inference representing the QA task](docs/images/question_answering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "id": "zVvslsfMIrIh"
   },
   "outputs": [],
   "source": [
    "# 根据你使用的模型和GPU资源情况，调整以下关键参数\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "whPRbBNbIrIl"
   },
   "source": [
    "## 下载数据集\n",
    "\n",
    "在本教程中，我们将使用[斯坦福问答数据集(SQuAD）](https://rajpurkar.github.io/SQuAD-explorer/)。\n",
    "\n",
    "### SQuAD 数据集\n",
    "\n",
    "**斯坦福问答数据集(SQuAD)** 是一个阅读理解数据集，由众包工作者在一系列维基百科文章上提出问题组成。每个问题的答案都是相应阅读段落中的文本片段或范围，或者该问题可能无法回答。\n",
    "\n",
    "SQuAD2.0将SQuAD1.1中的10万个问题与由众包工作者对抗性地撰写的5万多个无法回答的问题相结合，使其看起来与可回答的问题类似。要在SQuAD2.0上表现良好，系统不仅必须在可能时回答问题，还必须确定段落中没有支持任何答案，并放弃回答。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "IreSlFmlIrIm"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270,
     "referenced_widgets": [
      "69caab03d6264fef9fc5649bffff5e20",
      "3f74532faa86412293d90d3952f38c4a",
      "50615aa59c7247c4804ca5cbc7945bd7",
      "fe962391292a413ca55dc932c4279fa7",
      "299f4b4c07654e53a25f8192bd1d7bbd",
      "ad04ed1038154081bbb0c1444784dcc2",
      "7c667ad22b5740d5a6319f1b1e3a8097",
      "46c2b043c0f84806978784a45a4e203b",
      "80e2943be35f46eeb24c8ab13faa6578",
      "de5956b5008d4fdba807bae57509c393",
      "931db1f7a42f4b46b7ff8c2e1262b994",
      "6c1db72efff5476e842c1386fadbbdba",
      "ccd2f37647c547abb4c719b75a26f2de",
      "d30a66df5c0145e79693e09789d96b81",
      "5fa26fc336274073abbd1d550542ee33",
      "2b34de08115d49d285def9269a53f484",
      "d426be871b424affb455aeb7db5e822e",
      "160bf88485f44f5cb6eaeecba5e0901f",
      "745c0d47d672477b9bb0dae77b926364",
      "d22ab78269cd4ccfbcf70c707057c31b",
      "d298eb19eeff453cba51c2804629d3f4",
      "a7204ade36314c86907c562e0a2158b8",
      "e35d42b2d352498ca3fc8530393786b2",
      "75103f83538d44abada79b51a1cec09e",
      "f6253931d90543e9b5fd0bb2d615f73a",
      "051aa783ff9e47e28d1f9584043815f5",
      "0984b2a14115454bbb009df71c1cf36f",
      "8ab9dfce29854049912178941ef1b289",
      "c9de740e007141958545e269372780a4",
      "cbea68b25d6d4ba09b2ce0f27b1726d5",
      "5781fc45cf8d486cb06ed68853b2c644",
      "d2a92143a08a4951b55bab9bc0a6d0d3",
      "a14c3e40e5254d61ba146f6ec88eae25",
      "c4ffe6f624ce4e978a0d9b864544941a",
      "1aca01c1d8c940dfadd3e7144bb35718",
      "9fbbaae50e6743f2aa19342152398186",
      "fea27ca6c9504fc896181bc1ff5730e5",
      "940d00556cb849b3a689d56e274041c2",
      "5cdf9ed939fb42d4bf77301c80b8afca",
      "94b39ccfef0b4b08bf2fb61bb0a657c1",
      "9a55087c85b74ea08b3e952ac1d73cbe",
      "2361ab124daf47cc885ff61f2899b2af",
      "1a65887eb37747ddb75dc4a40f7285f2",
      "3c946e2260704e6c98593136bd32d921",
      "50d325cdb9844f62a9ecc98e768cb5af",
      "aa781f0cfe454e9da5b53b93e9baabd8",
      "6bb68d3887ef43809eb23feb467f9723",
      "7e29a8b952cf4f4ea42833c8bf55342f",
      "dd5997d01d8947e4b1c211433969b89b",
      "2ace4dc78e2f4f1492a181bcd63304e7",
      "bbee008c2791443d8610371d1f16b62b",
      "31b1c8a2e3334b72b45b083688c1a20c",
      "7fb7c36adc624f7dbbcb4a831c1e4f63",
      "0b7c8f1939074794b3d9221244b1344d",
      "a71908883b064e1fbdddb547a8c41743",
      "2f5223f26c8541fc87e91d2205c39995"
     ]
    },
    "id": "s_AY1ATSIrIq",
    "outputId": "fd0578d1-8895-443d-b56f-5908de9f1b6b"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a22392259b3422282cb387d26800370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e369a7bfc8d541eeaad5a9cbf46cefaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544572b98e14405c94d7cc62f7739fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38f4e629cf85422a9f86aa2d246e876a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e5e192efa04d4abbc59db258adae83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzfPtOMoIrIu"
   },
   "source": [
    "The `datasets` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "id": "GWiVUF0jIrIv",
    "outputId": "35e3ea43-f397-4a54-c90c-f2cf8d36873e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 对比数据集\n",
    "\n",
    "相比快速入门使用的 Yelp 评论数据集，我们可以看到 SQuAD 训练和测试集都新增了用于上下文、问题以及问题答案的列：\n",
    "\n",
    "**YelpReviewFull Dataset：**\n",
    "\n",
    "```json\n",
    "\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 650000\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['label', 'text'],\n",
    "        num_rows: 50000\n",
    "    })\n",
    "})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "id": "X6HrpprwIrIz",
    "outputId": "d7670bc0-42e4-4c09-8a6a-5c018ded7d95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '56d443ef2ccc5a1400d830db',\n",
       " 'title': 'Beyoncé',\n",
       " 'context': 'Beyoncé attended St. Mary\\'s Elementary School in Fredericksburg, Texas, where she enrolled in dance classes. Her singing talent was discovered when dance instructor Darlette Johnson began humming a song and she finished it, able to hit the high-pitched notes. Beyoncé\\'s interest in music and performing continued after winning a school talent show at age seven, singing John Lennon\\'s \"Imagine\" to beat 15/16-year-olds. In fall of 1990, Beyoncé enrolled in Parker Elementary School, a music magnet school in Houston, where she would perform with the school\\'s choir. She also attended the High School for the Performing and Visual Arts and later Alief Elsik High School. Beyoncé was also a member of the choir at St. John\\'s United Methodist Church as a soloist for two years.',\n",
       " 'question': \"What city was Beyoncé's elementary school located in?\",\n",
       " 'answers': {'text': ['Fredericksburg'], 'answer_start': [49]}}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][333]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 从上下文中组织回复内容\n",
    "\n",
    "我们可以看到答案是通过它们在文本中的起始位置（这里是第515个字符）以及它们的完整文本表示的，这是上面提到的上下文的子字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "id": "i3j8APAoIrI3"
   },
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "id": "SZy5tRB_IrI7",
    "outputId": "ba8f2124-e485-488f-8c0c-254f34f24f13"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5731ab21b9d445190005e44f</td>\n",
       "      <td>Religion_in_ancient_Rome</td>\n",
       "      <td>The meaning and origin of many archaic festivals baffled even Rome's intellectual elite, but the more obscure they were, the greater the opportunity for reinvention and reinterpretation — a fact lost neither on Augustus in his program of religious reform, which often cloaked autocratic innovation, nor on his only rival as mythmaker of the era, Ovid. In his Fasti, a long-form poem covering Roman holidays from January to June, Ovid presents a unique look at Roman antiquarian lore, popular customs, and religious practice that is by turns imaginative, entertaining, high-minded, and scurrilous; not a priestly account, despite the speaker's pose as a vates or inspired poet-prophet, but a work of description, imagination and poetic etymology that reflects the broad humor and burlesque spirit of such venerable festivals as the Saturnalia, Consualia, and feast of Anna Perenna on the Ides of March, where Ovid treats the assassination of the newly deified Julius Caesar as utterly incidental to the festivities among the Roman people. But official calendars preserved from different times and places also show a flexibility in omitting or expanding events, indicating that there was no single static and authoritative calendar of required observances. In the later Empire under Christian rule, the new Christian festivals were incorporated into the existing framework of the Roman calendar, alongside at least some of the traditional festivals.</td>\n",
       "      <td>What poet wrote a long poem describing Roman religious holidays?</td>\n",
       "      <td>{'text': ['Ovid'], 'answer_start': [346]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56e08b457aa994140058e5e3</td>\n",
       "      <td>Hydrogen</td>\n",
       "      <td>Hydrogen forms a vast array of compounds with carbon called the hydrocarbons, and an even vaster array with heteroatoms that, because of their general association with living things, are called organic compounds. The study of their properties is known as organic chemistry and their study in the context of living organisms is known as biochemistry. By some definitions, \"organic\" compounds are only required to contain carbon. However, most of them also contain hydrogen, and because it is the carbon-hydrogen bond which gives this class of compounds most of its particular chemical characteristics, carbon-hydrogen bonds are required in some definitions of the word \"organic\" in chemistry. Millions of hydrocarbons are known, and they are usually formed by complicated synthetic pathways, which seldom involve elementary hydrogen.</td>\n",
       "      <td>What is the form of hydrogen and carbon called?</td>\n",
       "      <td>{'text': ['hydrocarbons'], 'answer_start': [64]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56cef65baab44d1400b88d36</td>\n",
       "      <td>Spectre_(2015_film)</td>\n",
       "      <td>Christopher Orr, writing in The Atlantic, also criticised the film, saying that Spectre \"backslides on virtually every [aspect]\". Lawrence Toppman of The Charlotte Observer called Craig's performance \"Bored, James Bored.\" Alyssa Rosenberg, writing for The Washington Post, stated that the film turned into \"a disappointingly conventional Bond film.\"</td>\n",
       "      <td>What adjective did Lawrence Toppman use to describe Craig's portrayal of James Bond?</td>\n",
       "      <td>{'text': ['Bored'], 'answer_start': [201]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>571a30bb10f8ca1400304f53</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>King County Metro provides frequent stop bus service within the city and surrounding county, as well as a South Lake Union Streetcar line between the South Lake Union neighborhood and Westlake Center in downtown. Seattle is one of the few cities in North America whose bus fleet includes electric trolleybuses. Sound Transit currently provides an express bus service within the metropolitan area; two Sounder commuter rail lines between the suburbs and downtown; its Central Link light rail line, which opened in 2009, between downtown and Sea-Tac Airport gives the city its first rapid transit line that has intermediate stops within the city limits. Washington State Ferries, which manages the largest network of ferries in the United States and third largest in the world, connects Seattle to Bainbridge and Vashon Islands in Puget Sound and to Bremerton and Southworth on the Kitsap Peninsula.</td>\n",
       "      <td>To what two islands does the ferry service connect?</td>\n",
       "      <td>{'text': ['Bainbridge and Vashon'], 'answer_start': [796]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570d2cb4fed7b91900d45cb5</td>\n",
       "      <td>Macintosh</td>\n",
       "      <td>In 1998, after the return of Steve Jobs, Apple consolidated its multiple consumer-level desktop models into the all-in-one iMac G3, which became a commercial success and revitalized the brand. Since their transition to Intel processors in 2006, the complete lineup is entirely based on said processors and associated systems. Its current lineup comprises three desktops (the all-in-one iMac, entry-level Mac mini, and the Mac Pro tower graphics workstation), and four laptops (the MacBook, MacBook Air, MacBook Pro, and MacBook Pro with Retina display). Its Xserve server was discontinued in 2011 in favor of the Mac Mini and Mac Pro.</td>\n",
       "      <td>What took the place of Mac's Xserve server?</td>\n",
       "      <td>{'text': ['Mac Mini and Mac Pro'], 'answer_start': [613]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>570af6876b8089140040f646</td>\n",
       "      <td>Videoconferencing</td>\n",
       "      <td>Technological developments by videoconferencing developers in the 2010s have extended the capabilities of video conferencing systems beyond the boardroom for use with hand-held mobile devices that combine the use of video, audio and on-screen drawing capabilities broadcasting in real-time over secure networks, independent of location. Mobile collaboration systems now allow multiple people in previously unreachable locations, such as workers on an off-shore oil rig, the ability to view and discuss issues with colleagues thousands of miles away. Traditional videoconferencing system manufacturers have begun providing mobile applications as well, such as those that allow for live and still image streaming.</td>\n",
       "      <td>What is one example of an application that videoconferencing manufacturers have begun to offer?</td>\n",
       "      <td>{'text': ['still image streaming'], 'answer_start': [689]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>56e82d0100c9c71400d775eb</td>\n",
       "      <td>Dialect</td>\n",
       "      <td>Italy is home to a vast array of native regional minority languages, most of which are Romance-based and have their own local variants. These regional languages are often referred to colloquially or in non-linguistic circles as Italian \"dialects,\" or dialetti (standard Italian for \"dialects\"). However, the majority of the regional languages in Italy are in fact not actually \"dialects\" of standard Italian in the strict linguistic sense, as they are not derived from modern standard Italian but instead evolved locally from Vulgar Latin independent of standard Italian, with little to no influence from what is now known as \"standard Italian.\" They are therefore better classified as individual languages rather than \"dialects.\"</td>\n",
       "      <td>What are Italian dialects termed in the Italian language?</td>\n",
       "      <td>{'text': ['dialetti'], 'answer_start': [251]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>56e147e6cd28a01900c6772b</td>\n",
       "      <td>Universal_Studios</td>\n",
       "      <td>The Universal Film Manufacturing Company was incorporated in New York on April 30, 1912. Laemmle, who emerged as president in July 1912, was the primary figure in the partnership with Dintenfass, Baumann, Kessel, Powers, Swanson, Horsley, and Brulatour. Eventually all would be bought out by Laemmle. The new Universal studio was a vertically integrated company, with movie production, distribution and exhibition venues all linked in the same corporate entity, the central element of the Studio system era.</td>\n",
       "      <td>Along with exhibition and distribution, what business did the Universal Film Manufacturing Company engage in?</td>\n",
       "      <td>{'text': ['movie production'], 'answer_start': [368]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5731933a05b4da19006bd2d0</td>\n",
       "      <td>Steven_Spielberg</td>\n",
       "      <td>Spielberg's next film, Schindler's List, was based on the true story of Oskar Schindler, a man who risked his life to save 1,100 Jews from the Holocaust. Schindler's List earned Spielberg his first Academy Award for Best Director (it also won Best Picture). With the film a huge success at the box office, Spielberg used the profits to set up the Shoah Foundation, a non-profit organization that archives filmed testimony of Holocaust survivors. In 1997, the American Film Institute listed it among the 10 Greatest American Films ever Made (#9) which moved up to (#8) when the list was remade in 2007.</td>\n",
       "      <td>Whose life was 'Schindler's List' based on?</td>\n",
       "      <td>{'text': ['Oskar Schindler'], 'answer_start': [72]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>56de93f94396321400ee2a36</td>\n",
       "      <td>Arnold_Schwarzenegger</td>\n",
       "      <td>In 1985, Schwarzenegger appeared in \"Stop the Madness\", an anti-drug music video sponsored by the Reagan administration. He first came to wide public notice as a Republican during the 1988 presidential election, accompanying then-Vice President George H.W. Bush at a campaign rally.</td>\n",
       "      <td>In what presidential election year did Schwarzenegger make a name for himself as a prominent Republican?</td>\n",
       "      <td>{'text': ['1988'], 'answer_start': [184]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n9qywopnIrJH"
   },
   "source": [
    "## 预处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tokenizer（分词器）就像一位「语言拆解专家」**，专门帮计算机理解人类文字。它的核心作用可以用三步说清楚：\n",
    "\n",
    "---\n",
    "\n",
    "### 1️⃣ **拆解文本**  \n",
    "把句子拆成 **模型认识的片段**（词或子词）。  \n",
    "例如：  \n",
    "`\"我爱自然语言处理\"` → `[\"我\", \"爱\", \"自然\", \"语言\", \"处理\"]`  \n",
    "（英文如 `\"Hugging Face\"` → `[\"Hug\", \"##ging\", \"Face\"]`）\n",
    "\n",
    "---\n",
    "\n",
    "### 2️⃣ **添加「暗号」**  \n",
    "插入模型需要的**特殊标记**，比如：  \n",
    "- **`[CLS]`**：开头标记（BERT用）  \n",
    "- **`[SEP]`**：分隔标记（区分句子）  \n",
    "```python\n",
    "\"你好吗？\" → [\"[CLS]\", \"你\", \"好\", \"吗\", \"？\", \"[SEP]\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3️⃣ **转成密码数字**  \n",
    "把每个词换成**模型词汇表里的ID号**，类似密码本：  \n",
    "```python\n",
    "[\"[CLS]\", \"你\", \"好\", \"吗\"] → [101, 872, 1962, 3221, 102]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 🌰 **实际效果示例**  \n",
    "你输入：`\"今天厦门天气如何？\"`  \n",
    "Tokenizer处理后输出：  \n",
    "```python\n",
    "{\n",
    "  \"input_ids\": [101, 791, 1921, 1762, 1377, 1442, 3221, 102],\n",
    "  \"attention_mask\": [1, 1, 1, 1, 1, 1, 1, 1]  # 标记哪些是有效内容\n",
    "}\n",
    "```\n",
    "模型看到这些数字就能分析语义，生成回答啦！\n",
    "\n",
    "---\n",
    "\n",
    "### 🤖 **不同模型的差异**  \n",
    "- **BERT类**：拆词较细，加`[CLS]`/`[SEP]`  \n",
    "- **GPT类**：按字节拆分，加`<|endoftext|>`  \n",
    "- **多语言模型**：支持中/英/日等混合拆分  \n",
    "\n",
    "一句话总结：**Tokenizer就是把人类语言「翻译」成AI能懂的数字密码！** 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "id": "eXNLu_-nIrJI"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AutoTokenizer 就像「万能适配器」**  \n",
    "——你只需要告诉它用哪个AI模型（比如BERT、GPT-3），它就会自动匹配对应的文字翻译规则。\n",
    "\n",
    "举个栗子🌰：  \n",
    "- 你想用 **BERT** 模型 → 它自动加载BERT的分词规则  \n",
    "  ```python\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "  ```\n",
    "- 你想用 **GPT** 模型 → 它自动切换成GPT的分词方式  \n",
    "  ```python\n",
    "  tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "  ```\n",
    "\n",
    "**好处**：不用记不同模型的分词器名字（比如`BertTokenizer`、`GPT2Tokenizer`），一个`AutoTokenizer`通吃所有模型，就像万能充电器一样方便！\n",
    "\n",
    "---\n",
    "\n",
    "### 对比示例（手动 vs 自动）\n",
    "| 方式          | 手动选择分词器                   | AutoTokenizer                  |\n",
    "|---------------|----------------------------------|---------------------------------|\n",
    "| **BERT模型**  | `from transformers import BertTokenizer`<br>`tokenizer = BertTokenizer.from_pretrained(\"bert-base\")` | `AutoTokenizer.from_pretrained(\"bert-base\")` |\n",
    "| **GPT模型**   | `from transformers import GPT2Tokenizer`<br>`tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")` | `AutoTokenizer.from_pretrained(\"gpt2\")` |\n",
    "\n",
    "---\n",
    "\n",
    "⚠️ **注意**：名字要对（比如`bert-base-chinese`不能写成`bert-chinese`），否则这个万能充电器也会找不到插口~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vl6IidfdIrJK"
   },
   "source": [
    "以下断言确保我们的 Tokenizers 使用的是 FastTokenizer（Rust 实现，速度和功能性上有一定优势）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否是快速版分词器: True\n",
      "✅ Tokenizer 是快速版 (PreTrainedTokenizerFast)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "# 直接打印判断结果\n",
    "print(\"是否是快速版分词器:\", isinstance(tokenizer, transformers.PreTrainedTokenizerFast))\n",
    "\n",
    "# 或更详细的输出\n",
    "if isinstance(tokenizer, transformers.PreTrainedTokenizerFast):\n",
    "    print(\"✅ Tokenizer 是快速版 (PreTrainedTokenizerFast)\")\n",
    "else:\n",
    "    print(\"❌ Tokenizer 是普通版 (PreTrainedTokenizer)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PreTrainedTokenizer 就像个「文字翻译官」**，专门帮 AI 模型和人类文字打交道。  \n",
    "\n",
    "举个栗子🌰：  \n",
    "你想问 AI \"厦门今天热吗？\"  \n",
    "➡️ **翻译官的工作**：  \n",
    "1. 把这句话切成小块：`[\"厦门\", \"今天\", \"热\", \"吗\"]`  \n",
    "2. 偷偷加暗号：`[开头暗号] 厦门 今天 热 吗 [结尾暗号]`  \n",
    "3. 转成密码数字：`[101, 2345, 567, 8910, 102]`  \n",
    "\n",
    "然后 AI 就能看懂这些数字密码，给出回答啦！  \n",
    "（反过来也会把 AI 的数字密码翻译成人类文字给你看）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)`\n",
    "\n",
    "这个步骤是**可选的安全检查**，主要为了确保你加载的是**快速版分词器（PreTrainedTokenizerFast）**，而不是旧版的慢速分词器（PreTrainedTokenizer）。不检查也能运行，但可能会遇到以下问题：\n",
    "\n",
    "---\n",
    "\n",
    "### 🤔 **为什么要区分 Fast 和普通版？**\n",
    "| 特性                | PreTrainedTokenizerFast（快速版）          | PreTrainedTokenizer（普通版）       |\n",
    "|---------------------|--------------------------------------------|-------------------------------------|\n",
    "| **底层实现**         | Rust语言编写（速度快）                     | Python实现（速度慢）                 |\n",
    "| **批处理支持**       | ✅ 原生支持（如`batch_encode_plus`）        | ❌ 需手动循环处理                    |\n",
    "| **特殊标记处理**     | 自动管理（如填充、截断）                   | 需手动配置                          |\n",
    "| **典型场景**         | 生产环境、大数据处理                        | 教学或兼容旧代码                     |\n",
    "\n",
    "---\n",
    "\n",
    "### 💥 **不检查可能带来的问题**\n",
    "1. **性能下降**：处理1000条文本时，快速版可能比普通版快**5-10倍**。\n",
    "2. **功能缺失**：普通版可能缺少某些API（如`decode`的`skip_special_tokens`参数）。\n",
    "3. **意外错误**：某些库（如Datasets）默认要求快速版分词器。\n",
    "\n",
    "---\n",
    "\n",
    "### 🌰 **实际案例**\n",
    "假设你的`model_checkpoint`意外指向了一个没有快速版的模型：\n",
    "```python\n",
    "model_checkpoint = \"some-old-model\"  # 假设该模型只有普通版分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "# 此时 tokenizer 是 PreTrainedTokenizer 而非 Fast 版\n",
    "# 后续调用 batch_encode_plus 可能报错！\n",
    "```\n",
    "\n",
    "通过`assert`检查，可以**提前发现问题**，避免后续代码崩溃。\n",
    "\n",
    "---\n",
    "\n",
    "### 🔧 **替代方案（如果不做断言）**\n",
    "1. **直接使用**：如果确定模型有快速版，可以跳过检查。\n",
    "2. **降级处理**：捕获异常并改用普通版逻辑：\n",
    "```python\n",
    "if not isinstance(tokenizer, PreTrainedTokenizerFast):\n",
    "    print(\"警告：使用慢速分词器，性能可能受影响！\")\n",
    "    # 手动处理普通版的限制\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "总结：这个断言是**防御性编程**的体现，确保代码在性能和功能上按预期运行。对于关键项目建议保留，个人实验可跳过。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "您可以在大模型表上查看哪种类型的模型具有可用的快速标记器，哪种类型没有。\n",
    "\n",
    "您可以直接在两个句子上调用此标记器（一个用于答案，一个用于上下文）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "id": "a5hBlsrHIrJL",
    "outputId": "acdaa98a-a8cd-4a20-89b8-cc26437bbe90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2054, 2003, 2115, 2171, 1029, 102, 2026, 2171, 2003, 25353, 22144, 2378, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"What is your name?\", \"My name is Sylvain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2129, 2024, 2017, 1029, 102], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tokenizer(\"How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer 进阶操作\n",
    "\n",
    "在问答预处理中的一个特定问题是如何处理非常长的文档。\n",
    "\n",
    "在其他任务中，当文档的长度超过模型最大句子长度时，我们通常会截断它们，但在这里，删除上下文的一部分可能会导致我们丢失正在寻找的答案。\n",
    "\n",
    "为了解决这个问题，我们允许数据集中的一个（长）示例生成多个输入特征，每个特征的长度都小于模型的最大长度（或我们设置的超参数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum length of a feature (question and context)\n",
    "max_length = 384 \n",
    "# The authorized overlap between two part of the context when splitting it is needed.\n",
    "doc_stride = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **为何设置 `max_length=384`？**\n",
    "1. **模型限制**  \n",
    "   BERT等模型最大支持 **512 tokens**，需为以下内容留空间：  \n",
    "   - **问题本身**（约20-30 tokens）  \n",
    "   - **特殊标记**（如 `[CLS]`、`[SEP]`，占3-5 tokens）  \n",
    "   - **答案位置**（避免被截断）\n",
    "\n",
    "2. **经验比例**  \n",
    "   可用上下文长度 ≈ 总长的 **75%**（512×0.75≈384），平衡覆盖率和计算效率。\n",
    "\n",
    "3. **分块优化**  \n",
    "   结合 `doc_stride=128`（重叠量），确保答案在至少一个分块中完整出现。\n",
    "\n",
    "---\n",
    "\n",
    "### **实际案例**  \n",
    "- **输入**：问题（20 tokens）+ 上下文（500 tokens）  \n",
    "- **处理**：  \n",
    "  1. 分块1：问题 + 上下文0-363  \n",
    "  2. 分块2：问题 + 上下文236-500（与分块1重叠128 tokens）  \n",
    "- **结果**：即使答案在360-400区间，也能被分块2覆盖。\n",
    "\n",
    "---\n",
    "\n",
    "### **调整建议**\n",
    "- **短文本任务**：直接设为512  \n",
    "- **超长文档**：可降低到256（需更多分块）  \n",
    "- **支持更长模型**：如支持1024，可设为768  \n",
    "\n",
    "一句话总结：**384是平衡模型限制、答案完整性和计算效率的经验值。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`doc_stride=128` 的原理与 `max_length=384` 类似，但关注点不同。以下是简洁清晰的解释：\n",
    "\n",
    "---\n",
    "\n",
    "### **为何设置 `doc_stride=128`？**\n",
    "1. **核心目的**  \n",
    "   **避免答案被切割在分块边界**。通过设置分块间的重叠区域，确保即使答案位于分块边缘，也能被至少一个完整分块覆盖。\n",
    "\n",
    "2. **经验公式**  \n",
    "   `doc_stride` ≈ `max_length` 的 **1/3~1/4**（如 `384/3≈128`），平衡：\n",
    "   - **计算效率**（分块越少越好）\n",
    "   - **答案覆盖率**（重叠越多越安全）\n",
    "\n",
    "---\n",
    "\n",
    "### **分块逻辑示例**\n",
    "- **参数**：\n",
    "  - `max_length=384`（总长度）\n",
    "  - 问题长度 = 20 tokens\n",
    "  - 可用上下文长度 = `384 - 20 - 3（特殊标记）≈ 361 tokens`\n",
    "  - `doc_stride=128`\n",
    "- **分块步长** = `361 - 128 = 233 tokens`\n",
    "\n",
    "| 分块 | 起始位置 | 结束位置 | 覆盖的上下文范围 |\n",
    "|------|----------|----------|------------------|\n",
    "| 1    | 0        | 360      | tokens 0-360     |\n",
    "| 2    | 233      | 593      | tokens 233-593   |\n",
    "| 3    | 466      | 826      | tokens 466-826   |\n",
    "\n",
    "- **假设答案在 tokens 350-370**：\n",
    "  - 分块1：覆盖到360 → 答案部分截断（350-360保留）\n",
    "  - 分块2：从233开始 → 完整覆盖答案（350-370）\n",
    "\n",
    "---\n",
    "\n",
    "### **关键影响**\n",
    "| `doc_stride` 值 | 优点               | 缺点                 |\n",
    "|-----------------|--------------------|----------------------|\n",
    "| **较小（如64）** | 答案覆盖率↑        | 分块数量↑，计算量↑   |\n",
    "| **较大（如192）**| 分块数量↓，速度↑   | 漏答风险↑            |\n",
    "\n",
    "---\n",
    "\n",
    "### **调整建议**\n",
    "- **短答案任务**（如实体抽取）：`doc_stride=64~128`\n",
    "- **长答案任务**（如段落总结）：`doc_stride=128~256`\n",
    "\n",
    "一句话总结：**`doc_stride=128` 是经验性参数，通过分块重叠平衡效率与答案完整性。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "假设我们有以下参数：\n",
    "- **`max_length = 10`**（每个片段最多包含10个字符）\n",
    "- **`doc_stride = 4`**（相邻片段重叠4个字符）\n",
    "\n",
    "---\n",
    "\n",
    "### **切割过程**\n",
    "原始文本：`ABCDEFGHIJKLMN`（假设每个字母代表一个token）\n",
    "\n",
    "1. **第一个片段**：  \n",
    "   - 取前10个字符 → `ABCDEFGHIJ`（A到J）\n",
    "   - 结束位置：第10个字符（J）\n",
    "\n",
    "2. **第二个片段**：  \n",
    "   - 起始位置 = 前片段的起始位置 + (`max_length - doc_stride`) = 0 + (10 - 4) = 6  \n",
    "     （即从第7个字符开始，对应字母 `G`）\n",
    "   - 实际字符：`GHIJKLMN`（G到N，共8个字符，不足10个则保留）\n",
    "   - 重叠部分：`GHIJ`（与前一片段的后4个字符重叠）\n",
    "\n",
    "---\n",
    "\n",
    "### **图示切割效果**\n",
    "```\n",
    "原始文本： A B C D E F G H I J K L M N\n",
    "片段1：    [A B C D E F G H I J]          → 长度10\n",
    "片段2：            [G H I J K L M N]      → 起始位置6，重叠4个字符\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **为什么需要重叠？**\n",
    "假设答案在 `H I J K` 区域：\n",
    "- **无重叠**：可能被截断在片段1末尾或片段2开头\n",
    "- **有重叠**：确保答案完整包含在至少一个片段中\n",
    "\n",
    "---\n",
    "\n",
    "### **实际问答中的参数**\n",
    "当 `max_length=384` 且 `doc_stride=128` 时，逻辑完全一致，只是数值更大。这种滑动窗口切割是处理长文本问答的常用策略！ 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 超出最大长度的文本数据处理\n",
    "\n",
    "下面，我们从训练集中找出一个超过最大长度（384）的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "    if len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"]) > 384:\n",
    "        break\n",
    "# 挑选出来超过384（最大长度）的数据样例\n",
    "example = datasets[\"train\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 截断上下文不保留超出部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(example[\"question\"],\n",
    "              example[\"context\"],\n",
    "              max_length=max_length,\n",
    "              truncation=\"only_second\")[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于截断的策略\n",
    "\n",
    "- 直接截断超出部分: truncation=`only_second`\n",
    "- 仅截断上下文（context），保留问题（question）：`return_overflowing_tokens=True` & 设置`stride`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=doc_stride\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用此策略截断后，Tokenizer 将返回多个 `input_ids` 列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[384, 157]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(x) for x in tokenized_example[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码两个输入特征，可以看到重叠的部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] the men's basketball team has over 1, 600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 ncaa tournaments. former player austin carr holds the record for most points scored in a single game of the tournament with 61. although the team has never won the ncaa tournament, they were named by the helms athletic foundation as national champions twice. the team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending ucla's record 88 - game winning streak in 1974. the team has beaten an additional eight number - one teams, and those nine wins rank second, to ucla's 10, all - time in wins against the top team. the team plays in newly renovated purcell pavilion ( within the edmund p. joyce center ), which reopened for the beginning of the 2009 – 2010 season. the team is coached by mike brey, who, as of the 2014 – 15 season, his fifteenth at notre dame, has achieved a 332 - 165 record. in 2009 they were invited to the nit, where they advanced to the semifinals but were beaten by penn state who went on and beat baylor in the championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were [SEP]\n",
      "[CLS] how many wins does the notre dame men's basketball team have? [SEP] championship. the 2010 – 11 team concluded its regular season ranked number seven in the country, with a record of 25 – 5, brey's fifth straight 20 - win season, and a second - place finish in the big east. during the 2014 - 15 season, the team went 32 - 6 and won the acc conference tournament, later advancing to the elite 8, where the fighting irish lost on a missed buzzer - beater against then undefeated kentucky. led by nba draft picks jerian grant and pat connaughton, the fighting irish beat the eventual national champion duke blue devils twice during the season. the 32 wins were the most by the fighting irish team since 1908 - 09. [SEP]\n"
     ]
    }
   ],
   "source": [
    "for x in tokenized_example[\"input_ids\"][:2]:\n",
    "    print(tokenizer.decode(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用 offsets_mapping 获取原始的 input_ids\n",
    "\n",
    "设置 `return_offsets_mapping=True`，将使得截断分割生成的多个 input_ids 列表中的 token，通过映射保留原始文本的 input_ids。\n",
    "\n",
    "如下所示：第一个标记（[CLS]）的起始和结束字符都是（0, 0），因为它不对应问题/答案的任何部分，然后第二个标记与问题(question)的字符0到3相同."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), (23, 28), (29, 33), (34, 37), (37, 38), (38, 39), (40, 50), (51, 55), (56, 60), (60, 61), (0, 0), (0, 3), (4, 7), (7, 8), (8, 9), (10, 20), (21, 25), (26, 29), (30, 34), (35, 36), (36, 37), (37, 40), (41, 45), (45, 46), (47, 50), (51, 53), (54, 58), (59, 61), (62, 69), (70, 73), (74, 78), (79, 86), (87, 91), (92, 96), (96, 97), (98, 101), (102, 106), (107, 115), (116, 118), (119, 121), (122, 126), (127, 138), (138, 139), (140, 146), (147, 153), (154, 160), (161, 165), (166, 171), (172, 175), (176, 182), (183, 186), (187, 191), (192, 198), (199, 205), (206, 208), (209, 210), (211, 217), (218, 222), (223, 225), (226, 229), (230, 240), (241, 245), (246, 248), (248, 249), (250, 258), (259, 262), (263, 267), (268, 271), (272, 277), (278, 281), (282, 285), (286, 290), (291, 301), (301, 302), (303, 307), (308, 312), (313, 318), (319, 321), (322, 325), (326, 330), (330, 331), (332, 340), (341, 351), (352, 354), (355, 363), (364, 373), (374, 379), (379, 380), (381, 384), (385, 389), (390, 393), (394, 406), (407, 408), (409, 415), (416, 418)]\n"
     ]
    }
   ],
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],            # 第一个参数：问题文本\n",
    "    example[\"context\"],             # 第二个参数：上下文文本\n",
    "    max_length=max_length,          # 最大输入长度（如384）\n",
    "    truncation=\"only_second\",       # 关键参数1：截断策略\n",
    "    return_overflowing_tokens=True, # 关键参数2：返回分块结果\n",
    "    return_offsets_mapping=True,    # 关键参数3：返回字符级位置映射\n",
    "    return_token_type_ids=True,     # 显式要求返回 token_type_ids\n",
    "    stride=doc_stride               # 分块滑动步长（如128）\n",
    ")\n",
    "print(tokenized_example[\"offset_mapping\"][0][:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **参数详解**\n",
    "#### 1. `truncation=\"only_second\"`\n",
    "- **作用**：**只截断第二个参数（上下文）**，保持第一个参数（问题）完整\n",
    "- **场景**：当 `问题+上下文` 总长度超过 `max_length` 时，优先保留问题完整性\n",
    "- **示例**：\n",
    "  ```python\n",
    "  # 输入：问题长度20，上下文长度400 → 总长度420 > 384\n",
    "  # 处理：截断上下文为 384-20-3（特殊标记）= 361 tokens\n",
    "  ```\n",
    "\n",
    "#### 2. `return_overflowing_tokens=True`\n",
    "- **作用**：**返回分块后的多个输入特征**（当输入过长时自动分割）\n",
    "- **输出字段**：`overflow_to_sample_mapping`（分块对应原始样本的索引）\n",
    "- **分块逻辑**：\n",
    "  - 将长上下文按 `max_length - 问题长度` 切割\n",
    "  - 相邻分块重叠 `stride` tokens（确保答案不被切割）\n",
    "\n",
    "#### 3. `return_offsets_mapping=True`\n",
    "- **作用**：**返回每个 token 在原始文本中的字符位置**（起始和结束索引）\n",
    "- **输出字段**：`offset_mapping`（列表的列表，每个元素是 `(start, end)` 元组）\n",
    "- **关键用途**：将模型预测的 token 位置映射回原始文本（如定位答案）\n",
    "\n",
    "---\n",
    "\n",
    "### **`offset_mapping` 示例解析**\n",
    "```python\n",
    "# 假设打印结果前5个元素：\n",
    "[(0, 0), (0, 3), (4, 7), (8, 11), (12, 15), ...]\n",
    "\n",
    "# 对应含义：\n",
    "# [CLS]  What    is    your   name?  [SEP] ...\n",
    "# (0,0) (0,3) (4,7) (8,11) (12,15)   ...\n",
    "```\n",
    "- **特殊标记**：`[CLS]`、`[SEP]` 等无对应文本 → `(0, 0)`\n",
    "- **问题部分**：字符索引从问题文本的起始位置计算\n",
    "- **上下文部分**：字符索引从上下文文本的起始位置计算（需注意问题文本长度）\n",
    "\n",
    "---\n",
    "\n",
    "### **参数协同作用**\n",
    "| 参数组合                        | 实际效果                                                                 |\n",
    "|---------------------------------|------------------------------------------------------------------------|\n",
    "| `truncation=\"only_second\"` + `return_overflowing_tokens=True` | 将长上下文切割为多个分块，每个分块包含完整问题和部分上下文 |\n",
    "| `return_offsets_mapping=True`   | 提供分块中每个 token 在原始文本中的位置，用于答案位置映射               |\n",
    "\n",
    "---\n",
    "\n",
    "### **应用场景**\n",
    "1. **训练阶段**：将答案的字符位置转换为分块内的 token 位置\n",
    "2. **推理阶段**：将模型预测的 token 位置反向映射到原始上下文\n",
    "3. **数据验证**：检查分块是否覆盖正确答案的原始位置\n",
    "\n",
    "---\n",
    "\n",
    "通过这三个参数，实现了长文本问答任务中 **输入分块处理** 和 **位置精确映射** 的核心需求。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们可以使用这个映射来找到答案在给定特征中的起始和结束标记的位置。\n",
    "\n",
    "我们只需区分偏移的哪些部分对应于问题，哪些部分对应于上下文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2129\n",
      "(0, 3)\n",
      "how How\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(first_token_id)\n",
    "print(offsets)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "many many\n"
     ]
    }
   ],
   "source": [
    "second_token_id = tokenized_example[\"input_ids\"][0][2]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][2]\n",
    "print(tokenizer.convert_ids_to_tokens([second_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8214\n",
      "(29, 33)\n",
      "dame Dame\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][7]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][7]\n",
    "print(first_token_id)\n",
    "print(offsets)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1055\n",
      "(38, 39)\n",
      "s 0\n"
     ]
    }
   ],
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][10]\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][10]\n",
    "print(first_token_id)\n",
    "print(offsets)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"context\"][offsets[0]:offsets[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 分块 0 ===\n",
      "0 0\n",
      "Token 0: [CLS] → \n",
      "0 3\n",
      "Token 1: how → How\n",
      "4 8\n",
      "Token 2: many → many\n",
      "9 13\n",
      "Token 3: wins → wins\n",
      "14 18\n",
      "Token 4: does → does\n",
      "19 22\n",
      "Token 5: the → the\n",
      "23 28\n",
      "Token 6: notre → Notre\n",
      "29 33\n",
      "Token 7: dame → Dame\n",
      "34 37\n",
      "Token 8: men → men\n",
      "37 38\n",
      "Token 9: ' → '\n",
      "38 39\n",
      "Token 10: s → s\n",
      "40 50\n",
      "Token 11: basketball → basketball\n",
      "51 55\n",
      "Token 12: team → team\n",
      "56 60\n",
      "Token 13: have → have\n",
      "60 61\n",
      "Token 14: ? → ?\n",
      "0 0\n",
      "Token 15: [SEP] → \n",
      "0 3\n",
      "Token 16: the → The\n",
      "4 7\n",
      "Token 17: men → men\n",
      "7 8\n",
      "Token 18: ' → '\n",
      "8 9\n",
      "Token 19: s → s\n",
      "10 20\n",
      "Token 20: basketball → basketball\n",
      "21 25\n",
      "Token 21: team → team\n",
      "26 29\n",
      "Token 22: has → has\n",
      "30 34\n",
      "Token 23: over → over\n",
      "35 36\n",
      "Token 24: 1 → 1\n",
      "36 37\n",
      "Token 25: , → ,\n",
      "37 40\n",
      "Token 26: 600 → 600\n",
      "41 45\n",
      "Token 27: wins → wins\n",
      "45 46\n",
      "Token 28: , → ,\n",
      "47 50\n",
      "Token 29: one → one\n",
      "51 53\n",
      "Token 30: of → of\n",
      "54 58\n",
      "Token 31: only → only\n",
      "59 61\n",
      "Token 32: 12 → 12\n",
      "62 69\n",
      "Token 33: schools → schools\n",
      "70 73\n",
      "Token 34: who → who\n",
      "74 78\n",
      "Token 35: have → have\n",
      "79 86\n",
      "Token 36: reached → reached\n",
      "87 91\n",
      "Token 37: that → that\n",
      "92 96\n",
      "Token 38: mark → mark\n",
      "96 97\n",
      "Token 39: , → ,\n",
      "98 101\n",
      "Token 40: and → and\n",
      "102 106\n",
      "Token 41: have → have\n",
      "107 115\n",
      "Token 42: appeared → appeared\n",
      "116 118\n",
      "Token 43: in → in\n",
      "119 121\n",
      "Token 44: 28 → 28\n",
      "122 126\n",
      "Token 45: ncaa → NCAA\n",
      "127 138\n",
      "Token 46: tournaments → tournaments\n",
      "138 139\n",
      "Token 47: . → .\n",
      "140 146\n",
      "Token 48: former → Former\n",
      "147 153\n",
      "Token 49: player → player\n",
      "154 160\n",
      "Token 50: austin → Austin\n",
      "161 165\n",
      "Token 51: carr → Carr\n",
      "166 171\n",
      "Token 52: holds → holds\n",
      "172 175\n",
      "Token 53: the → the\n",
      "176 182\n",
      "Token 54: record → record\n",
      "183 186\n",
      "Token 55: for → for\n",
      "187 191\n",
      "Token 56: most → most\n",
      "192 198\n",
      "Token 57: points → points\n",
      "199 205\n",
      "Token 58: scored → scored\n",
      "206 208\n",
      "Token 59: in → in\n",
      "209 210\n",
      "Token 60: a → a\n",
      "211 217\n",
      "Token 61: single → single\n",
      "218 222\n",
      "Token 62: game → game\n",
      "223 225\n",
      "Token 63: of → of\n",
      "226 229\n",
      "Token 64: the → the\n",
      "230 240\n",
      "Token 65: tournament → tournament\n",
      "241 245\n",
      "Token 66: with → with\n",
      "246 248\n",
      "Token 67: 61 → 61\n",
      "248 249\n",
      "Token 68: . → .\n",
      "250 258\n",
      "Token 69: although → Although\n",
      "259 262\n",
      "Token 70: the → the\n",
      "263 267\n",
      "Token 71: team → team\n",
      "268 271\n",
      "Token 72: has → has\n",
      "272 277\n",
      "Token 73: never → never\n",
      "278 281\n",
      "Token 74: won → won\n",
      "282 285\n",
      "Token 75: the → the\n",
      "286 290\n",
      "Token 76: ncaa → NCAA\n",
      "291 301\n",
      "Token 77: tournament → Tournament\n",
      "301 302\n",
      "Token 78: , → ,\n",
      "303 307\n",
      "Token 79: they → they\n",
      "308 312\n",
      "Token 80: were → were\n",
      "313 318\n",
      "Token 81: named → named\n",
      "319 321\n",
      "Token 82: by → by\n",
      "322 325\n",
      "Token 83: the → the\n",
      "326 330\n",
      "Token 84: helm → Helm\n",
      "330 331\n",
      "Token 85: ##s → s\n",
      "332 340\n",
      "Token 86: athletic → Athletic\n",
      "341 351\n",
      "Token 87: foundation → Foundation\n",
      "352 354\n",
      "Token 88: as → as\n",
      "355 363\n",
      "Token 89: national → national\n",
      "364 373\n",
      "Token 90: champions → champions\n",
      "374 379\n",
      "Token 91: twice → twice\n",
      "379 380\n",
      "Token 92: . → .\n",
      "381 384\n",
      "Token 93: the → The\n",
      "385 389\n",
      "Token 94: team → team\n",
      "390 393\n",
      "Token 95: has → has\n",
      "394 406\n",
      "Token 96: orchestrated → orchestrated\n",
      "407 408\n",
      "Token 97: a → a\n",
      "409 415\n",
      "Token 98: number → number\n",
      "416 418\n",
      "Token 99: of → of\n",
      "419 424\n",
      "Token 100: upset → upset\n",
      "424 425\n",
      "Token 101: ##s → s\n",
      "426 428\n",
      "Token 102: of → of\n",
      "429 435\n",
      "Token 103: number → number\n",
      "436 439\n",
      "Token 104: one → one\n",
      "440 446\n",
      "Token 105: ranked → ranked\n",
      "447 452\n",
      "Token 106: teams → teams\n",
      "452 453\n",
      "Token 107: , → ,\n",
      "454 457\n",
      "Token 108: the → the\n",
      "458 462\n",
      "Token 109: most → most\n",
      "463 470\n",
      "Token 110: notable → notable\n",
      "471 473\n",
      "Token 111: of → of\n",
      "474 479\n",
      "Token 112: which → which\n",
      "480 483\n",
      "Token 113: was → was\n",
      "484 490\n",
      "Token 114: ending → ending\n",
      "491 495\n",
      "Token 115: ucla → UCLA\n",
      "495 496\n",
      "Token 116: ' → '\n",
      "496 497\n",
      "Token 117: s → s\n",
      "498 504\n",
      "Token 118: record → record\n",
      "505 507\n",
      "Token 119: 88 → 88\n",
      "507 508\n",
      "Token 120: - → -\n",
      "508 512\n",
      "Token 121: game → game\n",
      "513 520\n",
      "Token 122: winning → winning\n",
      "521 527\n",
      "Token 123: streak → streak\n",
      "528 530\n",
      "Token 124: in → in\n",
      "531 535\n",
      "Token 125: 1974 → 1974\n",
      "535 536\n",
      "Token 126: . → .\n",
      "537 540\n",
      "Token 127: the → The\n",
      "541 545\n",
      "Token 128: team → team\n",
      "546 549\n",
      "Token 129: has → has\n",
      "550 556\n",
      "Token 130: beaten → beaten\n",
      "557 559\n",
      "Token 131: an → an\n",
      "560 570\n",
      "Token 132: additional → additional\n",
      "571 576\n",
      "Token 133: eight → eight\n",
      "577 583\n",
      "Token 134: number → number\n",
      "583 584\n",
      "Token 135: - → -\n",
      "584 587\n",
      "Token 136: one → one\n",
      "588 593\n",
      "Token 137: teams → teams\n",
      "593 594\n",
      "Token 138: , → ,\n",
      "595 598\n",
      "Token 139: and → and\n",
      "599 604\n",
      "Token 140: those → those\n",
      "605 609\n",
      "Token 141: nine → nine\n",
      "610 614\n",
      "Token 142: wins → wins\n",
      "615 619\n",
      "Token 143: rank → rank\n",
      "620 626\n",
      "Token 144: second → second\n",
      "626 627\n",
      "Token 145: , → ,\n",
      "628 630\n",
      "Token 146: to → to\n",
      "631 635\n",
      "Token 147: ucla → UCLA\n",
      "635 636\n",
      "Token 148: ' → '\n",
      "636 637\n",
      "Token 149: s → s\n",
      "638 640\n",
      "Token 150: 10 → 10\n",
      "640 641\n",
      "Token 151: , → ,\n",
      "642 645\n",
      "Token 152: all → all\n",
      "645 646\n",
      "Token 153: - → -\n",
      "646 650\n",
      "Token 154: time → time\n",
      "651 653\n",
      "Token 155: in → in\n",
      "654 658\n",
      "Token 156: wins → wins\n",
      "659 666\n",
      "Token 157: against → against\n",
      "667 670\n",
      "Token 158: the → the\n",
      "671 674\n",
      "Token 159: top → top\n",
      "675 679\n",
      "Token 160: team → team\n",
      "679 680\n",
      "Token 161: . → .\n",
      "681 684\n",
      "Token 162: the → The\n",
      "685 689\n",
      "Token 163: team → team\n",
      "690 695\n",
      "Token 164: plays → plays\n",
      "696 698\n",
      "Token 165: in → in\n",
      "699 704\n",
      "Token 166: newly → newly\n",
      "705 714\n",
      "Token 167: renovated → renovated\n",
      "715 722\n",
      "Token 168: purcell → Purcell\n",
      "723 731\n",
      "Token 169: pavilion → Pavilion\n",
      "732 733\n",
      "Token 170: ( → (\n",
      "733 739\n",
      "Token 171: within → within\n",
      "740 743\n",
      "Token 172: the → the\n",
      "744 750\n",
      "Token 173: edmund → Edmund\n",
      "751 752\n",
      "Token 174: p → P\n",
      "752 753\n",
      "Token 175: . → .\n",
      "754 759\n",
      "Token 176: joyce → Joyce\n",
      "760 766\n",
      "Token 177: center → Center\n",
      "766 767\n",
      "Token 178: ) → )\n",
      "767 768\n",
      "Token 179: , → ,\n",
      "769 774\n",
      "Token 180: which → which\n",
      "775 783\n",
      "Token 181: reopened → reopened\n",
      "784 787\n",
      "Token 182: for → for\n",
      "788 791\n",
      "Token 183: the → the\n",
      "792 801\n",
      "Token 184: beginning → beginning\n",
      "802 804\n",
      "Token 185: of → of\n",
      "805 808\n",
      "Token 186: the → the\n",
      "809 813\n",
      "Token 187: 2009 → 2009\n",
      "813 814\n",
      "Token 188: – → –\n",
      "814 818\n",
      "Token 189: 2010 → 2010\n",
      "819 825\n",
      "Token 190: season → season\n",
      "825 826\n",
      "Token 191: . → .\n",
      "827 830\n",
      "Token 192: the → The\n",
      "831 835\n",
      "Token 193: team → team\n",
      "836 838\n",
      "Token 194: is → is\n",
      "839 846\n",
      "Token 195: coached → coached\n",
      "847 849\n",
      "Token 196: by → by\n",
      "850 854\n",
      "Token 197: mike → Mike\n",
      "855 857\n",
      "Token 198: br → Br\n",
      "857 859\n",
      "Token 199: ##ey → ey\n",
      "859 860\n",
      "Token 200: , → ,\n",
      "861 864\n",
      "Token 201: who → who\n",
      "864 865\n",
      "Token 202: , → ,\n",
      "866 868\n",
      "Token 203: as → as\n",
      "869 871\n",
      "Token 204: of → of\n",
      "872 875\n",
      "Token 205: the → the\n",
      "876 880\n",
      "Token 206: 2014 → 2014\n",
      "880 881\n",
      "Token 207: – → –\n",
      "881 883\n",
      "Token 208: 15 → 15\n",
      "884 890\n",
      "Token 209: season → season\n",
      "890 891\n",
      "Token 210: , → ,\n",
      "892 895\n",
      "Token 211: his → his\n",
      "896 905\n",
      "Token 212: fifteenth → fifteenth\n",
      "906 908\n",
      "Token 213: at → at\n",
      "909 914\n",
      "Token 214: notre → Notre\n",
      "915 919\n",
      "Token 215: dame → Dame\n",
      "919 920\n",
      "Token 216: , → ,\n",
      "921 924\n",
      "Token 217: has → has\n",
      "925 933\n",
      "Token 218: achieved → achieved\n",
      "934 935\n",
      "Token 219: a → a\n",
      "936 939\n",
      "Token 220: 332 → 332\n",
      "939 940\n",
      "Token 221: - → -\n",
      "940 943\n",
      "Token 222: 165 → 165\n",
      "944 950\n",
      "Token 223: record → record\n",
      "950 951\n",
      "Token 224: . → .\n",
      "952 954\n",
      "Token 225: in → In\n",
      "955 959\n",
      "Token 226: 2009 → 2009\n",
      "960 964\n",
      "Token 227: they → they\n",
      "965 969\n",
      "Token 228: were → were\n",
      "970 977\n",
      "Token 229: invited → invited\n",
      "978 980\n",
      "Token 230: to → to\n",
      "981 984\n",
      "Token 231: the → the\n",
      "985 987\n",
      "Token 232: ni → NI\n",
      "987 988\n",
      "Token 233: ##t → T\n",
      "988 989\n",
      "Token 234: , → ,\n",
      "990 995\n",
      "Token 235: where → where\n",
      "996 1000\n",
      "Token 236: they → they\n",
      "1001 1009\n",
      "Token 237: advanced → advanced\n",
      "1010 1012\n",
      "Token 238: to → to\n",
      "1013 1016\n",
      "Token 239: the → the\n",
      "1017 1027\n",
      "Token 240: semifinals → semifinals\n",
      "1028 1031\n",
      "Token 241: but → but\n",
      "1032 1036\n",
      "Token 242: were → were\n",
      "1037 1043\n",
      "Token 243: beaten → beaten\n",
      "1044 1046\n",
      "Token 244: by → by\n",
      "1047 1051\n",
      "Token 245: penn → Penn\n",
      "1052 1057\n",
      "Token 246: state → State\n",
      "1058 1061\n",
      "Token 247: who → who\n",
      "1062 1066\n",
      "Token 248: went → went\n",
      "1067 1069\n",
      "Token 249: on → on\n",
      "1070 1073\n",
      "Token 250: and → and\n",
      "1074 1078\n",
      "Token 251: beat → beat\n",
      "1079 1085\n",
      "Token 252: baylor → Baylor\n",
      "1086 1088\n",
      "Token 253: in → in\n",
      "1089 1092\n",
      "Token 254: the → the\n",
      "1093 1105\n",
      "Token 255: championship → championship\n",
      "1105 1106\n",
      "Token 256: . → .\n",
      "1107 1110\n",
      "Token 257: the → The\n",
      "1111 1115\n",
      "Token 258: 2010 → 2010\n",
      "1115 1116\n",
      "Token 259: – → –\n",
      "1116 1118\n",
      "Token 260: 11 → 11\n",
      "1119 1123\n",
      "Token 261: team → team\n",
      "1124 1133\n",
      "Token 262: concluded → concluded\n",
      "1134 1137\n",
      "Token 263: its → its\n",
      "1138 1145\n",
      "Token 264: regular → regular\n",
      "1146 1152\n",
      "Token 265: season → season\n",
      "1153 1159\n",
      "Token 266: ranked → ranked\n",
      "1160 1166\n",
      "Token 267: number → number\n",
      "1167 1172\n",
      "Token 268: seven → seven\n",
      "1173 1175\n",
      "Token 269: in → in\n",
      "1176 1179\n",
      "Token 270: the → the\n",
      "1180 1187\n",
      "Token 271: country → country\n",
      "1187 1188\n",
      "Token 272: , → ,\n",
      "1189 1193\n",
      "Token 273: with → with\n",
      "1194 1195\n",
      "Token 274: a → a\n",
      "1196 1202\n",
      "Token 275: record → record\n",
      "1203 1205\n",
      "Token 276: of → of\n",
      "1206 1208\n",
      "Token 277: 25 → 25\n",
      "1208 1209\n",
      "Token 278: – → –\n",
      "1209 1210\n",
      "Token 279: 5 → 5\n",
      "1210 1211\n",
      "Token 280: , → ,\n",
      "1212 1214\n",
      "Token 281: br → Br\n",
      "1214 1216\n",
      "Token 282: ##ey → ey\n",
      "1216 1217\n",
      "Token 283: ' → '\n",
      "1217 1218\n",
      "Token 284: s → s\n",
      "1219 1224\n",
      "Token 285: fifth → fifth\n",
      "1225 1233\n",
      "Token 286: straight → straight\n",
      "1234 1236\n",
      "Token 287: 20 → 20\n",
      "1236 1237\n",
      "Token 288: - → -\n",
      "1237 1240\n",
      "Token 289: win → win\n",
      "1241 1247\n",
      "Token 290: season → season\n",
      "1247 1248\n",
      "Token 291: , → ,\n",
      "1249 1252\n",
      "Token 292: and → and\n",
      "1253 1254\n",
      "Token 293: a → a\n",
      "1255 1261\n",
      "Token 294: second → second\n",
      "1261 1262\n",
      "Token 295: - → -\n",
      "1262 1267\n",
      "Token 296: place → place\n",
      "1268 1274\n",
      "Token 297: finish → finish\n",
      "1275 1277\n",
      "Token 298: in → in\n",
      "1278 1281\n",
      "Token 299: the → the\n",
      "1282 1285\n",
      "Token 300: big → Big\n",
      "1286 1290\n",
      "Token 301: east → East\n",
      "1290 1291\n",
      "Token 302: . → .\n",
      "1292 1298\n",
      "Token 303: during → During\n",
      "1299 1302\n",
      "Token 304: the → the\n",
      "1303 1307\n",
      "Token 305: 2014 → 2014\n",
      "1307 1308\n",
      "Token 306: - → -\n",
      "1308 1310\n",
      "Token 307: 15 → 15\n",
      "1311 1317\n",
      "Token 308: season → season\n",
      "1317 1318\n",
      "Token 309: , → ,\n",
      "1319 1322\n",
      "Token 310: the → the\n",
      "1323 1327\n",
      "Token 311: team → team\n",
      "1328 1332\n",
      "Token 312: went → went\n",
      "1333 1335\n",
      "Token 313: 32 → 32\n",
      "1335 1336\n",
      "Token 314: - → -\n",
      "1336 1337\n",
      "Token 315: 6 → 6\n",
      "1338 1341\n",
      "Token 316: and → and\n",
      "1342 1345\n",
      "Token 317: won → won\n",
      "1346 1349\n",
      "Token 318: the → the\n",
      "1350 1353\n",
      "Token 319: acc → ACC\n",
      "1354 1364\n",
      "Token 320: conference → conference\n",
      "1365 1375\n",
      "Token 321: tournament → tournament\n",
      "1375 1376\n",
      "Token 322: , → ,\n",
      "1377 1382\n",
      "Token 323: later → later\n",
      "1383 1392\n",
      "Token 324: advancing → advancing\n",
      "1393 1395\n",
      "Token 325: to → to\n",
      "1396 1399\n",
      "Token 326: the → the\n",
      "1400 1405\n",
      "Token 327: elite → Elite\n",
      "1406 1407\n",
      "Token 328: 8 → 8\n",
      "1407 1408\n",
      "Token 329: , → ,\n",
      "1409 1414\n",
      "Token 330: where → where\n",
      "1415 1418\n",
      "Token 331: the → the\n",
      "1419 1427\n",
      "Token 332: fighting → Fighting\n",
      "1428 1433\n",
      "Token 333: irish → Irish\n",
      "1434 1438\n",
      "Token 334: lost → lost\n",
      "1439 1441\n",
      "Token 335: on → on\n",
      "1442 1443\n",
      "Token 336: a → a\n",
      "1444 1450\n",
      "Token 337: missed → missed\n",
      "1451 1455\n",
      "Token 338: buzz → buzz\n",
      "1455 1457\n",
      "Token 339: ##er → er\n",
      "1457 1458\n",
      "Token 340: - → -\n",
      "1458 1462\n",
      "Token 341: beat → beat\n",
      "1462 1464\n",
      "Token 342: ##er → er\n",
      "1465 1472\n",
      "Token 343: against → against\n",
      "1473 1477\n",
      "Token 344: then → then\n",
      "1478 1488\n",
      "Token 345: undefeated → undefeated\n",
      "1489 1497\n",
      "Token 346: kentucky → Kentucky\n",
      "1497 1498\n",
      "Token 347: . → .\n",
      "1499 1502\n",
      "Token 348: led → Led\n",
      "1503 1505\n",
      "Token 349: by → by\n",
      "1506 1509\n",
      "Token 350: nba → NBA\n",
      "1510 1515\n",
      "Token 351: draft → draft\n",
      "1516 1521\n",
      "Token 352: picks → picks\n",
      "1522 1524\n",
      "Token 353: je → Je\n",
      "1524 1528\n",
      "Token 354: ##rian → rian\n",
      "1529 1534\n",
      "Token 355: grant → Grant\n",
      "1535 1538\n",
      "Token 356: and → and\n",
      "1539 1542\n",
      "Token 357: pat → Pat\n",
      "1543 1546\n",
      "Token 358: con → Con\n",
      "1546 1548\n",
      "Token 359: ##na → na\n",
      "1548 1552\n",
      "Token 360: ##ught → ught\n",
      "1552 1554\n",
      "Token 361: ##on → on\n",
      "1554 1555\n",
      "Token 362: , → ,\n",
      "1556 1559\n",
      "Token 363: the → the\n",
      "1560 1568\n",
      "Token 364: fighting → Fighting\n",
      "1569 1574\n",
      "Token 365: irish → Irish\n",
      "1575 1579\n",
      "Token 366: beat → beat\n",
      "1580 1583\n",
      "Token 367: the → the\n",
      "1584 1592\n",
      "Token 368: eventual → eventual\n",
      "1593 1601\n",
      "Token 369: national → national\n",
      "1602 1610\n",
      "Token 370: champion → champion\n",
      "1611 1615\n",
      "Token 371: duke → Duke\n",
      "1616 1620\n",
      "Token 372: blue → Blue\n",
      "1621 1627\n",
      "Token 373: devils → Devils\n",
      "1628 1633\n",
      "Token 374: twice → twice\n",
      "1634 1640\n",
      "Token 375: during → during\n",
      "1641 1644\n",
      "Token 376: the → the\n",
      "1645 1651\n",
      "Token 377: season → season\n",
      "1651 1652\n",
      "Token 378: . → .\n",
      "1653 1656\n",
      "Token 379: the → The\n",
      "1657 1659\n",
      "Token 380: 32 → 32\n",
      "1660 1664\n",
      "Token 381: wins → wins\n",
      "1665 1669\n",
      "Token 382: were → were\n",
      "0 0\n",
      "Token 383: [SEP] → \n",
      "\n",
      "=== 分块 1 ===\n",
      "0 0\n",
      "Token 0: [CLS] → \n",
      "0 3\n",
      "Token 1: how → How\n",
      "4 8\n",
      "Token 2: many → many\n",
      "9 13\n",
      "Token 3: wins → wins\n",
      "14 18\n",
      "Token 4: does → does\n",
      "19 22\n",
      "Token 5: the → the\n",
      "23 28\n",
      "Token 6: notre → Notre\n",
      "29 33\n",
      "Token 7: dame → Dame\n",
      "34 37\n",
      "Token 8: men → men\n",
      "37 38\n",
      "Token 9: ' → '\n",
      "38 39\n",
      "Token 10: s → s\n",
      "40 50\n",
      "Token 11: basketball → basketball\n",
      "51 55\n",
      "Token 12: team → team\n",
      "56 60\n",
      "Token 13: have → have\n",
      "60 61\n",
      "Token 14: ? → ?\n",
      "0 0\n",
      "Token 15: [SEP] → \n",
      "1093 1105\n",
      "Token 16: championship → championship\n",
      "1105 1106\n",
      "Token 17: . → .\n",
      "1107 1110\n",
      "Token 18: the → The\n",
      "1111 1115\n",
      "Token 19: 2010 → 2010\n",
      "1115 1116\n",
      "Token 20: – → –\n",
      "1116 1118\n",
      "Token 21: 11 → 11\n",
      "1119 1123\n",
      "Token 22: team → team\n",
      "1124 1133\n",
      "Token 23: concluded → concluded\n",
      "1134 1137\n",
      "Token 24: its → its\n",
      "1138 1145\n",
      "Token 25: regular → regular\n",
      "1146 1152\n",
      "Token 26: season → season\n",
      "1153 1159\n",
      "Token 27: ranked → ranked\n",
      "1160 1166\n",
      "Token 28: number → number\n",
      "1167 1172\n",
      "Token 29: seven → seven\n",
      "1173 1175\n",
      "Token 30: in → in\n",
      "1176 1179\n",
      "Token 31: the → the\n",
      "1180 1187\n",
      "Token 32: country → country\n",
      "1187 1188\n",
      "Token 33: , → ,\n",
      "1189 1193\n",
      "Token 34: with → with\n",
      "1194 1195\n",
      "Token 35: a → a\n",
      "1196 1202\n",
      "Token 36: record → record\n",
      "1203 1205\n",
      "Token 37: of → of\n",
      "1206 1208\n",
      "Token 38: 25 → 25\n",
      "1208 1209\n",
      "Token 39: – → –\n",
      "1209 1210\n",
      "Token 40: 5 → 5\n",
      "1210 1211\n",
      "Token 41: , → ,\n",
      "1212 1214\n",
      "Token 42: br → Br\n",
      "1214 1216\n",
      "Token 43: ##ey → ey\n",
      "1216 1217\n",
      "Token 44: ' → '\n",
      "1217 1218\n",
      "Token 45: s → s\n",
      "1219 1224\n",
      "Token 46: fifth → fifth\n",
      "1225 1233\n",
      "Token 47: straight → straight\n",
      "1234 1236\n",
      "Token 48: 20 → 20\n",
      "1236 1237\n",
      "Token 49: - → -\n",
      "1237 1240\n",
      "Token 50: win → win\n",
      "1241 1247\n",
      "Token 51: season → season\n",
      "1247 1248\n",
      "Token 52: , → ,\n",
      "1249 1252\n",
      "Token 53: and → and\n",
      "1253 1254\n",
      "Token 54: a → a\n",
      "1255 1261\n",
      "Token 55: second → second\n",
      "1261 1262\n",
      "Token 56: - → -\n",
      "1262 1267\n",
      "Token 57: place → place\n",
      "1268 1274\n",
      "Token 58: finish → finish\n",
      "1275 1277\n",
      "Token 59: in → in\n",
      "1278 1281\n",
      "Token 60: the → the\n",
      "1282 1285\n",
      "Token 61: big → Big\n",
      "1286 1290\n",
      "Token 62: east → East\n",
      "1290 1291\n",
      "Token 63: . → .\n",
      "1292 1298\n",
      "Token 64: during → During\n",
      "1299 1302\n",
      "Token 65: the → the\n",
      "1303 1307\n",
      "Token 66: 2014 → 2014\n",
      "1307 1308\n",
      "Token 67: - → -\n",
      "1308 1310\n",
      "Token 68: 15 → 15\n",
      "1311 1317\n",
      "Token 69: season → season\n",
      "1317 1318\n",
      "Token 70: , → ,\n",
      "1319 1322\n",
      "Token 71: the → the\n",
      "1323 1327\n",
      "Token 72: team → team\n",
      "1328 1332\n",
      "Token 73: went → went\n",
      "1333 1335\n",
      "Token 74: 32 → 32\n",
      "1335 1336\n",
      "Token 75: - → -\n",
      "1336 1337\n",
      "Token 76: 6 → 6\n",
      "1338 1341\n",
      "Token 77: and → and\n",
      "1342 1345\n",
      "Token 78: won → won\n",
      "1346 1349\n",
      "Token 79: the → the\n",
      "1350 1353\n",
      "Token 80: acc → ACC\n",
      "1354 1364\n",
      "Token 81: conference → conference\n",
      "1365 1375\n",
      "Token 82: tournament → tournament\n",
      "1375 1376\n",
      "Token 83: , → ,\n",
      "1377 1382\n",
      "Token 84: later → later\n",
      "1383 1392\n",
      "Token 85: advancing → advancing\n",
      "1393 1395\n",
      "Token 86: to → to\n",
      "1396 1399\n",
      "Token 87: the → the\n",
      "1400 1405\n",
      "Token 88: elite → Elite\n",
      "1406 1407\n",
      "Token 89: 8 → 8\n",
      "1407 1408\n",
      "Token 90: , → ,\n",
      "1409 1414\n",
      "Token 91: where → where\n",
      "1415 1418\n",
      "Token 92: the → the\n",
      "1419 1427\n",
      "Token 93: fighting → Fighting\n",
      "1428 1433\n",
      "Token 94: irish → Irish\n",
      "1434 1438\n",
      "Token 95: lost → lost\n",
      "1439 1441\n",
      "Token 96: on → on\n",
      "1442 1443\n",
      "Token 97: a → a\n",
      "1444 1450\n",
      "Token 98: missed → missed\n",
      "1451 1455\n",
      "Token 99: buzz → buzz\n",
      "1455 1457\n",
      "Token 100: ##er → er\n",
      "1457 1458\n",
      "Token 101: - → -\n",
      "1458 1462\n",
      "Token 102: beat → beat\n",
      "1462 1464\n",
      "Token 103: ##er → er\n",
      "1465 1472\n",
      "Token 104: against → against\n",
      "1473 1477\n",
      "Token 105: then → then\n",
      "1478 1488\n",
      "Token 106: undefeated → undefeated\n",
      "1489 1497\n",
      "Token 107: kentucky → Kentucky\n",
      "1497 1498\n",
      "Token 108: . → .\n",
      "1499 1502\n",
      "Token 109: led → Led\n",
      "1503 1505\n",
      "Token 110: by → by\n",
      "1506 1509\n",
      "Token 111: nba → NBA\n",
      "1510 1515\n",
      "Token 112: draft → draft\n",
      "1516 1521\n",
      "Token 113: picks → picks\n",
      "1522 1524\n",
      "Token 114: je → Je\n",
      "1524 1528\n",
      "Token 115: ##rian → rian\n",
      "1529 1534\n",
      "Token 116: grant → Grant\n",
      "1535 1538\n",
      "Token 117: and → and\n",
      "1539 1542\n",
      "Token 118: pat → Pat\n",
      "1543 1546\n",
      "Token 119: con → Con\n",
      "1546 1548\n",
      "Token 120: ##na → na\n",
      "1548 1552\n",
      "Token 121: ##ught → ught\n",
      "1552 1554\n",
      "Token 122: ##on → on\n",
      "1554 1555\n",
      "Token 123: , → ,\n",
      "1556 1559\n",
      "Token 124: the → the\n",
      "1560 1568\n",
      "Token 125: fighting → Fighting\n",
      "1569 1574\n",
      "Token 126: irish → Irish\n",
      "1575 1579\n",
      "Token 127: beat → beat\n",
      "1580 1583\n",
      "Token 128: the → the\n",
      "1584 1592\n",
      "Token 129: eventual → eventual\n",
      "1593 1601\n",
      "Token 130: national → national\n",
      "1602 1610\n",
      "Token 131: champion → champion\n",
      "1611 1615\n",
      "Token 132: duke → Duke\n",
      "1616 1620\n",
      "Token 133: blue → Blue\n",
      "1621 1627\n",
      "Token 134: devils → Devils\n",
      "1628 1633\n",
      "Token 135: twice → twice\n",
      "1634 1640\n",
      "Token 136: during → during\n",
      "1641 1644\n",
      "Token 137: the → the\n",
      "1645 1651\n",
      "Token 138: season → season\n",
      "1651 1652\n",
      "Token 139: . → .\n",
      "1653 1656\n",
      "Token 140: the → The\n",
      "1657 1659\n",
      "Token 141: 32 → 32\n",
      "1660 1664\n",
      "Token 142: wins → wins\n",
      "1665 1669\n",
      "Token 143: were → were\n",
      "1670 1673\n",
      "Token 144: the → the\n",
      "1674 1678\n",
      "Token 145: most → most\n",
      "1679 1681\n",
      "Token 146: by → by\n",
      "1682 1685\n",
      "Token 147: the → the\n",
      "1686 1694\n",
      "Token 148: fighting → Fighting\n",
      "1695 1700\n",
      "Token 149: irish → Irish\n",
      "1701 1705\n",
      "Token 150: team → team\n",
      "1706 1711\n",
      "Token 151: since → since\n",
      "1712 1716\n",
      "Token 152: 1908 → 1908\n",
      "1716 1717\n",
      "Token 153: - → -\n",
      "1717 1719\n",
      "Token 154: 09 → 09\n",
      "1719 1720\n",
      "Token 155: . → .\n",
      "0 0\n",
      "Token 156: [SEP] → \n"
     ]
    }
   ],
   "source": [
    "# 遍历每个分块\n",
    "for chunk_idx in range(len(tokenized_example[\"input_ids\"])):\n",
    "    print(f\"\\n=== 分块 {chunk_idx} ===\")\n",
    "    \n",
    "    # 获取当前分块的数据\n",
    "    input_ids = tokenized_example[\"input_ids\"][chunk_idx]\n",
    "    offset_mapping = tokenized_example[\"offset_mapping\"][chunk_idx]\n",
    "    token_type_ids = tokenized_example[\"token_type_ids\"][chunk_idx]\n",
    "\n",
    "    # 遍历分块内的每个 token\n",
    "    for token_idx, (token_id, offset, token_type) in enumerate(zip(input_ids, offset_mapping, token_type_ids)):\n",
    "        # 根据 token_type 选择来源文本\n",
    "        if token_type == 0:\n",
    "            source_text = example[\"question\"]\n",
    "        else:\n",
    "            source_text = example[\"context\"]\n",
    "\n",
    "        # 关键修复点：分解 offset 元组为 start 和 end\n",
    "        start = offset[0]  # 起始字符位置\n",
    "        end = offset[1]    # 结束字符位置\n",
    "        print(start, end)\n",
    "        original_text = source_text[start:end]\n",
    "        \n",
    "        # 转换 token_id 为可读文本\n",
    "        token_str = tokenizer.convert_ids_to_tokens([token_id])[0]  # 取列表第一个元素\n",
    "        \n",
    "        # 打印结果\n",
    "        print(f\"Token {token_idx}: {token_str} → {original_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用最简单的比喻解释这段代码：\n",
    "\n",
    "**1. 分块（切书）**  \n",
    "- 就像一本厚书拆成几本小册子，每本最多512页（模型一次读不完长文本）\n",
    "\n",
    "**2. 文字变数字（加密）**  \n",
    "- 把每个字变成数字密码，比如 \"贝\"→100，\"爷\"→101  \n",
    "- `input_ids` 就是这些密码组成的列表：[100, 101, ...]\n",
    "\n",
    "**3. 记位置（书签）**  \n",
    "- `offset_mapping` 记录每个密码在原文的位置，比如 (0,2) 表示前两个字\n",
    "\n",
    "**4. 区分问题和答案（贴标签）**  \n",
    "- `token_type_ids=0` 表示文字来自问题（如 \"贝爷哪年结婚？\"）  \n",
    "- `token_type_ids=1` 表示文字来自答案（如 \"2000年...\"）\n",
    "\n",
    "**5. 找对应文字（解密）**  \n",
    "- 用密码本把数字转回文字  \n",
    "- 根据位置标签，从问题或答案文本截取对应文字\n",
    "\n",
    "**就像这样：**  \n",
    "密码 `100` → 查密码本 → 是\"贝\" → 在问题第0-2个位置 → 截取\"贝爷\"\n",
    "\n",
    "整个过程让计算机像人类一样：先看问题，再快速翻书找答案位置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"How many wins does the Notre Dame men's basketball team have?\""
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The men's basketball team has over 1,600 wins, one of only 12 schools who have reached that mark, and have appeared in 28 NCAA tournaments. Former player Austin Carr holds the record for most points scored in a single game of the tournament with 61. Although the team has never won the NCAA Tournament, they were named by the Helms Athletic Foundation as national champions twice. The team has orchestrated a number of upsets of number one ranked teams, the most notable of which was ending UCLA's record 88-game winning streak in 1974. The team has beaten an additional eight number-one teams, and those nine wins rank second, to UCLA's 10, all-time in wins against the top team. The team plays in newly renovated Purcell Pavilion (within the Edmund P. Joyce Center), which reopened for the beginning of the 2009–2010 season. The team is coached by Mike Brey, who, as of the 2014–15 season, his fifteenth at Notre Dame, has achieved a 332-165 record. In 2009 they were invited to the NIT, where they advanced to the semifinals but were beaten by Penn State who went on and beat Baylor in the championship. The 2010–11 team concluded its regular season ranked number seven in the country, with a record of 25–5, Brey's fifth straight 20-win season, and a second-place finish in the Big East. During the 2014-15 season, the team went 32-6 and won the ACC conference tournament, later advancing to the Elite 8, where the Fighting Irish lost on a missed buzzer-beater against then undefeated Kentucky. Led by NBA draft picks Jerian Grant and Pat Connaughton, the Fighting Irish beat the eventual national champion Duke Blue Devils twice during the season. The 32 wins were the most by the Fighting Irish team since 1908-09.\""
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example[\"context\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "借助`tokenized_example`的`sequence_ids`方法，我们可以方便的区分token的来源编号：\n",
    "\n",
    "- 对于特殊标记：返回None，\n",
    "- 对于正文Token：返回句子编号（从0开始编号）。\n",
    "\n",
    "综上，现在我们可以很方便的在一个输入特征中找到答案的起始和结束 Token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **`sequence_ids`**：\n",
    "\n",
    "---\n",
    "\n",
    "### **类比场景**\n",
    "想象你在玩一个**双色荧光笔标记**的游戏：\n",
    "- **黄色**：标记问题（比如：\"贝爷哪年结婚？\"）\n",
    "- **蓝色**：标记书中的答案段落（比如书里写：\"贝爷2008年结婚...\"）\n",
    "- **红色**：标记特殊符号（比如书的封面、章节分隔页）\n",
    "\n",
    "`sequence_ids` 就是一个**颜色编号列表**，告诉你每个字属于哪部分。\n",
    "\n",
    "---\n",
    "\n",
    "### **三种标记规则**\n",
    "1. **`None` → 红色标记**  \n",
    "   - 对应特殊符号：`[CLS]`（开头标志）、`[SEP]`（分隔符）\n",
    "   - 例：`[CLS]` → `None`\n",
    "\n",
    "2. **`0` → 黄色标记**  \n",
    "   - 所有来自**问题**的文字  \n",
    "   - 例：\"贝爷\"、\"哪年\" → `0`\n",
    "\n",
    "3. **`1` → 蓝色标记**  \n",
    "   - 所有来自**书本文档**的文字  \n",
    "   - 例：\"2008年\"、\"结婚\" → `1`\n",
    "\n",
    "---\n",
    "\n",
    "### **实际效果示例**\n",
    "假设问题和文档组合后：\n",
    "```\n",
    "[CLS] 贝爷哪年结婚？ [SEP] 贝爷2008年与Jay-Z结婚... [SEP]\n",
    "```\n",
    "\n",
    "对应的 `sequence_ids` 就像这样：\n",
    "```\n",
    "[ None, 0,0,0,0, None, 1,1,1,1,1, None ]\n",
    "```\n",
    "可视化标记：\n",
    "```\n",
    "红色 [CLS] → 黄黄黄黄 → 红色 [SEP] → 蓝蓝蓝蓝蓝 → 红色 [SEP]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **核心用途**\n",
    "1. **快速定位答案范围**  \n",
    "   ```python\n",
    "   # 找到文档部分的起止位置\n",
    "   start = sequence_ids.index(1)                 # 第一个蓝色标记的位置\n",
    "   end = len(sequence_ids) - sequence_ids[::-1].index(1) - 1  # 最后一个蓝色标记\n",
    "   ```\n",
    "\n",
    "2. **过滤无效内容**  \n",
    "   ```python\n",
    "   # 只处理文档部分的文字\n",
    "   if sequence_ids[i] == 1:\n",
    "       print(\"这是书里的内容！\")\n",
    "   ```\n",
    "\n",
    "3. **处理长文本分块**  \n",
    "   - 当文档太长时，自动分成多块，每块都有自己的 `sequence_ids`\n",
    "   - 例：分块1的蓝色标记对应文档前半部分，分块2对应后半部分\n",
    "\n",
    "---\n",
    "\n",
    "### **为什么需要它？**\n",
    "就像读书时用荧光笔划重点：\n",
    "- **黄色**：明确问题（知道要找什么）\n",
    "- **蓝色**：快速锁定答案区域（不用读完整本书）\n",
    "- **红色**：忽略无关的封面/分隔页\n",
    "\n",
    "这让模型像人类一样：先看问题，再快速翻书找答案位置，而不是傻傻通读全文。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成分块数: 2\n",
      "\n",
      "=== 分块 0 ===\n",
      "Token数量: 384\n",
      "sequence_ids结构: [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1]...\n",
      "问题部分覆盖的token位置: [1, 2, 3, 4, 5]...\n",
      "\n",
      "=== 分块 1 ===\n",
      "Token数量: 157\n",
      "sequence_ids结构: [None, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, None, 1, 1, 1, 1]...\n",
      "问题部分覆盖的token位置: [1, 2, 3, 4, 5]...\n"
     ]
    }
   ],
   "source": [
    "# 检查分块数量\n",
    "num_chunks = len(tokenized_example[\"input_ids\"])\n",
    "print(f\"生成分块数: {num_chunks}\")\n",
    "\n",
    "# 遍历每个分块\n",
    "for chunk_idx in range(num_chunks):\n",
    "    print(f\"\\n=== 分块 {chunk_idx} ===\")\n",
    "    \n",
    "    # 正确获取当前分块的数据\n",
    "    chunk_input_ids = tokenized_example[\"input_ids\"][chunk_idx]\n",
    "    chunk_sequence_ids = tokenized_example.sequence_ids(chunk_idx)  # 关键修复点\n",
    "    \n",
    "    # 打印关键信息\n",
    "    print(f\"Token数量: {len(chunk_input_ids)}\")\n",
    "    print(f\"sequence_ids结构: {chunk_sequence_ids[:20]}...\")  # 打印前20个元素\n",
    "    \n",
    "    # 检查问题部分是否完整\n",
    "    question_segment = [i for i, sid in enumerate(chunk_sequence_ids) if sid == 0]\n",
    "    print(f\"问题部分覆盖的token位置: {question_segment[:5]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['over 1,600'], 'answer_start': [30]}\n",
      "30\n",
      "40\n",
      "23 26\n"
     ]
    }
   ],
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "print(answers)\n",
    "print(start_char)\n",
    "print(end_char)\n",
    "# 当前span在文本中的起始标记索引。\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "    token_start_index += 1\n",
    "\n",
    "# 当前span在文本中的结束标记索引。\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# 检测答案是否超出span范围（如果超出范围，该特征将以CLS标记索引标记）。\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "    # 将token_start_index和token_end_index移动到答案的两端。\n",
    "    # 注意：如果答案是最后一个单词，我们可以移到最后一个标记之后（边界情况）。\n",
    "    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "        token_start_index += 1\n",
    "    start_position = token_start_index - 1\n",
    "    while offsets[token_end_index][1] >= end_char:\n",
    "        token_end_index -= 1\n",
    "    end_position = token_end_index + 1\n",
    "    print(start_position, end_position)\n",
    "else:\n",
    "    print(\"答案不在此特征中。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印检查是否准确找到了起始位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 1, 600\n",
      "over 1,600\n"
     ]
    }
   ],
   "source": [
    "# 通过查找 offset mapping 位置，解码 context 中的答案 \n",
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "# 直接打印 数据集中的标准答案（answer[\"text\"])\n",
    "print(answers[\"text\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 关于填充的策略\n",
    "\n",
    "- 对于没有超过最大长度的文本，填充补齐长度。\n",
    "- 对于需要左侧填充的模型，交换 question 和 context 顺序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_on_right = tokenizer.padding_side == \"right\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整合以上所有预处理步骤\n",
    "\n",
    "让我们将所有内容整合到一个函数中，并将其应用到训练集。\n",
    "\n",
    "针对不可回答的情况（上下文过长，答案在另一个特征中），我们为开始和结束位置都设置了cls索引。\n",
    "\n",
    "如果allow_impossible_answers标志为False，我们还可以简单地从训练集中丢弃这些示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_features(examples):\n",
    "    # 一些问题的左侧可能有很多空白字符，这对我们没有用，而且会导致上下文的截断失败\n",
    "    # （标记化的问题将占用大量空间）。因此，我们删除左侧的空白字符。\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和填充对我们的示例进行标记化，但保留溢出部分，使用步幅（stride）。\n",
    "    # 当上下文很长时，这会导致一个示例可能提供多个特征，其中每个特征的上下文都与前一个特征的上下文有一些重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例可能给我们提供多个特征（如果它具有很长的上下文），我们需要一个从特征到其对应示例的映射。这个键就提供了这个映射关系。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # 偏移映射将为我们提供从令牌到原始上下文中的字符位置的映射。这将帮助我们计算开始位置和结束位置。\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # 让我们为这些示例进行标记！\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # 我们将使用 CLS 特殊 token 的索引来标记不可能的答案。\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # 获取与该示例对应的序列（以了解上下文和问题是什么）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # 一个示例可以提供多个跨度，这是包含此文本跨度的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # 如果没有给出答案，则将cls_index设置为答案。\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # 答案在文本中的开始和结束字符索引。\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # 当前跨度在文本中的开始令牌索引。\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # 当前跨度在文本中的结束令牌索引。\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # 检测答案是否超出跨度（在这种情况下，该特征的标签将使用CLS索引）。\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # 否则，将token_start_index和token_end_index移到答案的两端。\n",
    "                # 注意：如果答案是最后一个单词（边缘情况），我们可以在最后一个偏移之后继续。\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **功能目标**\n",
    "这个函数就像一位 **数据加工厂的流水线工人**，负责把原始问答数据改造成适合模型理解的格式。主要解决两个问题：\n",
    "1. **长文本切割**：当答案文章太长时，切成多个短块（类似将长视频分段）\n",
    "2. **答案定位**：在每个短块中标注答案的位置（类似视频剪辑时标记精彩片段的起止时间）\n",
    "\n",
    "---\n",
    "\n",
    "### **核心处理步骤**\n",
    "\n",
    "#### **1. 清理问题文字（去左空格）**\n",
    "- **问题**：用户提问可能包含多余空格，例如 `\"   Beyonce哪年结婚？\"`\n",
    "- **处理**：去掉左边的空格 → `\"Beyonce哪年结婚？\"`\n",
    "- **原因**：防止空格占用分词名额，导致正文被过度截断\n",
    "\n",
    "#### **2. 文本分块处理**\n",
    "- **操作**：将长文章切成多个小块（每块最长 `max_length`，块间重叠 `stride`）\n",
    "- **示例**：\n",
    "  ```\n",
    "  原文章：段落1...段落2...段落3...（总长超过max_length）\n",
    "  分块1：段落1...段落2（前半）\n",
    "  分块2：段落2（后半）...段落3\n",
    "  ```\n",
    "\n",
    "#### **3. 记录分块关系**\n",
    "- **overflow_to_sample_mapping**：记录每个分块属于哪个原始样本  \n",
    "  （类似快递分箱时在每箱贴原订单号）\n",
    "- **offset_mapping**：记录每个分词对应的原始字符位置  \n",
    "  （类似每块积木对应原图纸的位置）\n",
    "\n",
    "#### **4. 处理无答案情况**\n",
    "- **场景**：当答案不在当前分块中（例如答案在另一个分块里）\n",
    "- **标记**：将答案位置设为 `[CLS]` 的位置（模型看到这个就知道当前块无答案）\n",
    "\n",
    "#### **5. 精确定位答案**\n",
    "- **步骤**：\n",
    "  1. **确定答案字符范围**：`start_char` 到 `end_char`\n",
    "  2. **找到分块的上下文部分**（跳过问题和特殊标记）\n",
    "  3. **检查答案是否在本分块**：\n",
    "     - 是 → 调整到精确的分词位置\n",
    "     - 否 → 标记为 `[CLS]`\n",
    "\n",
    "---\n",
    "\n",
    "### **实际案例演示**\n",
    "**输入数据**：\n",
    "```python\n",
    "{\n",
    "    \"question\": \"Beyonce哪年结婚？\",\n",
    "    \"context\": \"Beyonce于2008年与Jay-Z结婚...（长文本）\",\n",
    "    \"answers\": {\"text\": [\"2008年\"], \"answer_start\": }\n",
    "}\n",
    "```\n",
    "\n",
    "**处理过程**：\n",
    "1. **分块**：将长 `context` 分成两个块\n",
    "2. **块1处理**：\n",
    "   - 发现答案 `2008年` 在块1中\n",
    "   - 标注起始位置为 `token 6`，结束位置为 `token 7`\n",
    "3. **块2处理**：\n",
    "   - 块2不包含答案 → 标注为 `[CLS]`\n",
    "\n",
    "**输出特征**：\n",
    "```python\n",
    "{\n",
    "    \"input_ids\": [101, 2345, 3456, ..., 102],  # 分块后的token\n",
    "    \"start_positions\": 6, \n",
    "    \"end_positions\": 7\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **参数控制行为**\n",
    "| 参数 | 作用 | 类比解释 |\n",
    "|------|------|----------|\n",
    "| `max_length=384` | 每块最大长度 | 每段视频最长5分钟 |\n",
    "| `stride=128` | 分块间重叠长度 | 两段视频间重叠30秒防止漏内容 |\n",
    "| `pad_on_right=True` | 问题在右/左填充 | 字幕在视频左下方还是右下方 |\n",
    "\n",
    "---\n",
    "\n",
    "### **总结**\n",
    "这个函数就像一位智能剪辑师：\n",
    "1. **切分长视频**（分块处理）\n",
    "2. **标记关键片段**（答案定位）\n",
    "3. **处理特殊情况**（无答案时打标记）\n",
    "\n",
    "最终输出模型可以直接学习的标准化数据格式，是训练高质量问答模型的关键预处理步骤！ 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zS-6iXTkIrJT"
   },
   "source": [
    "#### datasets.map 的进阶使用\n",
    "\n",
    "使用 `datasets.map` 方法将 `prepare_train_features` 应用于所有训练、验证和测试数据：\n",
    "\n",
    "- batched: 批量处理数据。\n",
    "- remove_columns: 因为预处理更改了样本的数量，所以在应用它时需要删除旧列。\n",
    "- load_from_cache_file：是否使用datasets库的自动缓存\n",
    "\n",
    "datasets 库针对大规模数据，实现了高效缓存机制，能够自动检测传递给 map 的函数是否已更改（因此需要不使用缓存数据）。如果在调用 map 时设置 `load_from_cache_file=False`，可以强制重新应用预处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **核心流程类比**\n",
    "想象你经营一个 **大型快递分拣中心**，需要处理三种包裹（训练集、验证集、测试集）。`datasets.map` 就是你的 **自动化分拣流水线**，`prepare_train_features` 是你定制的 **智能分拣规则**。\n",
    "\n",
    "---\n",
    "\n",
    "### **分拣线参数解析**\n",
    "```python\n",
    "tokenized_datasets = datasets.map(\n",
    "    prepare_train_features,  # 你的智能分拣规则\n",
    "    batched=True,            # 整箱处理（而不是单件）\n",
    "    remove_columns=原始包裹标签  # 撕掉旧标签\n",
    ")\n",
    "```\n",
    "\n",
    "#### 1. **`batched=True` → 整箱处理模式**\n",
    "- **传统方式**：工人逐个检查包裹（单条数据处理）\n",
    "- **高效模式**：整箱倒进机器，同时处理数百个包裹（批量处理）\n",
    "- **优势**：速度提升 10-100 倍，特别适合 GPU 并行计算\n",
    "\n",
    "#### 2. **`remove_columns` → 清除旧标签**\n",
    "- **原因**：经过分拣后，包裹形状改变（数据列变化）\n",
    "- **操作**：\n",
    "  - 原始标签：发件人、收件人（`question`, `context` 等）\n",
    "  - 新标签：目的地代码、重量分级（`input_ids`, `attention_mask` 等）\n",
    "- **示例**：就像快递重新包装后，需要去掉旧面单\n",
    "\n",
    "#### 3. **缓存机制 → 智能暂存区**\n",
    "- **自动检测**：如果分拣规则没变，直接使用暂存区处理好的包裹\n",
    "- **强制刷新**：`load_from_cache_file=False` 就像要求「不管有没有旧包裹，全部重新分拣」\n",
    "- **优势**：节省 70% 以上时间，避免重复劳动\n",
    "\n",
    "---\n",
    "\n",
    "### **完整工作流程**\n",
    "1. **收包裹**：三种类型包裹进入流水线（训练/验证/测试集）\n",
    "2. **规则应用**：\n",
    "   - 智能切割大包裹（长文本分块）\n",
    "   - 贴上精准目的地标签（答案位置标记）\n",
    "   - 丢弃破损包裹（无效样本）\n",
    "3. **输出结果**：\n",
    "   - 标准化快递箱（模型可读的 `input_ids` 等）\n",
    "   - 精准物流标签（`start_positions`, `end_positions`）\n",
    "\n",
    "---\n",
    "\n",
    "### **技术细节对应**\n",
    "| 快递场景 | 数据处理 |\n",
    "|---------|----------|\n",
    "| 包裹类型区分 | 保持训练/验证/测试集结构 |\n",
    "| 分拣机器人 | `prepare_train_features` 函数 |\n",
    "| 整箱处理 | 批量矩阵运算 |\n",
    "| 暂存区 | Hugging Face 的缓存文件（通常存于 ~/.cache/huggingface/datasets）|\n",
    "\n",
    "---\n",
    "\n",
    "### **为什么需要这样设计？**\n",
    "1. **效率优先**：如同快递行业追求每日百万件处理量，深度学习的核心就是 **大规模数据吞吐**\n",
    "2. **资源管理**：缓存机制像双十一的预售包装，提前完成部分工作减轻高峰压力\n",
    "3. **质量管控**：`remove_columns` 确保不会把生鲜和普通包裹混淆（防止数据污染）\n",
    "\n",
    "通过这套系统，你的模型就像高效的物流网络，能快速准确地将「问题包裹」送达「答案目的地」！🚚✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "DDtsaJeVIrJT",
    "outputId": "aa4734bf-4ef5-4437-9948-2c16363da719"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "058a7843bd524904b48dacd181f13666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11c35c3f2cee456dad6bdb82f1359e45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(prepare_train_features,\n",
    "                                  batched=True,\n",
    "                                  remove_columns=datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "545PP3o8IrJV"
   },
   "source": [
    "## 微调模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBiW8UpKIrJW"
   },
   "source": [
    "现在我们的数据已经准备好用于训练，我们可以下载预训练模型并进行微调。\n",
    "\n",
    "由于我们的任务是问答，我们使用 `AutoModelForQuestionAnswering` 类。(对比 Yelp 评论打分使用的是 `AutoModelForSequenceClassification` 类）\n",
    "\n",
    "警告通知我们正在丢弃一些权重（`vocab_transform` 和 `vocab_layer_norm` 层），并随机初始化其他一些权重（`pre_classifier` 和 `classifier` 层）。在微调模型情况下是绝对正常的，因为我们正在删除用于预训练模型的掩码语言建模任务的头部，并用一个新的头部替换它，对于这个新头部，我们没有预训练的权重，所以库会警告我们在用它进行推理之前应该对这个模型进行微调，而这正是我们要做的事情。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 一、任务与模型类的对应关系\n",
    "Hugging Face Transformers 库为不同任务提供了专用类，就像选择不同的工具：\n",
    "\n",
    "| 任务类型                  | 对应模型类                          | 示例场景                     |\n",
    "|--------------------------|-----------------------------------|----------------------------|\n",
    "| 文本分类                  | `AutoModelForSequenceClassification` | 情感分析、评分预测          |\n",
    "| 问答任务                  | `AutoModelForQuestionAnswering`     | SQuAD 问答、阅读理解        |\n",
    "| 文本生成                  | `AutoModelForCausalLM`              | 故事续写、对话生成          |\n",
    "| 掩码语言建模              | `AutoModelForMaskedLM`              | BERT 式填空任务             |\n",
    "| 序列到序列                | `AutoModelForSeq2SeqLM`             | 翻译、摘要生成              |\n",
    "| 标记分类                  | `AutoModelForTokenClassification`   | 命名实体识别、词性标注      |\n",
    "| 多选任务                  | `AutoModelForMultipleChoice`        | 多选题回答                  |\n",
    "\n",
    "---\n",
    "\n",
    "### 二、Transformers 库的主要模型类\n",
    "以下是常用的模型类（以 **BERT** 架构为例，其他模型类似）：\n",
    "\n",
    "#### 1. 基础模型\n",
    "```python\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")  # 通用特征提取\n",
    "```\n",
    "\n",
    "#### 2. 任务专用模型\n",
    "```python\n",
    "# 文本分类（如情感分析）\n",
    "AutoModelForSequenceClassification.from_pretrained(...)\n",
    "\n",
    "# 问答任务（如SQuAD）\n",
    "AutoModelForQuestionAnswering.from_pretrained(...)\n",
    "\n",
    "# 文本生成（如GPT风格）\n",
    "AutoModelForCausalLM.from_pretrained(...)\n",
    "\n",
    "# 序列到序列（如BART/T5）\n",
    "AutoModelForSeq2SeqLM.from_pretrained(...)\n",
    "\n",
    "# 标记级分类（如NER）\n",
    "AutoModelForTokenClassification.from_pretrained(...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 三、处理自定义任务的三种方案\n",
    "如果你的任务没有现成类，可以通过以下方法解决：\n",
    "\n",
    "#### 方案 1：改造现有模型（推荐）\n",
    "```python\n",
    "from transformers import AutoModel\n",
    "\n",
    "# 加载基础模型\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 添加自定义头部\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bert = model\n",
    "        self.custom_head = nn.Linear(768, 3)  # 假设你的任务需要3类输出\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        outputs = self.bert(**inputs)\n",
    "        pooled = outputs.last_hidden_state[:,0]  # 取CLS标记\n",
    "        return self.custom_head(pooled)\n",
    "```\n",
    "\n",
    "#### 方案 2：继承并扩展\n",
    "```python\n",
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "class MyCustomModel(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config)\n",
    "        self.my_layer = nn.Linear(config.hidden_size, 5)  # 自定义输出维度\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        return self.my_layer(sequence_output[:,0])  # 使用CLS标记\n",
    "```\n",
    "\n",
    "#### 方案 3：使用 `AutoModelWithHeads`\n",
    "（需安装 `adapters` 库）\n",
    "```python\n",
    "from transformers.adapters import AutoAdapterModel\n",
    "\n",
    "model = AutoAdapterModel.from_pretrained(\"bert-base-uncased\")\n",
    "model.add_classification_head(\"my_task\", num_labels=3)  # 添加分类头\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 四、关于警告信息的解释\n",
    "当你看到类似这样的警告：\n",
    "```\n",
    "Some weights were not used... (vocab_transform, vocab_layer_norm)\n",
    "You should probably TRAIN this model...\n",
    "```\n",
    "这是 **正常现象**！因为：\n",
    "1. 预训练模型的原始头部（如MLM头部）被移除\n",
    "2. 新的任务头部（如分类器）需要重新训练\n",
    "3. 库在提醒你需要微调后才能用于推理\n",
    "\n",
    "---\n",
    "\n",
    "### 五、学习资源推荐\n",
    "1. [官方任务指南](https://huggingface.co/docs/transformers/task_summary)\n",
    "2. [自定义模型教程](https://huggingface.co/docs/transformers/custom_models)\n",
    "3. [社区论坛](https://discuss.huggingface.co/)（遇到问题时优先搜索）\n",
    "\n",
    "通过灵活组合这些方法，你可以应对任何自定义任务需求！🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "id": "TlqNaB8jIrJW",
    "outputId": "84916cf3-6e6c-47f3-d081-032ec30a4132"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 版本: 2.19.0\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import version\n",
    "print(\"TensorBoard 版本:\", version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8001 (pid 1319472), started 17:47:30 ago. (Use '!kill 1319472' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-6c031199972a8469\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-6c031199972a8469\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8001;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "# 指定日志目录和端口（注意这里的端口要与检测的8001一致）\n",
    "log_dir = \"your_logs_directory\"  # 替换为实际的日志目录路径\n",
    "%tensorboard --logdir $log_dir --port 8001 --bind_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "# 创建TCP套接字\n",
    "sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "# 尝试连接本地8001端口（非阻塞方式）\n",
    "result = sock.connect_ex(('localhost', 8001))\n",
    "# 断言验证（0表示端口开放）\n",
    "assert result == 0, \"TensorBoard 端口 8001 未开启！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 有问题，暂时不用，先训练\n",
    "\n",
    "# import evaluate\n",
    "\n",
    "# # 加载F1指标（支持分类任务的micro/macro/weighted）\n",
    "# squad_metric = evaluate.load(\"/root/projects/LLM-learning/evaluate/squad_v2.py\" if squad_v2 else \"/root/projects/LLM-learning/evaluate/squad.py\")\n",
    "\n",
    "# # 定义计算函数（处理模型输出）\n",
    "# def compute_metrics(eval_pred):\n",
    "#     predictions, labels = eval_pred\n",
    "#     predictions = np.argmax(predictions, axis=1)  # 分类任务取最大概率类别\n",
    "#     return squad_metric.compute(\n",
    "#         predictions=predictions, \n",
    "#         references=labels,\n",
    "#         average=\"macro\"  # \"micro\"（全局统计）、\"macro\"（类别平均）、\"weighted\"（加权平均）\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_N8urzhyIrJY"
   },
   "source": [
    "#### 训练超参数（TrainingArguments）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "Bliy8zgjIrJY"
   },
   "outputs": [],
   "source": [
    "batch_size=64\n",
    "model_dir = f\"models/{model_checkpoint}-finetuned-squad\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=model_dir,  # 模型/日志保存路径\n",
    "    evaluation_strategy = \"epoch\",  # 每个epoch后评估（可选\"steps\"按步评估）\n",
    "    learning_rate=2e-5,  # 经典微调学习率（预训练模型的典型学习率范围：1e-5~5e-5）\n",
    "    per_device_train_batch_size=batch_size,  # 每个GPU的训练批次（总batch_size = 该值 * GPU数量）\n",
    "    per_device_eval_batch_size=batch_size,   # 每个GPU的评估批次（可大于训练batch_size）\n",
    "    num_train_epochs=3,  # 训练轮次（SQuAD等中型数据集常用2-5轮）\n",
    "    weight_decay=0.01,  # L2正则化强度（防止过拟合，常用0.01-0.1）\n",
    "    fp16=True,  # 启用FP16混合精度\n",
    "    # save_strategy=\"epoch\",       # 每个epoch保存检查点\n",
    "    # load_best_model_at_end=True, # 训练结束加载最佳模型\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Collator（数据整理器）\n",
    "\n",
    "数据整理器将训练数据整理为批次数据，用于模型训练时的批次处理。本教程使用默认的 `default_data_collator`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXuFTAzDIrJe"
   },
   "source": [
    "### 实例化训练器（Trainer）\n",
    "\n",
    "为了减少训练时间（需要大量算力支持），我们不在本教程的训练模型过程中计算模型评估指标。\n",
    "\n",
    "而是训练完成后，再独立进行模型评估。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "imY1oC3SIrJf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU 使用情况\n",
    "\n",
    "训练数据与模型配置：\n",
    "\n",
    "- SQUAD v1.1\n",
    "- model_checkpoint = \"distilbert-base-uncased\"\n",
    "- batch_size = 64\n",
    "\n",
    "NVIDIA GPU 使用情况：\n",
    "\n",
    "```shell\n",
    "Every 5.0s: nvidia-smi                                                                                                                                 deepseek-r1-t4-test: Wed Mar 12 17:52:30 2025\n",
    "\n",
    "Wed Mar 12 17:52:30 2025\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  Tesla T4                       On  | 00000000:00:07.0 Off |                    0 |\n",
    "| N/A   53C    P0              63W /  70W |  10945MiB / 15360MiB |    100%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "\n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |\n",
    "|=======================================================================================|\n",
    "|    0   N/A  N/A   1271234      C   /root/miniconda3/envs/peft/bin/python     10942MiB |\n",
    "+---------------------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "id": "uNx5pyRlIrJh",
    "outputId": "077e661e-d36c-469b-89b8-7ff7f73541ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4152/4152 49:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.498200</td>\n",
       "      <td>1.273643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.121800</td>\n",
       "      <td>1.185791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.985200</td>\n",
       "      <td>1.167942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4152, training_loss=1.3124258081807336, metrics={'train_runtime': 2959.0362, 'train_samples_per_second': 89.749, 'train_steps_per_second': 1.403, 'total_flos': 2.602335381127373e+16, 'train_loss': 1.3124258081807336, 'epoch': 3.0})"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练完成后，第一时间保存模型权重文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save = trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**评估模型输出需要一些额外的处理：将模型的预测映射回上下文的部分。**\n",
    "\n",
    "模型直接输出的是预测答案的`起始位置`和`结束位置`的**logits**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型的输出是一个类似字典的对象，其中包含损失（因为我们提供了标签），以及起始和结束logits。我们不需要损失来进行预测，让我们看一下logits："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  43, 118, 108,  72,  35, 108,  34,  73,  41,  80,  91,\n",
       "         156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  41,  35,  42,  77,\n",
       "          11,  44,  27, 133,  66,  40,  87,  44,  43,  83, 127,  26,  28,  33,\n",
       "          87, 127,  95,  25,  43, 132,  42,  29,  44,  46,  24,  44,  65,  58,\n",
       "          81,  14,  59,  72,  25,  36,  55,  43], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  44, 118, 109,  75,  37, 109,  36,  76,  42,  83,  94,\n",
       "         158,  35,  83,  94,  83,  60,  80,  31,  43,  54,  42,  35,  43,  80,\n",
       "          13,  45,  28, 133,  66,  41,  89,  45,  44,  85, 127,  27,  30,  34,\n",
       "          89, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,\n",
       "          81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 如何从模型输出的位置 logit 组合成答案\n",
    "\n",
    "我们有每个特征和每个标记的logit。在每个特征中为每个标记预测答案最明显的方法是，将起始logits的最大索引作为起始位置，将结束logits的最大索引作为结束位置。\n",
    "\n",
    "在许多情况下这种方式效果很好，但是如果此预测给出了不可能的结果该怎么办？比如：起始位置可能大于结束位置，或者指向问题中的文本片段而不是答案。在这种情况下，我们可能希望查看第二好的预测，看它是否给出了一个可能的答案，并选择它。\n",
    "\n",
    "选择第二好的答案并不像选择最佳答案那么容易：\n",
    "- 它是起始logits中第二佳索引与结束logits中最佳索引吗？\n",
    "- 还是起始logits中最佳索引与结束logits中第二佳索引？\n",
    "- 如果第二好的答案也不可能，那么对于第三好的答案，情况会更加棘手。\n",
    "\n",
    "为了对答案进行分类，\n",
    "1. 将使用通过添加起始和结束logits获得的分数\n",
    "1. 设计一个名为`n_best_size`的超参数，限制不对所有可能的答案进行排序。\n",
    "1. 我们将选择起始和结束logits中的最佳索引，并收集这些预测的所有答案。\n",
    "1. 在检查每一个是否有效后，我们将按照其分数对它们进行排序，并保留最佳的答案。\n",
    "\n",
    "以下是我们如何在批次中的第一个特征上执行此操作的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# 获取最佳的起始和结束位置的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# 遍历起始位置和结束位置的索引组合\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # 需要进一步测试以检查答案是否在上下文中\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # 我们需要找到一种方法来获取与上下文中答案对应的原始子字符串\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "然后，我们可以根据它们的得分对`valid_answers`进行排序，并仅保留最佳答案。唯一剩下的问题是如何检查给定的跨度是否在上下文中（而不是问题中），以及如何获取其中的文本。为此，我们需要向我们的验证特征添加两个内容：\n",
    "\n",
    "- 生成该特征的示例的ID（因为每个示例可以生成多个特征，如前所示）；\n",
    "- 偏移映射，它将为我们提供从标记索引到上下文中字符位置的映射。\n",
    "\n",
    "这就是为什么我们将使用以下函数稍微不同于`prepare_train_features`来重新处理验证集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_validation_features(examples):\n",
    "    # 一些问题的左侧有很多空白，这些空白并不有用且会导致上下文截断失败（分词后的问题会占用很多空间）。\n",
    "    # 因此我们移除这些左侧空白\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "\n",
    "    # 使用截断和可能的填充对我们的示例进行分词，但使用步长保留溢出的令牌。这导致一个长上下文的示例可能产生\n",
    "    # 几个特征，每个特征的上下文都会稍微与前一个特征的上下文重叠。\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # 由于一个示例在上下文很长时可能会产生几个特征，我们需要一个从特征映射到其对应示例的映射。这个键就是为了这个目的。\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # 我们保留产生这个特征的示例ID，并且会存储偏移映射。\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # 获取与该示例对应的序列（以了解哪些是上下文，哪些是问题）。\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # 一个示例可以产生几个文本段，这里是包含该文本段的示例的索引。\n",
    "        sample_index = sample_mapping[i]\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # 将不属于上下文的偏移映射设置为None，以便容易确定一个令牌位置是否属于上下文。\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将`prepare_validation_features`应用到整个验证集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d136b1633f240deb31acbb9d95b9b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can grab the predictions for all features by using the `Trainer.predict` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Trainer`会隐藏模型不使用的列（在这里是`example_id`和`offset_mapping`，我们需要它们进行后处理），所以我们需要将它们重新设置回来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们可以改进之前的测试：\n",
    "\n",
    "由于在偏移映射中，当它对应于问题的一部分时，我们将其设置为None，因此可以轻松检查答案是否完全在上下文中。我们还可以从考虑中排除非常长的答案（可以调整的超参数）。\n",
    "\n",
    "展开说下具体实现：\n",
    "- 首先从模型输出中获取起始和结束的逻辑值（logits），这些值表明答案在文本中可能开始和结束的位置。\n",
    "- 然后，它使用偏移映射（offset_mapping）来找到这些逻辑值在原始文本中的具体位置。\n",
    "- 接下来，代码遍历可能的开始和结束索引组合，排除那些不在上下文范围内或长度不合适的答案。\n",
    "- 对于有效的答案，它计算出一个分数（基于开始和结束逻辑值的和），并将答案及其分数存储起来。\n",
    "- 最后，它根据分数对答案进行排序，并返回得分最高的几个答案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 15.2265625, 'text': 'Denver Broncos'},\n",
       " {'score': 13.082031,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 11.640625, 'text': 'Carolina Panthers'},\n",
       " {'score': 11.4296875, 'text': 'Broncos'},\n",
       " {'score': 11.277344,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 10.154297,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 10.123047, 'text': 'Denver'},\n",
       " {'score': 9.285156,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 9.1328125,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 8.009766,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 8.008057,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 7.694336,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 7.317871,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 7.251465,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.'},\n",
       " {'score': 7.1833496,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 6.987793, 'text': 'AFC) champion Denver Broncos'},\n",
       " {'score': 6.864746, 'text': 'champion Denver Broncos'},\n",
       " {'score': 6.614258,\n",
       "  'text': 'National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 6.426758, 'text': 'Panthers'},\n",
       " {'score': 6.2529297, 'text': 'Carolina'}]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# 第一个特征来自第一个示例。对于更一般的情况，我们需要将example_id匹配到一个示例索引\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# 收集最佳开始/结束逻辑的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # 我们需要细化这个测试，以检查答案是否在上下文中\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印比较模型输出和标准答案（Ground-truth）是否一致:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**模型最高概率的输出与标准答案一致**\n",
    "\n",
    "正如上面的代码所示，这在第一个特征上很容易，因为我们知道它来自第一个示例。\n",
    "\n",
    "对于其他特征，我们需要建立一个示例与其对应特征的映射关系。\n",
    "\n",
    "此外，由于一个示例可以生成多个特征，我们需要将由给定示例生成的所有特征中的所有答案汇集在一起，然后选择最佳答案。\n",
    "\n",
    "下面的代码构建了一个示例索引到其对应特征索引的映射关系："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当`squad_v2 = True`时，有一定概率出现不可能的答案（impossible answer)。\n",
    "\n",
    "上面的代码仅保留在上下文中的答案，我们还需要获取不可能答案的分数（其起始和结束索引对应于CLS标记的索引）。\n",
    "\n",
    "当一个示例生成多个特征时，我们必须在所有特征中的不可能答案都预测出现不可能答案时（因为一个特征可能之所以能够预测出不可能答案，是因为答案不在它可以访问的上下文部分），这就是为什么一个示例中不可能答案的分数是该示例生成的每个特征中的不可能答案的分数的最小值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size = 20, max_answer_length = 30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # 构建一个从示例到其对应特征的映射。\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # 我们需要填充的字典。\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    # 日志记录。\n",
    "    print(f\"正在后处理 {len(examples)} 个示例的预测，这些预测分散在 {len(features)} 个特征中。\")\n",
    "\n",
    "    # 遍历所有示例！\n",
    "    for example_index, example in enumerate(tqdm(examples)):\n",
    "        # 这些是与当前示例关联的特征的索引。\n",
    "        feature_indices = features_per_example[example_index]\n",
    "\n",
    "        min_null_score = None # 仅在squad_v2为True时使用。\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        # 遍历与当前示例关联的所有特征。\n",
    "        for feature_index in feature_indices:\n",
    "            # 我们获取模型对这个特征的预测。\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # 这将允许我们将logits中的某些位置映射到原始上下文中的文本跨度。\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # 更新最小空预测。\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # 浏览所有的最佳开始和结束logits，为 `n_best_size` 个最佳选择。\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            # 在极少数情况下我们没有一个非空预测，我们创建一个假预测以避免失败。\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # 选择我们的最终答案：最佳答案或空答案（仅适用于squad_v2）\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "            predictions[example[\"id\"]] = answer\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在原始结果上应用后处理问答结果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 10570 个示例的预测，这些预测分散在 10784 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca311ea914044afaa9a7b2b3ebf30363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `datasets.load_metric` 中加载 `SQuAD v2` 的评估指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e80d506dcc4b7d8960aacd2775c45c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fe64f5e7a9471d989d18404be582a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.11k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们可以调用上面定义的函数进行评估。\n",
    "\n",
    "只需稍微调整一下预测和标签的格式，因为它期望的是一系列字典而不是一个大字典。\n",
    "\n",
    "在使用`squad_v2`数据集时，我们还需要设置`no_answer_probability`参数（我们在这里将其设置为0.0，因为如果我们选择了答案，我们已经将答案设置为空）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 74.33301797540209, 'f1': 83.26051790761488}"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载本地保存的模型，进行评估和再训练更高的 F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = AutoModelForQuestionAnswering.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trained_trainer = Trainer(\n",
    "    trained_model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4152' max='4152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4152/4152 49:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.846900</td>\n",
       "      <td>1.212233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.713500</td>\n",
       "      <td>1.232366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.686100</td>\n",
       "      <td>1.247295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-3500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n",
      "Checkpoint destination directory models/distilbert-base-uncased-finetuned-squad/checkpoint-4000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4152, training_loss=0.7533242697890324, metrics={'train_runtime': 2980.1413, 'train_samples_per_second': 89.114, 'train_steps_per_second': 1.393, 'total_flos': 2.602335381127373e+16, 'train_loss': 0.7533242697890324, 'epoch': 3.0})"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_save_2 = trained_trainer.save_model(f\"{model_dir}-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['loss', 'start_logits', 'end_logits'])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "for batch in trained_trainer.get_eval_dataloader():\n",
    "    break\n",
    "batch = {k: v.to(trained_trainer.args.device) for k, v in batch.items()}\n",
    "with torch.no_grad():\n",
    "    output = trained_trainer.model(**batch)\n",
    "output.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 384]), torch.Size([64, 384]))"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.shape, output.end_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 46,  57,  78,  54, 118, 107,  72,  35, 107,  34,  73,  52,  80,  91,\n",
       "         156,  35,  83,  91,  80,  58,  77,  31,  42,  53,  52,  35,  53,  77,\n",
       "          11,  44,  27, 133,  66,  40,  87,  44,  43,  41, 127,  26,  28,  33,\n",
       "          87, 127,  95,  25,  43, 132,  42,  29,  44,  46,  24,  44,  65,  58,\n",
       "          81,  14,  59,  72,  25,  36,  57,  43], device='cuda:0'),\n",
       " tensor([ 47,  58,  81,  44, 118, 110,  75,  37, 110,  36,  76,  42,  83,  94,\n",
       "         158,  35,  83,  94,  83,  60,  80,  31,  43,  54,  42,  35,  43,  91,\n",
       "          13,  45,  28, 133,  66,  41,  89,  45,  44,  42, 127,  27,  30,  34,\n",
       "          90, 127,  97,  26,  44, 132,  43,  30,  45,  47,  25,  45,  65,  59,\n",
       "          81,  14,  60,  72,  25,  36,  58,  43], device='cuda:0'))"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_best_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "\n",
    "# 获取最佳的起始和结束位置的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "valid_answers = []\n",
    "\n",
    "# 遍历起始位置和结束位置的索引组合\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        if start_index <= end_index:  # 需要进一步测试以检查答案是否在上下文中\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": \"\"  # 我们需要找到一种方法来获取与上下文中答案对应的原始子字符串\n",
    "                }\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_predictions = trained_trainer.predict(validation_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features.set_format(type=validation_features.format[\"type\"], columns=list(validation_features.features.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_answer_length = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 18.515625, 'text': 'Denver Broncos'},\n",
       " {'score': 15.769531,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 14.910156, 'text': 'Broncos'},\n",
       " {'score': 13.9296875, 'text': 'Carolina Panthers'},\n",
       " {'score': 13.019531,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 12.859375,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos'},\n",
       " {'score': 12.537109, 'text': 'Denver'},\n",
       " {'score': 12.1640625,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 10.9296875,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.'},\n",
       " {'score': 10.2734375,\n",
       "  'text': 'The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 10.113281,\n",
       "  'text': 'American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
       " {'score': 9.332031,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina'},\n",
       " {'score': 9.089844,\n",
       "  'text': 'Carolina Panthers 24–10 to earn their third Super Bowl title.'},\n",
       " {'score': 8.664551, 'text': 'AFC) champion Denver Broncos'},\n",
       " {'score': 8.66333,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference'},\n",
       " {'score': 8.26416,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC)'},\n",
       " {'score': 7.742676, 'text': 'Panthers'},\n",
       " {'score': 7.600586,\n",
       "  'text': 'Denver Broncos defeated the National Football Conference (NFC'},\n",
       " {'score': 7.4921875, 'text': 'Carolina'},\n",
       " {'score': 7.3242188,\n",
       "  'text': 'Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24–10 to earn their third Super Bowl title.'}]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_logits = output.start_logits[0].cpu().numpy()\n",
    "end_logits = output.end_logits[0].cpu().numpy()\n",
    "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
    "\n",
    "# 第一个特征来自第一个示例。对于更一般的情况，我们需要将example_id匹配到一个示例索引\n",
    "context = datasets[\"validation\"][0][\"context\"]\n",
    "\n",
    "# 收集最佳开始/结束逻辑的索引：\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "valid_answers = []\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # 不考虑超出范围的答案，原因是索引超出范围或对应于输入ID的部分不在上下文中。\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        # 不考虑长度小于0或大于max_answer_length的答案。\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "        if start_index <= end_index: # 我们需要细化这个测试，以检查答案是否在上下文中\n",
    "            start_char = offset_mapping[start_index][0]\n",
    "            end_char = offset_mapping[end_index][1]\n",
    "            valid_answers.append(\n",
    "                {\n",
    "                    \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                    \"text\": context[start_char: end_char]\n",
    "                }\n",
    "            )\n",
    "\n",
    "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
    "valid_answers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['Denver Broncos', 'Denver Broncos', 'Denver Broncos'],\n",
       " 'answer_start': [177, 177, 177]}"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"validation\"][0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "examples = datasets[\"validation\"]\n",
    "features = validation_features\n",
    "\n",
    "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "features_per_example = collections.defaultdict(list)\n",
    "for i, feature in enumerate(features):\n",
    "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在后处理 10570 个示例的预测，这些预测分散在 10784 个特征中。\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cafe784ba2c40ec9c95307728b6fb2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10570 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"squad_v2\" if squad_v2 else \"squad\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 74.91958372753075, 'f1': 83.83155050300782}"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if squad_v2:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()]\n",
    "else:\n",
    "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
    "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in datasets[\"validation\"]]\n",
    "metric.compute(predictions=formatted_predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Question Answering on SQUAD",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
