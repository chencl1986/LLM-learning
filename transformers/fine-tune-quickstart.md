# Hugging Face Transformers å¾®è°ƒè®­ç»ƒå…¥é—¨

æœ¬ç¤ºä¾‹å°†ä»‹ç»åŸºäº Transformers å®ç°æ¨¡å‹å¾®è°ƒè®­ç»ƒçš„ä¸»è¦æµç¨‹ï¼ŒåŒ…æ‹¬ï¼š
- æ•°æ®é›†ä¸‹è½½
- æ•°æ®é¢„å¤„ç†
- è®­ç»ƒè¶…å‚æ•°é…ç½®
- è®­ç»ƒè¯„ä¼°æŒ‡æ ‡è®¾ç½®
- è®­ç»ƒå™¨åŸºæœ¬ä»‹ç»
- å®æˆ˜è®­ç»ƒ
- æ¨¡å‹ä¿å­˜

## YelpReviewFull æ•°æ®é›†

**Hugging Face æ•°æ®é›†ï¼š[ YelpReviewFull ](https://huggingface.co/datasets/yelp_review_full)**

### æ•°æ®é›†æ‘˜è¦

Yelpè¯„è®ºæ•°æ®é›†åŒ…æ‹¬æ¥è‡ªYelpçš„è¯„è®ºã€‚å®ƒæ˜¯ä»Yelp Dataset Challenge 2015æ•°æ®ä¸­æå–çš„ã€‚

### æ”¯æŒçš„ä»»åŠ¡å’Œæ’è¡Œæ¦œ
æ–‡æœ¬åˆ†ç±»ã€æƒ…æ„Ÿåˆ†ç±»ï¼šè¯¥æ•°æ®é›†ä¸»è¦ç”¨äºæ–‡æœ¬åˆ†ç±»ï¼šç»™å®šæ–‡æœ¬ï¼Œé¢„æµ‹æƒ…æ„Ÿã€‚

### è¯­è¨€
è¿™äº›è¯„è®ºä¸»è¦ä»¥è‹±è¯­ç¼–å†™ã€‚

### æ•°æ®é›†ç»“æ„

#### æ•°æ®å®ä¾‹
ä¸€ä¸ªå…¸å‹çš„æ•°æ®ç‚¹åŒ…æ‹¬æ–‡æœ¬å’Œç›¸åº”çš„æ ‡ç­¾ã€‚

æ¥è‡ªYelpReviewFullæµ‹è¯•é›†çš„ç¤ºä¾‹å¦‚ä¸‹ï¼š

```json
{
    'label': 0,
    'text': 'I got \'new\' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\nI took the tire over to Flynn\'s and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he\'d give me a new tire \\"this time\\". \\nI will never go back to Flynn\'s b/c of the way this guy treated me and the simple fact that they gave me a used tire!'
}
```

#### æ•°æ®å­—æ®µ

- 'text': è¯„è®ºæ–‡æœ¬ä½¿ç”¨åŒå¼•å·ï¼ˆ"ï¼‰è½¬ä¹‰ï¼Œä»»ä½•å†…éƒ¨åŒå¼•å·éƒ½é€šè¿‡2ä¸ªåŒå¼•å·ï¼ˆ""ï¼‰è½¬ä¹‰ã€‚æ¢è¡Œç¬¦ä½¿ç”¨åæ–œæ åè·Ÿä¸€ä¸ª "n" å­—ç¬¦è½¬ä¹‰ï¼Œå³ "\n"ã€‚
- 'label': å¯¹åº”äºè¯„è®ºçš„åˆ†æ•°ï¼ˆä»‹äº1å’Œ5ä¹‹é—´ï¼‰ã€‚

#### æ•°æ®æ‹†åˆ†

Yelpè¯„è®ºå®Œæ•´æ˜Ÿçº§æ•°æ®é›†æ˜¯é€šè¿‡éšæœºé€‰å–æ¯ä¸ª1åˆ°5æ˜Ÿè¯„è®ºçš„130,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ10,000ä¸ªæµ‹è¯•æ ·æœ¬æ„å»ºçš„ã€‚æ€»å…±æœ‰650,000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ50,000ä¸ªæµ‹è¯•æ ·æœ¬ã€‚

## ä¸‹è½½æ•°æ®é›†


```python
from datasets import load_dataset

dataset = load_dataset("yelp_review_full")
```


```python
dataset
```




    DatasetDict({
        train: Dataset({
            features: ['label', 'text'],
            num_rows: 650000
        })
        test: Dataset({
            features: ['label', 'text'],
            num_rows: 50000
        })
    })




```python
dataset["train"][333]
```




    {'label': 3,
     'text': "All in favor of a deep dish pizza say I!.......IIIIIII,  ok now that i have that out of my system. This place is such a great hangout/eat-in spot. I hadn't been here and years and some friends invited us out for the evening. I was so glad they were paying cause  I was low on funds at the time.\\n\\nWe arrived on a friday night and of course it was busy there. We waited about 10 minutes to get a table which wasn't bad considering the crowd. We looked over the menu and they have so many great choices. Pizza, pasta, appetizers, seafood, burgers, salads and sandwiches. \\n\\nAfter ordering two mango lemonades that were wayyyyy over sweetened we ordered our food. We both are going gluten free which is tough but UNO's gave us a nice selection of dishes to choose from. Plus! They make a thin crust gluten free pizza which taste great. My hubby ordered the mediterrean thin crust because he loves kalamata olives and I ordered the Guac-alicious burger with a Caesar side salad. My salad came out pretty quick which was nice but it had a little too much dressing on it. I didn't complain, it still tasted great.\\n\\nI'm not into red meat so I tried to order a black bean burger or get chicken instead of beef, but the ran out of black bean and they couldn't get the chicken so i just ordered it anyway. The burger was piled really high with all the toppings including guacamole and it was very creamy but i couldn't get over the taste of the burger because it just didn't have any flavor. Very saddening. I ended up just eating the veggies and discarding the meat. I snacked on some of my hubbies pizza even though it was only a small amount. \\n\\nWe came here twice in one week. The second time we ordered the 9-grain deep dish with mushrooms, parmesan and a garlic white sauce. Was this pizza amazing or what?? I will probably always eat this pizza whenever I come. \\n\\nOnly down side is slow service. It took 20 minutes for our pizza to come out and my hubbies was a little over cooked. He got the numero uno which was ok but mine had way more flavor!!"}




```python
dataset["train"][666]
```




    {'label': 2,
     'text': 'Just ate there, right next to GameStop & Google, has 3 small booths, & ordered the pepper steak w/ onion ($10.95). Food is fast fresh & hot, but mine had too much onion & not enough steak. At the end of the meal I was just eating onions with rice, though I hear this is healthy for you. Counter lady was cordial, but didn\'t reply when customers told her, \\"Have a nice day\\" #awkward. I know that English isn\'t her first language but she needs to catch on that people are wishing her well. Wasn\'t stuffed full either despite having eaten a large plate (I usually get this feeling eating Asian). This is basically a nice place to go for lunch that won\'t ruin your appetite for dinner. (Side note: Food is very clean. Brushed my teeth an hour before w/ Tom\'s of Maine fluoride-free peppermint & still had minty fresh breath an hour after eating)'}




```python
import random
import pandas as pd
import datasets
from IPython.display import display, HTML
```


```python
def show_random_elements(dataset, num_examples=15):
    assert num_examples <= len(dataset), "Can't pick more elements than there are in the dataset."
    picks = []
    for _ in range(num_examples):
        pick = random.randint(0, len(dataset)-1)
        while pick in picks:
            pick = random.randint(0, len(dataset)-1)
        picks.append(pick)
    
    df = pd.DataFrame(dataset[picks])
    for column, typ in dataset.features.items():
        if isinstance(typ, datasets.ClassLabel):
            df[column] = df[column].transform(lambda i: typ.names[i])
    display(HTML(df.to_html()))
```


```python
show_random_elements(dataset["train"])
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1 star</td>
      <td>My first experience at \"Sugar Factory\" was the one at Paris Hotel! Remembered great service and good food!\n\nMy visit this time was because of a Groupon deal - $30 for $62 worth of dinner/drinks.\n\nWe made reservations but was running a little late. When I came up to the hostess and told her that, she pretty much just threw me at the worst possible seat outside. My mistake for assuming that I should've been asked if I wanted indoors or outdoors, or if I wanted to wait for indoor seats. \n\nTook about 10 mins for us to see our waiter, Justin. He took our water order, for we were waiting for our friend, and told us about their half-off special for the night on drinks.\nTook another 10-15 mins to ACTUALLY get our water. \n\nBecause we were waiting for our friend still, we decided to order a couple things - we got a $29 White Gummi Goblet, my $18 3-Piece Chicken Fingers and Onion Rings and my friend's $8 4-Piece, Cracker-Sized Bruschetta.\n\nOur goblet arrives.. As they're suppose to, they bring the cup filled with ice and they poor the drink in front of you. Justin pours the drink .. Then DEMANDS for my friend to sip it because it was going to over flow.. Like DEMANDED, as in told her \"hurry up, sip it because it's going to spill all over the table\" ... W T F?!\n\nOur food comes shortly after the uncomfortable incident, it was good and all but just not worth what we paid for..\n\nWe had ordered another goblet (Berry Bliss) and for what they're worth, they are definitely good drinks. Although they were sweet and fruity, they pack the alcohol punch that comes creepin' after a while of drinking it!\n\nSomewhere along that time period Justin had made really awkward, not so funny jokes.. It made us feel so uncomfortable and he did it in no flirty, nor professional manner!\n\nAs closing time approaches, Justin had asked for my phone for the Groupon voucher, he took it so our discount can be applied to the bill. He came back, dropped the check, and literally ran off .. \nSooooo, where's my phone?!?!? Seriously took a few minutes to see him.. When he finally noticed I was eyeing him down, I asked \"so where's my phone?!\".\nHe ran towards his manager, dropped off my phone and AGAIN awkwardly, unprofessionally joked about his manager looking through my pictures.. True or not, I shouldn't have had to ask for my phone! Nor did he have to make that stupid joke! \nI've seriously have had it with him that night and wonder how and why he works there.\n\nTo end the night, we saw on our bill that gratuity was included... For THREE people ?!? Because we had our Groupon deal??? I honestly didn't understand but I would've really liked for our \"funny\" waiter, Justin, to have explained things..\n\nFor the record, I don't think I'd be coming back ... If I do ..... Nvm yeah def not, thanks to Justin!</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3 stars</td>
      <td>Cool little taco shop in a questionable part of town.  we went early and the place was dead. The waitress was very nice and helpful even going in to the kitchen to get us samples of the meats they offer.  The salsa bar was huge with lots of interesting choices, nothing rocked my world but the cilantro cream was tasty.  we ordered a variety of items, my least favorite being the quesadilla, just tasted like fried greasy tortilla couldn't get past it to eat more than a bite or two.  the tacos were good I had the shark and hubby had the pork it was his favorite. we had a carne asada burrito as well and the meat was tasty.  It was good not great but I think we will be back to try it again.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3 stars</td>
      <td>Burgers.  Great!!  Those chips they serve not as much.  Our portion  unedible  greasy thru and thru. And mentioned to server  she sent manager over. Very nice guy brought us a fresh order and they were not much better.  The server also mixed up the placement of the burger on the table and we had no idea til half way thru when my mom commented the burger was spicy. \nHad the Monty. Great burger nice portion. My niece had the buffalo and the taste was great.  Would be a 4 but for $12 burger the whole meal should be good.</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1 star</td>
      <td>I have never written a bad review before - however, I cannot not write this.  I am still shocked at the treatment I received from Gary, supposedly a service oriented nail 'professional.'    He is rude (to say the least), arrogant (without reason), his Groupon is a lie (he doesn't even do Gel nails).  He does NOT even deserve to be in business.  Again, I have NEVER felt so strongly about such horrible customer service.</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3 stars</td>
      <td>Best pizza in Vegas! Vegas--you're in trouble if that's true. It's not bad pizza, I just think that the election is rigged. The owners are smart and humor runs high in their marketing ploys. Gimmicks like getting a discount on certain day if you're name is Mark are pretty brilliant. \n\nThe service is also pretty on point and upbeat. The place itself is designed to feel homey like a pizza joint should and succeeds. I am especially fond of the mammoth booths. Extraneous seating is always a winner in my book.  \n\nThe special on my day is that you got free Nuke fries with your order. The spicy Nuke fries are o.k., but I think I would be upset if I paid for them. Gesture appreciated though. Metro has four Eastside specialties on their menu--so my friend and I went with the Four Corners, which gives you two slices of each Eastsides. \n\nWith exciting toppings like eggplant and ricotta in the mix--I thought the Four Corners was going to be a knockout, but the uppercut never came. I got rabbit punched instead. The idea is good topping-wise on these pizzas, but the ingredients all feel very commercial--like straight out of a can or factory. \n\nAlso, the Large pizza is pretty large--so only order thick crust if you are starving or if you want to take some with you. The Four Corners hit the spot as far as my pizza craving, but surely there is better pizza in town. That being said, the pizza hounds have been let loose and the hunt begins!</td>
    </tr>
    <tr>
      <th>5</th>
      <td>4 stars</td>
      <td>Having moved from Utah, where we first found this place, I was very sad to think we'd never be back to it. We were thrilled when one opened up so close to us. And we were not disappointed with the quality. \n\nI usually get a combo with soup and sandwich, though I've recently switched to getting just a regular soup. I never finish the whole sandwich and it feels like a waste to leave half of it there. \nMy husband's favorite sandwich is gone now, replaced by a similar-but-not-the-same option.  Not bad though. \nMy favorite soup by far is the chicken enchilada chili, but I've liked every other one I've tried as well. My parents also love the place. \nThe chocolate covered strawberries are a wonderful touch. I've gotten two desserts so far and they were pretty good. Love the beverage options with the syrups - sprite with cherry and blackberry is my favorite. \nI do wish the bread given with the soups seemed a little more fresh though.</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1 star</td>
      <td>I recently moved to the area and have been looking for a nail place close by. Let me just preface this by saying I am the type of customer who never goes into a business requesting extended services close to closing time. That being said, I drove up at 3:55pm on a Sunday (their posted closing time is 5pm) I had planned a mani/pedi but with only an hour I thought I'd guage how many services to get once I walked in.\n\nThe place is pretty and well decorated. It smelled nice and I was promptly greeted as I walked in. The place was empty except for one customer getting a manicure and they were already painting her nails so it was safe to assume she was almost done with her manicure. \n\nThe lady who greeted me asked what I would like. In the interest of time I decided to only get a pedicure and told her I would like a pedicure. She looked at the clock, grimaced and said \"oh, sorry...you wouldnt have enough time for a pedicure before closing\"\n\nI was floored. An hour and 10 mins in an empty salon isn't enough time for a pedicure? ? ? Did they send their staff home early? Were they trying to close early? Not sure...\n\nBecause of the reviews I was expecting a great experience. Not the case! \n\nI went across the street (Albertson's shopping plaza) to Nice Nails and got a mani/pedi and some much needed relaxation :-)\n\nNot sure if I'll give nail room another try, didn't feel like they wanted my business....</td>
    </tr>
    <tr>
      <th>7</th>
      <td>2 star</td>
      <td>$3.95 for a soda?  I should have known right there they weren't interested in locals (yes, I'm familiar with the prices at other local restaurants this is, after all, my neighborhood).  My brisket sandwich had so much fat that I gave up and took the rest home for the dog.  Too bad, because I was looking forward to having a high quality BBQ restaurant, nearby.  I will watch to see if there are any substantial changes in their approach, before returning.</td>
    </tr>
    <tr>
      <th>8</th>
      <td>4 stars</td>
      <td>Love this park! it's a nice green space in the midle of downtown. every afternoon you will find runners jogging the path around the park, kids and parents playing on the playground, people playing volleyball on the sand court, and people walking their dogs. compared to where I used to live in Austin, dog owners here are VERY responsible. I rarely ever see someone who doesn't pick up after their pet. (note: the park does not have stations that provide doggy poop bags, you have to bring your own)\n\nonly reason this park doesn't get five stars is because there are quite a few homeless people who use the area for sleeping or hanging out. if you walk under the bridge toward the library you walk right into a HUGE crowd of homeless people hanging out outside the library. it can be a little intimidating.\n\nthere are two shaded ramadas in the park which you can reserve for parties or other events for a fee. the information for how to reserve one is posted on the ramadas themselves.</td>
    </tr>
    <tr>
      <th>9</th>
      <td>2 star</td>
      <td>My family arrived to celebrate my moms birthday! The hosting staff did a great job of seating us in a minimal amount of time. We had an emergency change in plans and they accommodated us quickly. They also accommodated the temperature of the room for our easily cooled friends. Everyone enjoyed their food. And the server was great UNTIL......two spiders crawled up onto our table right between me and my son. When I complained, I got some half assed excuse that they had just sprayed the night before from the manager. My husband, not wanting to look douchey and not like we were just trying to get a free meal, told them not to worry about it. But personally, I think they still should have done something about it. When you pay for a $500 meal, you don't want to deal with bugs. I was in the service industry for nearly 10 years, I understand the prejudice that goes on behind closed doors. I understand that they probably thought I was lying, but frankly, I couldn't care less. That's gross. I don't want to pay for a meal with bugs.</td>
    </tr>
    <tr>
      <th>10</th>
      <td>2 star</td>
      <td>I got a promotion in the mail for ridiculously low rates so I had to take them up on the offer. I kind of wish I hadn't. The hotel, or at least my room has seen better days. It was really worn down. Everything was just good enough nothing more nothing less. The towels were a little rough and the bed was a bit lumpy. \n\nThe casino is actually kind of small and limited, it seems this place lives off more from its nightlife. I don't think I would stay at the Palms again. There are so many other places in Vegas that you can get a whatever experience.</td>
    </tr>
    <tr>
      <th>11</th>
      <td>2 star</td>
      <td>I lived here from 2008-2010 and did genuinely enjoy my experience with the apartment itself.  The building is old construction which was nice because I didn't have an issue hearing my neighbors.  Also didn't have a big issue with the heat/cold as the place was well insulated.  The location is also great as you are close to Old Town and the 101.\n\nThe issues I did have were with the management company.  Rent is due by the 2nd and HAS to be paid online.  The office doesn't tell you that it has to be paid by 12 AM Central Standard time otherwise you will get a $50 late charge.  I incurred a late charge and asked the office to reverse it since I had paid rent on time but was told since it was paid at 12:05 AM on the 3rd CST, they would not reverse the charges.  \n\nAt first when I moved in there were around 4 washers/dryers for 40 units.  The laundry room was remodeled and the washer/dryers were reduced down to 2 units each.  The explanation was that the new models were \"higher efficiency\" and would be much faster but just because the wash cycle is faster doesn't mean your neighbor will be courteous and remove their laundry on time.  The worst part was I had someone steal my underwear on 3 separate occasions.  When I called the office to report it I was told \"maybe you should just stay with your laundry so that people don't steal your personal items.\"  It was only after the 3rd time that I filed a police report and found out that the SDPD had had SEVERAL complaints about this issue and the office chose to over look them.   \n\nI also had issues with people parking in my assigned space and even had a glove box full of \"nasty grams\" to put on the offenders windshield.  One time I called the # listed on the carport to have the car towed and was told I needed a special \"code\" to have them come out.  I called the office and asked for the code and was told they don't give that out to residents but I could call during office hours and they would be happy to help with my parking issue.  I explained it was happening at 11 PM and the office wasn't open but was told to just find an uncovered spot to park in.\n\nThe kicker was when I moved out.  I had lived in the apartment for 2 years and did my best on upkeep.  Once I moved out I got a bill saying I owed them $200 over other $200 refundable deposit I had put down when I moved in.  I asked for an itemized list of what I was charged for and found out I was charged $180 to repaint my 500 sq foot apartment which I found odd since there were no holes in the wall nor had I made any paint color changes.  Was also charged to replace the carpet which was funny because it wasn't new when I moved in, just shampooed.\n\nOh yeah, dumpster divers totally true.  Happened at least once a week</td>
    </tr>
    <tr>
      <th>12</th>
      <td>3 stars</td>
      <td>Quick review\nI expected the wait for seating, but not the long wait for the food (it's only burgers).\nAfter the wait, still got my temp wrong  on my burger (more waiting)\nTaste was good but portion were small for the high price on Maui onion rings, fries and burgers.\nService was real good. \nI believe KGB (Kerry Gourmet burger) and Burger bar are much better burger places.\nStill a fan of Chef Ramsey.</td>
    </tr>
    <tr>
      <th>13</th>
      <td>5 stars</td>
      <td>This is my favorite Asian restaurant in the Valley. This place is not much for ambiance, but it features wonderfully authentic Cantonese- Hong Kong cuisine at outrageously cheap prices, served by very friendly and helpful staff. The menu of Asian Cafe Express is extensive, but skip the first few pages (these list the \"Americanized Chinese\" dishes that most people are familiar with) and go directly to the pages with the items categorized as \"Hong Kong style\" or \"HK style.\" If you're not sure what to order, you can browse the huge pictures of popular dishes, on the wall. But it will be best to ask the waitpersons for suggestions -- they'll be more than happy to point out dishes in the menu that are favorites of the many regular customers there. Portions are more than ample, even for the soups: the smallest serving (\"medium\"  in the menu) of soup serves two generously.  And they have several varieties of congee, including my favorite: with pork and preserved egg -- comfort food like no other for this Asian ;-)\n\nFor those who use the light rail, this is a convenient dining destination because it is so close to the Mesa terminus of the light rail. Be prepared for a taste treat, and don't act too surprised when the bill arrives and you find that you don't pay much for such great food.</td>
    </tr>
    <tr>
      <th>14</th>
      <td>2 star</td>
      <td>Am I the ONLY person in the state who doesn't need to eat here?  Can someone tell me what I am missing?  I just don't get the hype.  The food is ok, nothing stands out as \"must come back for\".  I will keep coming, because my sister and mother are addicted, and I am sure there is something on the menu I can find that I will like.</td>
    </tr>
  </tbody>
</table>


## é¢„å¤„ç†æ•°æ®

ä¸‹è½½æ•°æ®é›†åˆ°æœ¬åœ°åï¼Œä½¿ç”¨ Tokenizer æ¥å¤„ç†æ–‡æœ¬ï¼Œå¯¹äºé•¿åº¦ä¸ç­‰çš„è¾“å…¥æ•°æ®ï¼Œå¯ä»¥ä½¿ç”¨å¡«å……ï¼ˆpaddingï¼‰å’Œæˆªæ–­ï¼ˆtruncationï¼‰ç­–ç•¥æ¥å¤„ç†ã€‚

Datasets çš„ `map` æ–¹æ³•ï¼Œæ”¯æŒä¸€æ¬¡æ€§åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ã€‚

ä¸‹é¢ä½¿ç”¨å¡«å……åˆ°æœ€å¤§é•¿åº¦çš„ç­–ç•¥ï¼Œå¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼š

1. **ä¸ºä»€ä¹ˆè¦å¤„ç†æ–‡æœ¬é•¿åº¦ï¼Ÿ**  
   å°±åƒè¡£æœæœ‰å°ºç ï¼Œç¥ç»ç½‘ç»œæ¨¡å‹ä¹Ÿæœ‰å›ºå®šçš„"è¾“å…¥å°ºå¯¸"ã€‚æ¯”å¦‚BERTæ¨¡å‹æœ€å¤š"åƒ"512ä¸ªå•è¯ç‰‡æ®µã€‚å¤ªé•¿çš„æ–‡æœ¬ä¼šè¢«æˆªæ–­ï¼ˆåˆ‡æ‰å°¾å·´ï¼‰ï¼Œå¤ªçŸ­çš„ä¼šè¡¥é›¶ï¼ˆç›¸å½“äºç»™è¡£æœåŠ å¡«å……ç‰©ï¼‰ã€‚

2. **åˆ†è¯å™¨åœ¨åšä»€ä¹ˆï¼Ÿ**  
   æŠŠæ–‡å­—è½¬æ¢æˆæ•°å­—å¯†ç ï¼ˆå¦‚"ä½ å¥½"â†’[101, 2345])ï¼ŒåŒæ—¶ï¼š
   â€¢ è‡ªåŠ¨åŠ ç‰¹æ®Šç¬¦å·ï¼šæ¯”å¦‚[CLS]å¼€å¤´ã€[SEP]åˆ†éš”
   â€¢ è®°å½•å“ªäº›æ˜¯çœŸå®å†…å®¹ï¼ˆattention_maské‡Œ1è¡¨ç¤ºçœŸå®ï¼Œ0æ˜¯å¡«å……çš„ï¼‰

3. **mapæ–¹æ³•çš„ç¥å¥‡ä¹‹å¤„**  
   è¿™ä¸ªæ“ä½œå°±åƒæµæ°´çº¿ä½œä¸šï¼ŒæŠŠæ•´ä¸ªæ•°æ®é›†æ‰¹é‡é€è¿›å¤„ç†å‡½æ•°ã€‚å‡è®¾æ•°æ®é›†æœ‰1ä¸‡æ¡æ–‡æœ¬ï¼Œç”¨`batched=True`å‚æ•°ï¼Œå¯èƒ½åˆ†100æ‰¹æ¬¡å¤„ç†ï¼ˆæ¯æ‰¹100æ¡ï¼‰ï¼Œæ•ˆç‡æ¯”é€æ¡å¤„ç†é«˜å¾—å¤šã€‚

4. **å¤„ç†åçš„æ•°æ®ç»“æ„**  
   æ¯ä¸ªæ ·æœ¬ä¼šå˜æˆåŒ…å«å¤šä¸ªæ•°ç»„çš„å­—å…¸ï¼š
   ```python
   {
     'input_ids': [101, 2345, 1032, 0, 0],  # æ•°å­—åŒ–çš„æ–‡æœ¬
     'token_type_ids': [0,0,..],            # åŒºåˆ†å¥å­ï¼ˆç”¨äºé—®ç­”ä»»åŠ¡ï¼‰
     'attention_mask': [1,1,..0,0]          # æ ‡è®°æœ‰æ•ˆå†…å®¹ä½ç½®
   }
   ```

ä¸¾ä¸ªç”Ÿæ´»åŒ–çš„ä¾‹å­ï¼š
åŸå§‹å¥å­ï¼š"æˆ‘çˆ±åƒæŠ«è¨" â†’ å¤„ç†åä¼šå˜æˆç±»ä¼¼ï¼š
```
[CLS] æˆ‘ çˆ± åƒ æŠ«è¨ [PAD] [PAD] [PAD]...
å¯¹åº”çš„æ•°å­—ï¼š[101, 2769, 3342, 1563, 5643, 0, 0, 0...]
æ³¨æ„åŠ›çš„é®ç½©ï¼š[1,1,1,1,1,0,0,0...]
```
å…¶ä¸­ï¼š
â€¢ [CLS]æ˜¯BERTè¦æ±‚çš„èµ·å§‹ç¬¦å·
â€¢ [PAD]æ˜¯å¡«å……çš„å ä½ç¬¦ï¼ˆå®é™…ç”¨0è¡¨ç¤ºï¼‰
â€¢ æ³¨æ„åŠ›é®ç½©å‘Šè¯‰æ¨¡å‹å“ªäº›ä½ç½®éœ€è¦å…³æ³¨


```python
# ä»transformersåº“å¯¼å…¥è‡ªåŠ¨åˆ†è¯å™¨
from transformers import AutoTokenizer

# åŠ è½½é¢„è®­ç»ƒçš„åˆ†è¯å™¨ï¼ˆè¿™é‡Œç”¨çš„æ˜¯BERTçš„åŒºåˆ†å¤§å°å†™ç‰ˆæœ¬ï¼‰
# [2,4](@ref)ï¼šHugging Faceçš„Tokenizeræ”¯æŒå¡«å……å’Œæˆªæ–­ç­–ç•¥
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

# å®šä¹‰ä¸€ä¸ªå¤„ç†æ•°æ®é›†çš„å‡½æ•°
def tokenize_function(examples):
    # å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯ï¼Œå¹¶åº”ç”¨ä¸¤ä¸ªé‡è¦ç­–ç•¥ï¼š
    # 1. padding="max_length"ï¼šå°†æ‰€æœ‰æ–‡æœ¬å¡«å……åˆ°æ¨¡å‹å…è®¸çš„æœ€å¤§é•¿åº¦ï¼ˆå¦‚512ï¼‰
    # 2. truncation=Trueï¼šè¶…è¿‡æœ€å¤§é•¿åº¦çš„éƒ¨åˆ†ä¼šè¢«æˆªæ–­
    # [2,4](@ref)ï¼šè¿™æ˜¯Hugging Faceæ¨èçš„æ ‡å‡†åŒ–å¤„ç†æ–¹å¼
    return tokenizer(examples["text"], padding="max_length", truncation=True)

# å°†å¤„ç†å‡½æ•°åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†ï¼ˆæ”¯æŒæ‰¹é‡å¤„ç†åŠ é€Ÿï¼‰
# batched=Trueè¡¨ç¤ºä¸€æ¬¡æ€§å¤„ç†å¤šä¸ªæ ·æœ¬ï¼Œæ¯”é€æ¡å¤„ç†å¿«10å€ä»¥ä¸Š
tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

    /root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
      warnings.warn(



```python
print(tokenized_datasets.cache_files)
```

    {'train': [{'filename': '/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/0.0.0/c1f9ee939b7d05667af864ee1cb066393154bf85/cache-42c6b839c042ef53.arrow'}], 'test': [{'filename': '/root/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/0.0.0/c1f9ee939b7d05667af864ee1cb066393154bf85/cache-90992f974cd05082.arrow'}]}



```python
# éšæœºå±•ç¤ºå¤„ç†åçš„æ ·æœ¬ï¼ˆå‡è®¾show_random_elementsæ˜¯è‡ªå®šä¹‰çš„æ£€æŸ¥å‡½æ•°ï¼‰
# é€šè¿‡è¿™ä¸ªå¯ä»¥æŸ¥çœ‹å¤„ç†åçš„æ•°æ®ç»“æ„ï¼Œä¾‹å¦‚ï¼š
# {
#   'input_ids': [101, 2345, 1032, ..., 0, 0], 
#   'attention_mask': [1,1,..1,0,0]
# }
show_random_elements(tokenized_datasets["train"], num_examples=1)
```


<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>text</th>
      <th>input_ids</th>
      <th>token_type_ids</th>
      <th>attention_mask</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5 stars</td>
      <td>I went to the Charleston location yesterday and the staff was a bit rude and didn't know their own product. Today I decided to go to the Ft. Apache and Trop location and the ladies were just darling and were great help. Ashley was wonderful and helpful. The staff went out of their way to help. One of the girls helped even tie the bowl part to the roof of my car since it wouldn't fit anywhere into the car. Thank you guys. You all ROCK :) I totally love my new papasan chair.</td>
      <td>[101, 146, 1355, 1106, 1103, 10874, 2450, 8128, 1105, 1103, 2546, 1108, 170, 2113, 14708, 1105, 1238, 112, 189, 1221, 1147, 1319, 3317, 119, 3570, 146, 1879, 1106, 1301, 1106, 1103, 143, 1204, 119, 16995, 1105, 157, 12736, 2450, 1105, 1103, 8564, 1127, 1198, 18556, 1105, 1127, 1632, 1494, 119, 9017, 1108, 7310, 1105, 14739, 119, 1109, 2546, 1355, 1149, 1104, 1147, 1236, 1106, 1494, 119, 1448, 1104, 1103, 2636, 2375, 1256, 5069, 1103, 7329, 1226, 1106, 1103, 3664, 1104, 1139, 1610, 1290, 1122, 2010, 112, 189, 4218, 5456, 1154, 1103, 1610, 119, 4514, 1128, 3713, 119, 1192, 1155, 155, ...]</td>
      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...]</td>
      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...]</td>
    </tr>
  </tbody>
</table>


ä»¥ä¸‹æ˜¯è¯¥è¡¨æ ¼ä¸­å„ä¸ªå­—æ®µçš„è¯¦ç»†è§£é‡Šï¼ŒæŒ‰NLPå¤„ç†æµç¨‹åˆ†é˜¶æ®µè¯´æ˜ï¼š

---

### â–‹ å­—æ®µç»“æ„è§£æ (é’ˆå¯¹BERTç±»æ¨¡å‹)

| å­—æ®µåç§°          | ç¤ºä¾‹å€¼ç‰‡æ®µ                     | ä½œç”¨å±‚çº§      | æŠ€æœ¯ç»†èŠ‚                                                                 |
|-------------------|------------------------------|--------------|--------------------------------------------------------------------------|
| **label**         | "1 star"                     | ä¸šåŠ¡æ ‡ç­¾å±‚    | åŸå§‹ä¸šåŠ¡æ ‡ç­¾ï¼ˆæ­¤å¤„å±•ç¤ºä¸ºå¯è¯»å½¢å¼ï¼Œå®é™…è®­ç»ƒéœ€è½¬æ¢ä¸ºæ•°å€¼å¦‚0-4å¯¹åº”1-5æ˜Ÿï¼‰    |
| **text**          | ç”¨æˆ·è¯„è®ºåŸæ–‡                 | åŸå§‹æ•°æ®å±‚    | æœªå¤„ç†çš„åŸå§‹æ–‡æœ¬è¾“å…¥                                                     |
| **input_ids**     | [101, 23158, 1204,...]       | Tokenç¼–ç å±‚   | å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯è¯†åˆ«çš„æ•°å­—IDåºåˆ—                                        |
| **token_type_ids**| [0, 0, 0,...]                | å¥å­åˆ†æ®µå±‚     | æ ‡è¯†tokenå±äºå“ªä¸ªå¥å­ï¼ˆå•å¥å­ä»»åŠ¡å…¨ä¸º0ï¼‰                                  |
| **attention_mask**| [1, 1, 1,...]                | æ³¨æ„åŠ›æœºåˆ¶å±‚   | æ§åˆ¶æ¨¡å‹å…³æ³¨æœ‰æ•ˆå†…å®¹ï¼ˆ1=æœ‰æ•ˆtokenï¼Œ0=å¡«å……ä½ï¼‰                             |

---

### â–‹ å…³é”®æŠ€æœ¯ç‚¹è¯¦è§£

#### 1. **labelå­—æ®µçš„ç‰¹æ®Šå¤„ç†**
```python
# å®é™…è®­ç»ƒæ—¶åº”è½¬æ¢ä¸ºæ•°å€¼æ ‡ç­¾
label_mapping = {"1 star": 0, "2 stars": 1, ..., "5 stars": 4}
dataset = dataset.map(lambda x: {"label": label_mapping[x["label"]]})
```

#### 2. **input_idsçš„æ„é€ è¿‡ç¨‹**
- **ç‰¹æ®Šæ ‡è®°è¯´æ˜**ï¼š
  - `101`: [CLS] åˆ†ç±»æ ‡è®°ï¼ˆBERTç­‰æ¨¡å‹çš„èµ·å§‹ç¬¦ï¼‰
  - `102`: [SEP] åˆ†éš”æ ‡è®°ï¼ˆæ­¤ä¾‹æœªå‡ºç°ï¼Œå› å•å¥è¾“å…¥ï¼‰
  - `0`: [PAD] å¡«å……æ ‡è®°ï¼ˆæ­¤ä¾‹æœªå‡ºç°ï¼Œå› å·²ç”¨max_lengthå¡«å……ï¼‰

#### 3. **token_type_idsçš„æ‰©å±•åº”ç”¨**
```python
# åŒå¥å­ä»»åŠ¡æ—¶çš„å…¸å‹ç»“æ„ï¼ˆå¦‚QAï¼‰
tokenizer("How are you?", "I'm fine", return_token_type_ids=True)
# è¾“å‡ºï¼š
# token_type_ids = [0,0,0,0,0, 1,1,1,1]
```

#### 4. **attention_maskçš„åŠ¨æ€æ€§**
```python
# å®é™…å¤„ç†å˜é•¿æ–‡æœ¬æ—¶çš„maskç¤ºä¾‹ï¼š
åŸå§‹æ–‡æœ¬: "Hello world"
å¡«å……å: "Hello world [PAD] [PAD]"
attention_mask: [1,1,0,0]
```

---

### â–‹ æ•°æ®å¤„ç†æµç¨‹å¯è§†åŒ–
```
åŸå§‹æ–‡æœ¬
   â†“ (åˆ†è¯å™¨å¤„ç†)
[CLS] Went there to... [SEP] â†’ åˆ†è¯ç»“æœ
   â†“ (è¯æ±‡è¡¨æ˜ å°„)
101 23158 1204 ... 102 â†’ input_ids
   â†“ (å¥å­æ ‡è¯†)
0   0     0    ... 0   â†’ token_type_ids
   â†“ (æœ‰æ•ˆæ ‡è¯†)
1   1     1    ... 1   â†’ attention_mask
```

---

### â–‹ æœ€ä½³å®è·µå»ºè®®
1. **åŠ¨æ€å¡«å……ç­–ç•¥**ï¼š
```python
# æ›¿ä»£å›ºå®šé•¿åº¦å¡«å……ï¼Œæå‡æ•ˆç‡
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer)
```

2. **éªŒè¯å­—æ®µä¸€è‡´æ€§**ï¼š
```python
# æ£€æŸ¥å„å­—æ®µé•¿åº¦æ˜¯å¦åŒ¹é…
assert len(input_ids) == len(token_type_ids) == len(attention_mask)
```

3. **è§£ç éªŒè¯**ï¼š
```python
# åå‘éªŒè¯ç¼–ç æ­£ç¡®æ€§
decoded_text = tokenizer.decode(input_ids, skip_special_tokens=True)
assert decoded_text == original_text
```

---

æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥äº†è§£å¦‚ä½•å°†è¿™äº›é¢„å¤„ç†åçš„æ•°æ®è¾“å…¥æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Ÿ

Wed Mar  5 15:34:53 2025       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       On  | 00000000:00:07.0 Off |                    0 |
| N/A   38C    P0              26W /  70W |    985MiB / 15360MiB |      0%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A    103559      C   /root/miniconda3/envs/peft/bin/python       982MiB |
+---------------------------------------------------------------------------------------+

æ ¹æ®æä¾›çš„æ—¥å¿—å’Œç¡¬ä»¶ç›‘æ§ä¿¡æ¯ï¼Œå½“å‰ç³»ç»ŸçŠ¶æ€å¯ä»ä»¥ä¸‹è§’åº¦åˆ†æï¼š

---

### **ä¸€ã€æ–‡ä»¶ä¸‹è½½ä¸æ¨¡å‹åŠ è½½**
1. **Tokenizer ç›¸å…³æ–‡ä»¶å¤„ç†**  
   â€¢ `tokenizer_config.json`ï¼ˆå·²å®Œæˆ100%ä¸‹è½½ï¼‰ï¼šè¯¥æ–‡ä»¶å®šä¹‰äº†åˆ†è¯å™¨çš„é…ç½®å‚æ•°ï¼ˆå¦‚æ˜¯å¦åŒºåˆ†å¤§å°å†™ã€ç‰¹æ®Šæ ‡è®°æ˜ å°„è·¯å¾„ç­‰ï¼‰ã€‚ä¾‹å¦‚ï¼Œ`do_lower_case=True` è¡¨ç¤ºè¾“å…¥æ–‡æœ¬ä¼šè¢«ç»Ÿä¸€è½¬ä¸ºå°å†™ã€‚
   â€¢ `vocab.txt`ï¼ˆä¸‹è½½ä¸­ï¼‰ï¼šè¯æ±‡è¡¨æ–‡ä»¶ï¼ŒåŒ…å«æ‰€æœ‰æ ‡è®°åŠå…¶å”¯ä¸€ç´¢å¼•ï¼Œç”¨äºå°†æ–‡æœ¬è½¬åŒ–ä¸ºæ¨¡å‹å¯è¯†åˆ«çš„æ•°å­—åºåˆ—ã€‚ä¾‹å¦‚ï¼Œ`[CLS]`å¯èƒ½å¯¹åº”ç´¢å¼•0ï¼Œ`[SEP]`å¯¹åº”ç´¢å¼•1ã€‚
   â€¢ `tokenizer.json`ï¼ˆä¸‹è½½ä¸­ï¼‰ï¼šåŒ…å«åˆ†è¯å™¨çš„å®Œæ•´é…ç½®å’Œæ¨¡å‹ç±»å‹ï¼ˆå¦‚BPEã€WordPieceï¼‰ï¼Œæ˜¯åˆ†è¯å™¨çš„æ ¸å¿ƒæ–‡ä»¶ã€‚

2. **æ¨¡å‹é…ç½®æ–‡ä»¶åŠ è½½**  
   â€¢ `config.json`ï¼ˆä¸‹è½½ä¸­ï¼‰ï¼šå®šä¹‰æ¨¡å‹æ¶æ„å‚æ•°ï¼Œå¦‚éšè—å±‚ç»´åº¦ï¼ˆ`hidden_size`ï¼‰ã€æ³¨æ„åŠ›å¤´æ•°ï¼ˆ`num_attention_heads`ï¼‰ã€å±‚æ•°ï¼ˆ`num_hidden_layers`ï¼‰ç­‰ã€‚ä¾‹å¦‚ï¼Œ`hidden_size=768`è¡¨ç¤ºæ¯å±‚æœ‰768ä¸ªç¥ç»å…ƒã€‚

3. **è¿›åº¦è§£è¯»**  
   â€¢ `Map: 35%` å¯èƒ½è¡¨ç¤ºæ¨¡å‹æƒé‡æ­£åœ¨ä»æ–‡ä»¶æ˜ å°„åˆ°å†…å­˜ï¼Œæˆ–åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆ35%ã€‚

---

### **äºŒã€GPUèµ„æºå ç”¨**
â€¢ **Tesla T4ä½¿ç”¨æƒ…å†µ**  
  â€¢ **æ˜¾å­˜å ç”¨**ï¼š985MiB/15360MiBï¼Œå æ¯”çº¦6.4%ï¼Œæ˜¾ç¤ºå½“å‰ä»»åŠ¡å¯¹GPUå‹åŠ›è¾ƒä½ã€‚
  â€¢ **è¿›ç¨‹ä¿¡æ¯**ï¼šPythonè¿›ç¨‹ï¼ˆPID 103559ï¼‰æ­£åœ¨è¿è¡Œï¼Œå¯èƒ½ä¸æ¨¡å‹æ¨ç†æˆ–è®­ç»ƒç›¸å…³ã€‚ä¾‹å¦‚ï¼ŒåŠ è½½æ¨¡å‹æƒé‡ï¼ˆå¦‚`model.safetensors`ï¼‰æˆ–æ‰§è¡Œå‰å‘è®¡ç®—ã€‚
  â€¢ **è®¡ç®—æ¨¡å¼**ï¼š`Compute M.`æ˜¾ç¤ºä¸º`Default`ï¼Œè¡¨æ˜æœªå¯ç”¨ç‰¹å®šè®¡ç®—æ¨¡å¼ï¼ˆå¦‚MIGå¤šå®ä¾‹GPUï¼‰ã€‚

---

### **ä¸‰ã€ç»¼åˆè¡Œä¸ºæ¨æ–­**
å½“å‰ç³»ç»Ÿå¯èƒ½æ­£åœ¨æ‰§è¡Œä»¥ä¸‹æ“ä½œä¹‹ä¸€ï¼š
1. **æ¨¡å‹åˆå§‹åŒ–**  
   â€¢ é€šè¿‡Hugging Faceçš„`from_pretrained()`æ–¹æ³•åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œè‡ªåŠ¨ä¸‹è½½é…ç½®æ–‡ä»¶ï¼ˆå¦‚`config.json`ï¼‰å’Œåˆ†è¯å™¨æ–‡ä»¶ã€‚
   â€¢ ç¤ºä¾‹ä»£ç ç±»ä¼¼ï¼š
     ```python
     from transformers import AutoTokenizer, AutoModel
     tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
     model = AutoModel.from_pretrained("bert-base-uncased").cuda()
     ```

2. **æ–‡æœ¬é¢„å¤„ç†**  
   â€¢ ä½¿ç”¨åˆ†è¯å™¨å°†è¾“å…¥æ–‡æœ¬ï¼ˆå¦‚ç”¨æˆ·æé—®ï¼‰è½¬æ¢ä¸ºToken IDåºåˆ—ï¼Œéœ€ä¾èµ–`vocab.txt`å’Œ`tokenizer.json`ã€‚

3. **è½»é‡çº§æ¨ç†ä»»åŠ¡**  
   â€¢ æ˜¾å­˜å ç”¨è¾ƒä½å¯èƒ½è¡¨æ˜ä»»åŠ¡è§„æ¨¡è¾ƒå°ï¼ˆå¦‚çŸ­æ–‡æœ¬åˆ†ç±»æˆ–é—®ç­”ï¼‰ï¼Œæœªæ¶‰åŠå…¨é‡è®­ç»ƒã€‚

---

### **å››ã€æ½œåœ¨é£é™©ä¸ä¼˜åŒ–å»ºè®®**
â€¢ **æ˜¾å­˜åˆ©ç”¨ç‡ä½**ï¼šTesla T4çš„æ˜¾å­˜ä½¿ç”¨ç‡ä¸è¶³10%ï¼Œå¯è€ƒè™‘æ‰¹é‡å¤„ç†ä»»åŠ¡æˆ–å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆ`fp16`/`bf16`ï¼‰ä»¥æå‡ååé‡ã€‚
â€¢ **ä¸‹è½½é€Ÿåº¦é™åˆ¶**ï¼š`5.64kB/s`çš„ä¸‹è½½é€Ÿç‡å¯èƒ½å—ç½‘ç»œå¸¦å®½å½±å“ï¼Œå»ºè®®æ£€æŸ¥ä»£ç†è®¾ç½®æˆ–åˆ‡æ¢è‡³æœ¬åœ°ç¼“å­˜æ¨¡å‹ã€‚

---

**æ€»ç»“**ï¼šç³»ç»Ÿæ­£åœ¨åŠ è½½ä¸€ä¸ªåŸºäºTransformeræ¶æ„çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚BERTæˆ–GPTï¼‰ï¼Œå®Œæˆåˆ†è¯å™¨å’Œæ¨¡å‹é…ç½®çš„åˆå§‹åŒ–ï¼Œå¹¶åˆ©ç”¨GPUæ‰§è¡Œè½»é‡çº§è®¡ç®—ä»»åŠ¡ã€‚

### æ•°æ®æŠ½æ ·

ä½¿ç”¨ 1000 ä¸ªæ•°æ®æ ·æœ¬ï¼Œåœ¨ BERT ä¸Šæ¼”ç¤ºå°è§„æ¨¡è®­ç»ƒï¼ˆåŸºäº Pytorch Trainerï¼‰

`shuffle()`å‡½æ•°ä¼šéšæœºé‡æ–°æ’åˆ—åˆ—çš„å€¼ã€‚å¦‚æœæ‚¨å¸Œæœ›å¯¹ç”¨äºæ´—ç‰Œæ•°æ®é›†çš„ç®—æ³•æœ‰æ›´å¤šæ§åˆ¶ï¼Œå¯ä»¥åœ¨æ­¤å‡½æ•°ä¸­æŒ‡å®šgeneratorå‚æ•°æ¥ä½¿ç”¨ä¸åŒçš„numpy.random.Generatorã€‚


```python
# ä»å®Œæ•´è®­ç»ƒé›†ä¸­åˆ›å»ºå°å‹è®­ç»ƒå­é›†ï¼ˆ1000æ¡æ ·æœ¬ï¼‰
small_train_dataset = tokenized_datasets["train"].shuffle(seed=42).select(range(1000))
"""
æ‰§è¡Œæ­¥éª¤ï¼š
1. shuffle(seed=42): å…ˆå¯¹è®­ç»ƒé›†è¿›è¡Œéšæœºæ‰“ä¹±ï¼ˆè®¾ç½®éšæœºç§å­ä¿è¯å¯å¤ç°æ€§ï¼‰
2. select(range(1000)): é€‰å–å‰1000æ¡æ‰“ä¹±åçš„æ ·æœ¬
ä½œç”¨ï¼š
- åˆ›å»ºå°è§„æ¨¡è®­ç»ƒé›†ï¼ŒåŠ é€Ÿå®éªŒè¿­ä»£
- ä¿æŒæ•°æ®åˆ†å¸ƒçš„éšæœºæ€§
- å›ºå®šéšæœºç§å­ä¿è¯æ¯æ¬¡è¿è¡Œç»“æœä¸€è‡´
"""

# ä»å®Œæ•´æµ‹è¯•é›†ä¸­åˆ›å»ºå°å‹éªŒè¯å­é›†ï¼ˆ1000æ¡æ ·æœ¬ï¼‰ 
small_eval_dataset = tokenized_datasets["test"].shuffle(seed=42).select(range(1000))
"""
å…¸å‹åº”ç”¨åœºæ™¯ï¼š
1. å¿«é€ŸéªŒè¯æ¨¡å‹æ˜¯å¦èƒ½è¿‡æ‹Ÿåˆï¼ˆç”¨å°‘é‡æ•°æ®æµ‹è¯•å­¦ä¹ èƒ½åŠ›ï¼‰
2. èµ„æºæœ‰é™æ—¶è¿›è¡Œè¶…å‚æ•°è°ƒè¯•
3. åŸå‹å¼€å‘é˜¶æ®µçš„å¿«é€Ÿå®éªŒ
4. æ•™å­¦æ¼”ç¤ºåœºæ™¯ï¼ˆç¼©çŸ­è®­ç»ƒæ—¶é—´ï¼‰

æ³¨æ„äº‹é¡¹ï¼ˆä½¿ç”¨æ—¶éœ€çŸ¥ï¼‰ï¼š
- å°æ ·æœ¬å¯èƒ½æ— æ³•ä»£è¡¨å®Œæ•´æ•°æ®åˆ†å¸ƒ
- è¯„ä¼°æŒ‡æ ‡ä¼šæœ‰è¾ƒå¤§æ–¹å·®
- æ­£å¼è®­ç»ƒæ—¶å»ºè®®ä½¿ç”¨å®Œæ•´æ•°æ®é›†
- ç”Ÿäº§ç¯å¢ƒéœ€è¦æ›´ä¸¥è°¨çš„éªŒè¯é›†åˆ’åˆ†
"""

# æ‰©å±•ï¼šæŸ¥çœ‹æ•°æ®é›†ç»“æ„ç¤ºä¾‹
print(small_train_dataset)
# è¾“å‡ºç¤ºä¾‹ï¼šDataset(features: ['input_ids', 'token_type_ids', 'attention_mask', 'label'], num_rows: 1000)

print(small_eval_dataset)
```

    Dataset({
        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 1000
    })
    Dataset({
        features: ['label', 'text', 'input_ids', 'token_type_ids', 'attention_mask'],
        num_rows: 1000
    })


## å¾®è°ƒè®­ç»ƒé…ç½®

### åŠ è½½ BERT æ¨¡å‹

è­¦å‘Šé€šçŸ¥æˆ‘ä»¬æ­£åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆ`vocab_transform` å’Œ `vocab_layer_norm` å±‚ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–å…¶ä»–ä¸€äº›æƒé‡ï¼ˆ`pre_classifier` å’Œ `classifier` å±‚ï¼‰ã€‚åœ¨å¾®è°ƒæ¨¡å‹æƒ…å†µä¸‹æ˜¯ç»å¯¹æ­£å¸¸çš„ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨åˆ é™¤ç”¨äºé¢„è®­ç»ƒæ¨¡å‹çš„æ©ç è¯­è¨€å»ºæ¨¡ä»»åŠ¡çš„å¤´éƒ¨ï¼Œå¹¶ç”¨ä¸€ä¸ªæ–°çš„å¤´éƒ¨æ›¿æ¢å®ƒï¼Œå¯¹äºè¿™ä¸ªæ–°å¤´éƒ¨ï¼Œæˆ‘ä»¬æ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œæ‰€ä»¥åº“ä¼šè­¦å‘Šæˆ‘ä»¬åœ¨ç”¨å®ƒè¿›è¡Œæ¨ç†ä¹‹å‰åº”è¯¥å¯¹è¿™ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬è¦åšçš„äº‹æƒ…ã€‚


```python
from transformers import AutoModelForSequenceClassification

# å…³é”®å‚æ•°è§£æï¼š
# "bert-base-cased" - ä½¿ç”¨åŒºåˆ†å¤§å°å†™çš„BERTåŸºç¡€ç‰ˆ
# num_labels=5       - äº”åˆ†ç±»ä»»åŠ¡ï¼ˆå¯¹åº”Yelpçš„1-5æ˜Ÿè¯„åˆ†ï¼‰
model = AutoModelForSequenceClassification.from_pretrained(
    "bert-base-cased", 
    num_labels=5
)
```

    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.


### è®­ç»ƒè¶…å‚æ•°ï¼ˆTrainingArgumentsï¼‰

å®Œæ•´é…ç½®å‚æ•°ä¸é»˜è®¤å€¼ï¼šhttps://huggingface.co/docs/transformers/v4.36.1/en/main_classes/trainer#transformers.TrainingArguments

æºä»£ç å®šä¹‰ï¼šhttps://github.com/huggingface/transformers/blob/v4.36.1/src/transformers/training_args.py#L161

**æœ€é‡è¦é…ç½®ï¼šæ¨¡å‹æƒé‡ä¿å­˜è·¯å¾„(output_dir)**


```python
from transformers import TrainingArguments

# ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„é…ç½®ï¼ˆæœ€é‡è¦å‚æ•°ï¼æ‰€æœ‰è®­ç»ƒäº§å‡ºç‰©éƒ½ä¼šä¿å­˜åˆ°è¿™é‡Œï¼‰
model_dir = "models/bert-base-cased-finetune-yelp"  # æ¨èä½¿ç”¨æ¨¡å‹å+ä»»åŠ¡åçš„ç›®å½•ç»“æ„

# ğŸ›ï¸ åˆ›å»ºè®­ç»ƒå‚æ•°é…ç½®å®ä¾‹
training_args = TrainingArguments(
    # å¿…é¡»å‚æ•°
    output_dir=model_dir,                   # æ¨¡å‹/æ—¥å¿—/æ£€æŸ¥ç‚¹çš„ä¿å­˜æ ¹ç›®å½•
                                          # ğŸ“‚ ç›®å½•å°†åŒ…å«ï¼š
                                          #   |- config.json
                                          #   |- trainer_state.json
                                          #   |- checkpoint-100/...
    
    # âš¡ è®­ç»ƒæ•ˆç‡å‚æ•°
    per_device_train_batch_size=16,       # æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°ï¼ˆè°ƒæ•´ä¾æ®æ˜¾å­˜ï¼‰
                                          # ğŸ’¡ 3080æ˜¾å¡å»ºè®®å€¼ï¼š16-32
                                          # â— æ€»æ‰¹æ¬¡å¤§å° = è¯¥å€¼ * GPUæ•° * gradient_accumulation_steps
    
    # â±ï¸ è®­ç»ƒæ—¶é•¿æ§åˆ¶
    num_train_epochs=5,                   # è®­ç»ƒæ€»è½®æ¬¡ï¼ˆå»ºè®®å…ˆç”¨1è½®è¯•è·‘ï¼Œå†å…¨é‡è®­ç»ƒï¼‰
                                          # ğŸ”„ 1ä¸ªepoch = å®Œæ•´è¿‡ä¸€éè®­ç»ƒé›†
    
    # ğŸ“Š æ—¥å¿—é…ç½®
    logging_steps=100,                    # æ¯è®­ç»ƒ100æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—ï¼ˆé»˜è®¤500ï¼‰
                                          # ğŸ“ˆ è®¾å°å€¼å¯æ›´å¯†é›†ç›‘æ§è®­ç»ƒè¿‡ç¨‹
                                          # ğŸ’» æ§åˆ¶å°å°†è¾“å‡ºï¼š
                                          #    Step | Loss   | Learning Rate
                                          #    100  | 0.532  | 0.00005
)

# æ‰©å±•å»ºè®®å‚æ•°ï¼ˆæ ¹æ®éœ€æ±‚æ·»åŠ ï¼‰ï¼š
"""
# ğŸ¯ ä¼˜åŒ–å™¨é…ç½®
learning_rate=5e-5,                     # BERTå¸¸ç”¨åˆå§‹å­¦ä¹ ç‡ï¼ˆ5e-5 ~ 3e-5ï¼‰
weight_decay=0.01,                      # æƒé‡è¡°å‡é˜²è¿‡æ‹Ÿåˆ

# ğŸ“‰ å­¦ä¹ ç‡è°ƒåº¦
warmup_steps=500,                       # é¢„çƒ­æ­¥æ•°ï¼ˆä»å°å­¦ä¹ ç‡é€æ­¥ä¸Šå‡ï¼‰

# ğŸ’½ å†…å­˜ä¼˜åŒ–
fp16=True,                              # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆæ˜¾å­˜å‡åŠï¼‰
gradient_accumulation_steps=2,          # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡ï¼‰

# ğŸ§ª éªŒè¯é…ç½®
evaluation_strategy="steps",             # æ¯eval_stepsè¯„ä¼°ä¸€æ¬¡
eval_steps=200,                         # è¯„ä¼°é¢‘ç‡
load_best_model_at_end=True,            # è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡å‹
"""

```




    '\n# ğŸ¯ ä¼˜åŒ–å™¨é…ç½®\nlearning_rate=5e-5,                     # BERTå¸¸ç”¨åˆå§‹å­¦ä¹ ç‡ï¼ˆ5e-5 ~ 3e-5ï¼‰\nweight_decay=0.01,                      # æƒé‡è¡°å‡é˜²è¿‡æ‹Ÿåˆ\n\n# ğŸ“‰ å­¦ä¹ ç‡è°ƒåº¦\nwarmup_steps=500,                       # é¢„çƒ­æ­¥æ•°ï¼ˆä»å°å­¦ä¹ ç‡é€æ­¥ä¸Šå‡ï¼‰\n\n# ğŸ’½ å†…å­˜ä¼˜åŒ–\nfp16=True,                              # å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆæ˜¾å­˜å‡åŠï¼‰\ngradient_accumulation_steps=2,          # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡ï¼‰\n\n# ğŸ§ª éªŒè¯é…ç½®\nevaluation_strategy="steps",             # æ¯eval_stepsè¯„ä¼°ä¸€æ¬¡\neval_steps=200,                         # è¯„ä¼°é¢‘ç‡\nload_best_model_at_end=True,            # è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡å‹\n'




```python
# å®Œæ•´çš„è¶…å‚æ•°é…ç½®
print(training_args)
```

    TrainingArguments(
    _n_gpu=1,
    adafactor=False,
    adam_beta1=0.9,
    adam_beta2=0.999,
    adam_epsilon=1e-08,
    auto_find_batch_size=False,
    bf16=False,
    bf16_full_eval=False,
    data_seed=None,
    dataloader_drop_last=False,
    dataloader_num_workers=0,
    dataloader_persistent_workers=False,
    dataloader_pin_memory=True,
    ddp_backend=None,
    ddp_broadcast_buffers=None,
    ddp_bucket_cap_mb=None,
    ddp_find_unused_parameters=None,
    ddp_timeout=1800,
    debug=[],
    deepspeed=None,
    disable_tqdm=False,
    dispatch_batches=None,
    do_eval=False,
    do_predict=False,
    do_train=False,
    eval_accumulation_steps=None,
    eval_delay=0,
    eval_steps=None,
    evaluation_strategy=no,
    fp16=False,
    fp16_backend=auto,
    fp16_full_eval=False,
    fp16_opt_level=O1,
    fsdp=[],
    fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
    fsdp_min_num_params=0,
    fsdp_transformer_layer_cls_to_wrap=None,
    full_determinism=False,
    gradient_accumulation_steps=1,
    gradient_checkpointing=False,
    gradient_checkpointing_kwargs=None,
    greater_is_better=None,
    group_by_length=False,
    half_precision_backend=auto,
    hub_always_push=False,
    hub_model_id=None,
    hub_private_repo=False,
    hub_strategy=every_save,
    hub_token=<HUB_TOKEN>,
    ignore_data_skip=False,
    include_inputs_for_metrics=False,
    include_num_input_tokens_seen=False,
    include_tokens_per_second=False,
    jit_mode_eval=False,
    label_names=None,
    label_smoothing_factor=0.0,
    learning_rate=5e-05,
    length_column_name=length,
    load_best_model_at_end=False,
    local_rank=0,
    log_level=passive,
    log_level_replica=warning,
    log_on_each_node=True,
    logging_dir=models/bert-base-cased-finetune-yelp/runs/Mar06_16-27-33_deepseek-r1-t4-test,
    logging_first_step=False,
    logging_nan_inf_filter=True,
    logging_steps=100,
    logging_strategy=steps,
    lr_scheduler_kwargs={},
    lr_scheduler_type=linear,
    max_grad_norm=1.0,
    max_steps=-1,
    metric_for_best_model=None,
    mp_parameters=,
    neftune_noise_alpha=None,
    no_cuda=False,
    num_train_epochs=5,
    optim=adamw_torch,
    optim_args=None,
    output_dir=models/bert-base-cased-finetune-yelp,
    overwrite_output_dir=False,
    past_index=-1,
    per_device_eval_batch_size=8,
    per_device_train_batch_size=16,
    prediction_loss_only=False,
    push_to_hub=False,
    push_to_hub_model_id=None,
    push_to_hub_organization=None,
    push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
    ray_scope=last,
    remove_unused_columns=True,
    report_to=[],
    resume_from_checkpoint=None,
    run_name=models/bert-base-cased-finetune-yelp,
    save_on_each_node=False,
    save_only_model=False,
    save_safetensors=True,
    save_steps=500,
    save_strategy=steps,
    save_total_limit=None,
    seed=42,
    skip_memory_metrics=True,
    split_batches=False,
    tf32=None,
    torch_compile=False,
    torch_compile_backend=None,
    torch_compile_mode=None,
    torchdynamo=None,
    tpu_metrics_debug=False,
    tpu_num_cores=None,
    use_cpu=False,
    use_ipex=False,
    use_legacy_prediction_loop=False,
    use_mps_device=False,
    warmup_ratio=0.0,
    warmup_steps=0,
    weight_decay=0.0,
    )


ä»¥ä¸‹æ˜¯å¯¹ `TrainingArguments` é…ç½®çš„é€šä¿—è§£é‡Šï¼ŒæŒ‰åŠŸèƒ½åˆ†ç±»å¹¶æ·»åŠ æ³¨é‡Šï¼š

---

### ğŸŒŸ **æ ¸å¿ƒè®­ç»ƒå‚æ•°**
```python
output_dir="models/bert-base-cased-finetune-yelp"  # æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆæœ€é‡è¦ï¼è®­ç»ƒç»“æœå…¨å­˜åœ¨è¿™ï¼‰
per_device_train_batch_size=16   # æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶è°ƒå°æ­¤å€¼ï¼‰
num_train_epochs=5               # è®­ç»ƒæ€»è½®æ¬¡ï¼ˆé€šå¸¸3-5è½®è¶³å¤Ÿå¾®è°ƒï¼‰
learning_rate=5e-05              # å­¦ä¹ ç‡ï¼ˆBERTå¸¸ç”¨5e-5, å¤ªå¤§å®¹æ˜“éœ‡è¡ï¼Œå¤ªå°æ”¶æ•›æ…¢ï¼‰
```

---

### ğŸ’» **èµ„æºç›¸å…³å‚æ•°**
```python
fp16=False                       # æ˜¯å¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆTrueå¯çœæ˜¾å­˜ï¼Œéœ€GPUæ”¯æŒï¼‰
gradient_accumulation_steps=1    # æ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼ˆæ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡ï¼Œæ˜¾å­˜ä¸è¶³æ—¶ä½¿ç”¨ï¼‰
optim="adamw_torch"              # ä¼˜åŒ–å™¨ç±»å‹ï¼ˆAdamçš„æ”¹è¿›ç‰ˆï¼Œé€‚åˆæ·±åº¦å­¦ä¹ ï¼‰
```

---

### â±ï¸ **è®­ç»ƒè¿‡ç¨‹æ§åˆ¶**
```python
logging_steps=100                # æ¯100æ­¥æ‰“å°ä¸€æ¬¡æ—¥å¿—ï¼ˆé»˜è®¤500ï¼Œè°ƒå°å¯æ›´é¢‘ç¹ç›‘æ§ï¼‰
save_steps=500                   # æ¯500æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡å‹ï¼ˆé¢‘ç¹ä¿å­˜ä¼šå ç”¨ç£ç›˜ï¼‰
evaluation_strategy="no"         # è¯„ä¼°ç­–ç•¥ï¼ˆ"no"ä¸è¯„ä¼°ï¼Œ"steps"æŒ‰æ­¥è¯„ä¼°ï¼Œ"epoch"æ¯è½®è¯„ä¼°ï¼‰
```

---

### ğŸ› ï¸ **ä¼˜åŒ–ç›¸å…³å‚æ•°**
```python
weight_decay=0.0                 # æƒé‡è¡°å‡ç³»æ•°ï¼ˆé˜²è¿‡æ‹Ÿåˆï¼Œå¸¸ç”¨0.01ï¼‰
warmup_steps=0                   # é¢„çƒ­æ­¥æ•°ï¼ˆåˆå§‹é˜¶æ®µç”¨å°å­¦ä¹ ç‡ï¼‰
max_grad_norm=1.0                # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé˜²æ¢¯åº¦çˆ†ç‚¸ï¼‰
```

---

### ğŸ“Š **æ—¥å¿—ä¸ä¿å­˜**
```python
logging_dir="models/.../runs/..." # TensorBoardæ—¥å¿—è·¯å¾„
report_to=[]                     # ä¸ŠæŠ¥å¹³å°ï¼ˆä¾‹å¦‚["wandb"]æ¥å…¥å¯è§†åŒ–ï¼‰
save_total_limit=None            # æœ€å¤§ä¿å­˜æ£€æŸ¥ç‚¹æ•°ï¼ˆè®¾ä¸º3åªä¿ç•™æœ€æ–°3ä¸ªæ¨¡å‹ï¼‰
```

---

### ğŸ”§ **å…¶ä»–å®ç”¨å‚æ•°**
```python
seed=42                          # éšæœºç§å­ï¼ˆå›ºå®šåç»“æœå¯å¤ç°ï¼‰
disable_tqdm=False               # æ˜¯å¦ç¦ç”¨è¿›åº¦æ¡ï¼ˆTrueæ—¶æ›´ç®€æ´ï¼‰
remove_unused_columns=True       # è‡ªåŠ¨åˆ é™¤æ¨¡å‹ä¸éœ€è¦çš„åˆ—ï¼ˆèŠ‚çœå†…å­˜ï¼‰
```

---

### ğŸš€ **å‚æ•°é€‰æ‹©å»ºè®®**
1. **å­¦ä¹ ç‡**ï¼šä» `5e-5` å¼€å§‹å°è¯•ï¼Œè§‚å¯ŸæŸå¤±å˜åŒ–
2. **æ‰¹æ¬¡å¤§å°**ï¼šåœ¨æ˜¾å­˜å…è®¸èŒƒå›´å†…å°½é‡è°ƒå¤§ï¼ˆå¦‚16â†’32ï¼‰
3. **è®­ç»ƒè½®æ¬¡**ï¼šç”¨æ—©åœæ³•ï¼ˆ`EarlyStoppingCallback`ï¼‰é˜²è¿‡æ‹Ÿåˆ
4. **æ··åˆç²¾åº¦**ï¼šè®¾ç½® `fp16=True` å¯å‡å°‘30%æ˜¾å­˜å ç”¨
5. **å¤šGPUæ”¯æŒ**ï¼šæ— éœ€ä¿®æ”¹ä»£ç ï¼Œå¯åŠ¨æ—¶åŠ  `--nproc_per_node=GPUæ•°é‡`

---

### âš ï¸ **ç‰¹åˆ«æ³¨æ„é¡¹**
```python
do_train=False  # å½“å‰é…ç½®æœªå¯ç”¨è®­ç»ƒï¼ï¼ˆéœ€è®¾ä¸ºTrueæ‰ä¼šå¼€å§‹è®­ç»ƒï¼‰
do_eval=False   # å½“å‰æœªå¯ç”¨éªŒè¯ï¼ï¼ˆéœ€é…åˆeval_datasetä½¿ç”¨ï¼‰
```

---

### ğŸ”„ **å®Œæ•´è®­ç»ƒå¯åŠ¨ç¤ºä¾‹**
```python
from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_data,
    eval_dataset=val_data,       # éœ€è¦è¯„ä¼°æ—¶æ·»åŠ 
    compute_metrics=compute_metrics  # è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°
)

# å¼€å§‹è®­ç»ƒï¼ˆdo_train=Trueæ—¶æ‰ç”Ÿæ•ˆï¼‰
trainer.train() 
```

---

é€šè¿‡åˆç†é…ç½®è¿™äº›å‚æ•°ï¼Œå¯ä»¥å¹³è¡¡è®­ç»ƒé€Ÿåº¦ã€èµ„æºæ¶ˆè€—å’Œæ¨¡å‹æ€§èƒ½ã€‚å»ºè®®å…ˆç”¨å°æ•°æ®å­é›†è°ƒè¯•å‚æ•°ï¼Œå†å…¨é‡è®­ç»ƒã€‚

## ä»€ä¹ˆæ˜¯å­¦ä¹ ç‡ï¼Œä»€ä¹ˆæ˜¯éœ‡è¡ï¼Ÿ

### ğŸ“š å­¦ä¹ ç‡ï¼ˆLearning Rateï¼‰é€šä¿—è§£é‡Š

**å­¦ä¹ ç‡å°±åƒã€Œä¸‹å±±çš„æ­¥é•¿ã€**  
æƒ³è±¡ä½ è’™ç€çœ¼ä»å±±é¡¶å¾€ä¸‹èµ°ï¼Œè¦æ‰¾åˆ°æœ€ä½ç‚¹ï¼ˆæ¨¡å‹çš„æœ€ä¼˜å‚æ•°ï¼‰ã€‚å­¦ä¹ ç‡å°±æ˜¯ä½ æ¯æ­¥è¿ˆçš„å¹…åº¦ï¼š
- **å¤ªå¤§æ­¥ï¼ˆé«˜å­¦ä¹ ç‡ï¼‰**ï¼šå®¹æ˜“ä¸€æ­¥è·¨è¿‡å±±è°·ï¼Œåœ¨å¯¹é¢çš„å±±å¡æ¥å›è·³ï¼ˆéœ‡è¡ï¼‰
- **å¤ªå°æ­¥ï¼ˆä½å­¦ä¹ ç‡ï¼‰**ï¼šè¦èµ°å¾ˆä¹…æ‰èƒ½åˆ°åº•ï¼Œç”šè‡³å¡åœ¨åŠå±±è…°ï¼ˆæ”¶æ•›æ…¢ï¼‰

![](https://miro.medium.com/v2/resize:fit:720/format:webp/1*kA38KJq9aeZkBW-0rH2yCg.gif)

---

### ğŸ’¥ ä»€ä¹ˆæ˜¯éœ‡è¡ï¼ˆOscillationï¼‰ï¼Ÿ

**éœ‡è¡å°±åƒã€Œåœ¨å±±è°·ä¸¤è¾¹åå¤æ¨ªè·³ã€**  
å½“å­¦ä¹ ç‡å¤ªå¤§æ—¶ï¼Œå‚æ•°æ›´æ–°ä¼šåƒè¿™æ ·ï¼š
1. å½“å‰ç‚¹ï¼šAï¼ˆæŸå¤±è¾ƒé«˜ï¼‰
2. è®¡ç®—æ¢¯åº¦ï¼šæŒ‡å‘è°·åº•æ–¹å‘
3. å¤§æ­¥æ›´æ–°ï¼šç›´æ¥è·³åˆ°å¯¹é¢çš„Bç‚¹
4. å†è®¡ç®—æ¢¯åº¦ï¼šåˆæŒ‡å‘å¦ä¸€ä¸ªæ–¹å‘
5. ç»“æœï¼šåœ¨è°·åº•ä¸¤ä¾§åå¤è·³åŠ¨ï¼Œæ— æ³•ç¨³å®šåˆ°æœ€ä½ç‚¹

![](https://developers.google.com/static/machine-learning/crash-course/images/LearningRateTooLarge.svg)

---

### ğŸŒ° å…·ä½“æ¡ˆä¾‹å¯¹æ¯”
| å­¦ä¹ ç‡ | è®­ç»ƒè¡¨ç° | æŸå¤±æ›²çº¿ | é€‚ç”¨åœºæ™¯ |
|-------|---------|---------|---------|
| 0.1   | å‰§çƒˆéœ‡è¡ | é”¯é½¿çŠ¶æ³¢åŠ¨ | âŒ å‡ ä¹ä¸ç”¨ |
| 1e-3  | å¶å°”éœ‡è¡ | æ³¢åŠ¨ä¸‹é™ | ç®€å•ä»»åŠ¡ |
| 5e-5  | å¹³ç¨³ä¸‹é™ | å¹³æ»‘æ”¶æ•› | âœ… BERTå¾®è°ƒ |
| 1e-6  | ç¼“æ…¢ä¸‹é™ | è¿‘ä¹æ°´å¹³ | ç²¾ç»†è°ƒä¼˜ |

---

### ğŸ”§ å¦‚ä½•é¿å…éœ‡è¡ï¼Ÿ
1. **å­¦ä¹ ç‡é¢„çƒ­**ï¼šå‰1000æ­¥ä»å°å­¦ä¹ ç‡é€æ­¥å¢å¤§
2. **æ¢¯åº¦è£å‰ª**ï¼šé™åˆ¶å•æ­¥æ›´æ–°å¹…åº¦ï¼ˆ`max_grad_norm=1.0`ï¼‰
3. **è‡ªé€‚åº”ä¼˜åŒ–å™¨**ï¼šä½¿ç”¨AdamWè€Œä¸æ˜¯SGD
4. **ç›‘æ§æŸå¤±æ›²çº¿**ï¼šå‡ºç°éœ‡è¡æ—¶ç«‹å³é™ä½å­¦ä¹ ç‡

---

### ğŸ› ï¸ BERTçš„å®è·µç»éªŒ
```python
# å®‰å…¨çš„å­¦ä¹ ç‡èŒƒå›´å»ºè®®
learning_rate = 5e-5  # é»˜è®¤å®‰å…¨å€¼ï¼ˆé€‚åˆå¤§éƒ¨åˆ†æƒ…å†µï¼‰
learning_rate = 3e-5  # æ›´ä¿å®ˆçš„é€‰æ‹©ï¼ˆæ•°æ®é‡å°æ—¶ï¼‰
learning_rate = 1e-4  # é«˜é£é™©ï¼éœ€é…åˆæ¢¯åº¦è£å‰ªä½¿ç”¨
```

é€šè¿‡åˆç†æ§åˆ¶å­¦ä¹ ç‡ï¼Œå¯ä»¥è®©æ¨¡å‹æ—¢å¿«é€Ÿæ”¶æ•›ï¼Œåˆä¸ä¼šã€Œè·‘è¿‡å¤´ã€ã€‚å°±åƒå¼€è½¦æ—¶æ‰¾åˆ°åˆé€‚çš„æ²¹é—¨åŠ›åº¦ï¼Œæ—¢ä¸ä¼šæ€¥åˆ¹æ€¥åœï¼Œä¹Ÿä¸ä¼šé¾Ÿé€Ÿå‰è¿›ã€‚

### è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡è¯„ä¼°ï¼ˆEvaluate)

**[Hugging Face Evaluate åº“](https://huggingface.co/docs/evaluate/index)** æ”¯æŒä½¿ç”¨ä¸€è¡Œä»£ç ï¼Œè·å¾—æ•°åç§ä¸åŒé¢†åŸŸï¼ˆè‡ªç„¶è¯­è¨€å¤„ç†ã€è®¡ç®—æœºè§†è§‰ã€å¼ºåŒ–å­¦ä¹ ç­‰ï¼‰çš„è¯„ä¼°æ–¹æ³•ã€‚ å½“å‰æ”¯æŒ **å®Œæ•´è¯„ä¼°æŒ‡æ ‡ï¼šhttps://huggingface.co/evaluate-metric**

è®­ç»ƒå™¨ï¼ˆTrainerï¼‰åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šè‡ªåŠ¨è¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å‘è®­ç»ƒå™¨ä¼ é€’ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å’ŒæŠ¥å‘ŠæŒ‡æ ‡ã€‚ 

Evaluateåº“æä¾›äº†ä¸€ä¸ªç®€å•çš„å‡†ç¡®ç‡å‡½æ•°ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨`evaluate.load`å‡½æ•°åŠ è½½


```python
import numpy as np
import evaluate
import os

# ç¡®è®¤ç¯å¢ƒå˜é‡è®¾ç½®
print("å½“å‰HFç«¯ç‚¹:", os.getenv('HF_ENDPOINT', 'é»˜è®¤ï¼ˆæœªè®¾ç½®ï¼‰'))  # åº”è¯¥æ˜¾ç¤º https://hf-mirror.com

metric = evaluate.load("./accuracy.py")
```

    å½“å‰HFç«¯ç‚¹: https://hf-mirror.com



æ¥ç€ï¼Œè°ƒç”¨ `compute` å‡½æ•°æ¥è®¡ç®—é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚

åœ¨å°†é¢„æµ‹ä¼ é€’ç»™ compute å‡½æ•°ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å°† logits è½¬æ¢ä¸ºé¢„æµ‹å€¼ï¼ˆ**æ‰€æœ‰Transformers æ¨¡å‹éƒ½è¿”å› logits**ï¼‰ã€‚


```python
def compute_metrics(eval_pred):
    """è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡çš„æ ¸å¿ƒå‡½æ•°
    
    Args:
        eval_pred (tuple): åŒ…å«æ¨¡å‹è¾“å‡ºlogitså’ŒçœŸå®æ ‡ç­¾çš„å…ƒç»„
            logits (np.ndarray): æ¨¡å‹è¾“å‡ºçš„æœªå½’ä¸€åŒ–æ¦‚ç‡åˆ†å¸ƒï¼Œshape=(batch_size, num_classes)
            labels (np.ndarray): çœŸå®æ ‡ç­¾ï¼Œshape=(batch_size,)
    
    Returns:
        dict: åŒ…å«è¯„ä¼°æŒ‡æ ‡çš„å­—å…¸ï¼Œä¾‹å¦‚ {'accuracy': 0.85}
    
    Notes:
        - é€‚ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œå¤šåˆ†ç±»åœºæ™¯éœ€ç¡®ä¿axiså‚æ•°æ­£ç¡®
        - å¤šæ ‡ç­¾åˆ†ç±»éœ€æ”¹ç”¨sigmoid +é˜ˆå€¼å¤„ç†
    """
    # è§£åŒ…æ¨¡å‹è¾“å‡ºå’Œæ ‡ç­¾
    logits, labels = eval_pred  # ç­‰ä»·äº logits = eval_pred, labels = eval_pred
    
    # å°†logitsè½¬æ¢ä¸ºé¢„æµ‹ç±»åˆ«ï¼ˆå–æ¦‚ç‡æœ€å¤§çš„ç±»åˆ«ï¼‰
    predictions = np.argmax(logits, axis=-1)  # axis=-1è¡¨ç¤ºæœ€åä¸€ä¸ªç»´åº¦ï¼ˆåˆ†ç±»ç»´åº¦ï¼‰
    # ç¤ºä¾‹ï¼šlogits.shape=(32,5)â†’predictions.shape=(32,)
    
    # è°ƒç”¨è¯„ä¼°æŒ‡æ ‡è®¡ç®—ï¼ˆå‡è®¾metricæ˜¯accuracyï¼‰
    return metric.compute(
        predictions=predictions,  # æ¨¡å‹é¢„æµ‹çš„ç±»åˆ«ç´¢å¼•
        references=labels         # çœŸå®çš„ç±»åˆ«ç´¢å¼•
    )


# å…¸å‹åº”ç”¨åœºæ™¯ç¤ºä¾‹
"""
è¾“å…¥æ ·ä¾‹ï¼š
eval_pred = (
    np.array([[1.2, -0.5], [0.3, 2.1]], dtype=np.float32),  # logitsï¼ˆbatch_size=2, num_classes=2ï¼‰
    np.array([0, 1], dtype=np.int32)                        # labels
)
è¾“å‡ºç»“æœï¼š
{'accuracy': 0.5}  # ç¬¬0ä¸ªæ ·æœ¬é¢„æµ‹æ­£ç¡®ï¼Œç¬¬1ä¸ªé¢„æµ‹é”™è¯¯ï¼ˆargmax([0.3,2.1])=1ï¼Œä½†æ ‡ç­¾æ˜¯1ï¼Œå®é™…åº”è¯¥æ­£ç¡®ï¼Ÿè¿™é‡Œå¯èƒ½éœ€è¦æ£€æŸ¥ç¤ºä¾‹æ•°æ®ï¼‰
"""

# æ‰©å±•åŠŸèƒ½å»ºè®®
"""
1. å¤šæŒ‡æ ‡è®¡ç®—ï¼š
   f1 = evaluate.load("f1")
   return {
       "accuracy": metric.compute(...),
       "f1": f1.compute(...)
   }

2. å¤„ç†å¤šç»´åº¦è¾“å‡ºï¼ˆå¦‚NERï¼‰ï¼š
   predictions = np.argmax(logits, axis=-1)  # shape=(batch_size, seq_len)

3. æ¦‚ç‡æ ¡å‡†ï¼š
   probs = softmax(logits, axis=-1)
   return {"roc_auc": roc_auc_score(labels, probs[:,1])}
"""

```




    '\n1. å¤šæŒ‡æ ‡è®¡ç®—ï¼š\n   f1 = evaluate.load("f1")\n   return {\n       "accuracy": metric.compute(...),\n       "f1": f1.compute(...)\n   }\n\n2. å¤„ç†å¤šç»´åº¦è¾“å‡ºï¼ˆå¦‚NERï¼‰ï¼š\n   predictions = np.argmax(logits, axis=-1)  # shape=(batch_size, seq_len)\n\n3. æ¦‚ç‡æ ¡å‡†ï¼š\n   probs = softmax(logits, axis=-1)\n   return {"roc_auc": roc_auc_score(labels, probs[:,1])}\n'



#### è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡ç›‘æ§

é€šå¸¸ï¼Œä¸ºäº†ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¯„ä¼°æŒ‡æ ‡å˜åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨`TrainingArguments`æŒ‡å®š`evaluation_strategy`å‚æ•°ï¼Œä»¥ä¾¿åœ¨ epoch ç»“æŸæ—¶æŠ¥å‘Šè¯„ä¼°æŒ‡æ ‡ã€‚


```python
from transformers import TrainingArguments, Trainer

# ğŸ›ï¸ è®­ç»ƒå‚æ•°é…ç½®
training_args = TrainingArguments(
    # å¿…é¡»å‚æ•°
    output_dir=model_dir,  # æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆâš ï¸é‡è¦ï¼ä¼šè‡ªåŠ¨åˆ›å»ºä»¥ä¸‹å†…å®¹ï¼‰
    # ğŸ“‚ ç›®å½•ç»“æ„ç¤ºä¾‹ï¼š
    #   â”œâ”€â”€ config.json
    #   â”œâ”€â”€ runs/ (TensorBoardæ—¥å¿—)
    #   â””â”€â”€ checkpoint-100/ (è‡ªåŠ¨ä¿å­˜çš„æ£€æŸ¥ç‚¹)
    
    # ğŸ”„ éªŒè¯ç­–ç•¥
    evaluation_strategy="epoch",  # æ¯ä¸ªepochåéªŒè¯ï¼ˆå¯é€‰ï¼š"steps"æŒ‰æ­¥éªŒè¯/"no"ä¸éªŒè¯ï¼‰
    # eval_steps=500,           # å½“evaluation_strategy="steps"æ—¶ï¼Œæ¯500æ­¥éªŒè¯ä¸€æ¬¡
    
    # ğŸš€ è®­ç»ƒæ•ˆç‡å‚æ•°
    per_device_train_batch_size=16,  # æ¯ä¸ªGPUçš„æ‰¹æ¬¡å¤§å°ï¼ˆè°ƒæ•´ä¾æ®æ˜¾å­˜ï¼‰
    # æ€»æ‰¹æ¬¡å¤§å° = 16 * GPUæ•°é‡ * gradient_accumulation_steps
    
    # â±ï¸ è®­ç»ƒæ—¶é•¿æ§åˆ¶
    num_train_epochs=3,           # è®­ç»ƒæ€»è½®æ¬¡ï¼ˆæ¨è3-5è½®ç”¨äºå¾®è°ƒï¼‰
    
    # ğŸ“Š æ—¥å¿—ä¸ç›‘æ§
    logging_steps=30,             # æ¯30è®­ç»ƒæ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—ï¼ˆé»˜è®¤500ï¼‰
    # report_to="wandb",         # é›†æˆå¯è§†åŒ–å·¥å…·ï¼ˆå¯é€‰ï¼š"tensorboard"/"wandb"ï¼‰
    
    # ğŸ’¡ æ¨èæ·»åŠ çš„ä¼˜åŒ–å‚æ•°
    # learning_rate=5e-5,        # BERTå¸¸ç”¨å­¦ä¹ ç‡ï¼ˆé»˜è®¤5e-5ï¼‰
    # fp16=True,                # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆèŠ‚çœ30%æ˜¾å­˜ï¼‰
    # gradient_accumulation_steps=2, # æ¢¯åº¦ç´¯ç§¯ï¼ˆæ¨¡æ‹Ÿæ›´å¤§æ‰¹æ¬¡ï¼‰
    # warmup_steps=500,         # å­¦ä¹ ç‡é¢„çƒ­æ­¥æ•°
    # load_best_model_at_end=True, # è®­ç»ƒå®Œæˆæ—¶åŠ è½½æœ€ä½³æ¨¡å‹
)

# å…¸å‹åº”ç”¨åœºæ™¯ç¤ºä¾‹
"""
# åˆå§‹åŒ–è®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics  # éœ€è¦è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°
)

# å¯åŠ¨è®­ç»ƒ
trainer.train() 

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
trainer.save_model("final_model")
"""

```




    '\n# åˆå§‹åŒ–è®­ç»ƒå™¨\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    compute_metrics=compute_metrics  # éœ€è¦è‡ªå®šä¹‰è¯„ä¼°å‡½æ•°\n)\n\n# å¯åŠ¨è®­ç»ƒ\ntrainer.train() \n\n# ä¿å­˜æœ€ç»ˆæ¨¡å‹\ntrainer.save_model("final_model")\n'



## å¼€å§‹è®­ç»ƒ

### å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼ˆTrainerï¼‰

`kernel version` ç‰ˆæœ¬é—®é¢˜ï¼šæš‚ä¸å½±å“æœ¬ç¤ºä¾‹ä»£ç è¿è¡Œ


```python
# åˆå§‹åŒ–è®­ç»ƒå™¨ (æ ¸å¿ƒè®­ç»ƒå¼•æ“)
trainer = Trainer(
    # ğŸ§  æ¨¡å‹é…ç½®
    model=model,  # è¦è®­ç»ƒçš„æ¨¡å‹å®ä¾‹ï¼ˆéœ€ç»§æ‰¿è‡ªPreTrainedModelï¼‰
    # ç¤ºä¾‹ï¼šAutoModelForSequenceClassification.from_pretrained(...)
    
    # âš™ï¸ è®­ç»ƒå‚æ•°é…ç½®
    args=training_args,  # è®­ç»ƒå‚æ•°å¯¹è±¡ï¼ˆåŒ…å«batch_size/epochsç­‰è¶…å‚æ•°ï¼‰
    # é€šè¿‡TrainingArgumentsç±»åˆ›å»ºï¼Œæ§åˆ¶è®­ç»ƒå…¨è¿‡ç¨‹
    
    # ğŸ“¦ æ•°æ®é…ç½®
    train_dataset=small_train_dataset,  # è®­ç»ƒé›†ï¼ˆDatasetå¯¹è±¡ï¼‰
    # å»ºè®®æ ¼å¼ï¼šdatasets.Datasetæˆ–torch.utils.data.Dataset
    eval_dataset=small_eval_dataset,    # éªŒè¯é›†ï¼ˆç”¨äºè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼‰
    # æ•°æ®é¢„å¤„ç†åº”åœ¨åŠ è½½datasetå‰å®Œæˆ
    
    # ğŸ“Š è¯„ä¼°æŒ‡æ ‡é…ç½®
    compute_metrics=compute_metrics,  # è‡ªå®šä¹‰è¯„ä¼°æŒ‡æ ‡è®¡ç®—å‡½æ•°
    # è¯¥å‡½æ•°æ¥æ”¶eval_pred(logits, labels)ï¼Œè¿”å›æŒ‡æ ‡å­—å…¸
    # ç¤ºä¾‹ï¼šè®¡ç®—å‡†ç¡®ç‡/å¬å›ç‡ç­‰
    
    # ğŸ”„ å¯é€‰é«˜çº§é…ç½®ï¼ˆæŒ‰éœ€æ·»åŠ ï¼‰
    # data_collator=collate_fn,        # è‡ªå®šä¹‰æ‰¹æ¬¡æ•°æ®æ‰“åŒ…é€»è¾‘
    # callbacks=[EarlyStoppingCallback(...)], # è®­ç»ƒå›è°ƒï¼ˆå¦‚æ—©åœï¼‰
    # tokenizer=tokenizer,            # ç”¨äºæ•°æ®é¢„å¤„ç†è®°å½•
)

# å…¸å‹è®­ç»ƒæµç¨‹
"""
1. å¯åŠ¨è®­ç»ƒï¼š
   trainer.train() 
   
2. è¯„ä¼°æ¨¡å‹ï¼š
   eval_results = trainer.evaluate()
   
3. ä¿å­˜æœ€ä½³æ¨¡å‹ï¼š
   trainer.save_model("best_model")
   
4. æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼š
   !tensorboard --logdir=models/bert-base-cased-finetune-yelp/runs/
"""

# å‚æ•°è¯¦è§£è¡¨
"""
| å‚æ•°                | ä½œç”¨                                 | å…¸å‹å€¼ç¤ºä¾‹                     |
|---------------------|--------------------------------------|-------------------------------|
| model               | å®šä¹‰æ¨¡å‹æ¶æ„                         | BertForSequenceClassification |
| args                | æ§åˆ¶è®­ç»ƒè¶…å‚æ•°                       | TrainingArgumentså®ä¾‹         |
| train_dataset       | æ¨¡å‹å­¦ä¹ çš„è®­ç»ƒæ•°æ®                   | Datasetå¯¹è±¡ï¼ˆ1wæ¡æ ·æœ¬ï¼‰       |
| eval_dataset        | ç›‘æ§æ¨¡å‹æ€§èƒ½çš„éªŒè¯æ•°æ®               | Datasetå¯¹è±¡ï¼ˆ2kæ¡æ ·æœ¬ï¼‰       |
| compute_metrics     | å®šä¹‰è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ–¹æ³•                 | è¿”å›{accuracy: 0.85}çš„å‡½æ•°    |
| data_collator       | è‡ªå®šä¹‰æ•°æ®æ‰“åŒ…é€»è¾‘ï¼ˆå¦‚åŠ¨æ€å¡«å……ï¼‰      | DataCollatorWithPadding       |
"""

```

    Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.





    '\n| å‚æ•°                | ä½œç”¨                                 | å…¸å‹å€¼ç¤ºä¾‹                     |\n|---------------------|--------------------------------------|-------------------------------|\n| model               | å®šä¹‰æ¨¡å‹æ¶æ„                         | BertForSequenceClassification |\n| args                | æ§åˆ¶è®­ç»ƒè¶…å‚æ•°                       | TrainingArgumentså®ä¾‹         |\n| train_dataset       | æ¨¡å‹å­¦ä¹ çš„è®­ç»ƒæ•°æ®                   | Datasetå¯¹è±¡ï¼ˆ1wæ¡æ ·æœ¬ï¼‰       |\n| eval_dataset        | ç›‘æ§æ¨¡å‹æ€§èƒ½çš„éªŒè¯æ•°æ®               | Datasetå¯¹è±¡ï¼ˆ2kæ¡æ ·æœ¬ï¼‰       |\n| compute_metrics     | å®šä¹‰è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ–¹æ³•                 | è¿”å›{accuracy: 0.85}çš„å‡½æ•°    |\n| data_collator       | è‡ªå®šä¹‰æ•°æ®æ‰“åŒ…é€»è¾‘ï¼ˆå¦‚åŠ¨æ€å¡«å……ï¼‰      | DataCollatorWithPadding       |\n'



## ä½¿ç”¨ nvidia-smi æŸ¥çœ‹ GPU ä½¿ç”¨

ä¸ºäº†å®æ—¶æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µï¼Œå¯ä»¥ä½¿ç”¨ `watch` æŒ‡ä»¤å®ç°è½®è¯¢ï¼š`watch -n 1 nvidia-smi`:

```shell
Every 1.0s: nvidia-smi                                                   Wed Dec 20 14:37:41 2023

Wed Dec 20 14:37:41 2023
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  Tesla T4                       Off | 00000000:00:0D.0 Off |                    0 |
| N/A   64C    P0              69W /  70W |   6665MiB / 15360MiB |     98%      Default |
|                                         |                      |                  N/A |
+-----------------------------------------+----------------------+----------------------+

+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|    0   N/A  N/A     18395      C   /root/miniconda3/bin/python                6660MiB |
+---------------------------------------------------------------------------------------+
```


```python
trainer.train()
```



    <div>

      <progress value='189' max='189' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [189/189 05:38, Epoch 3/3]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1.449800</td>
      <td>1.203277</td>
      <td>0.500000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.031600</td>
      <td>0.998879</td>
      <td>0.576000</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.765300</td>
      <td>0.973219</td>
      <td>0.590000</td>
    </tr>
  </tbody>
</table><p>





    TrainOutput(global_step=189, training_loss=1.1220053264072962, metrics={'train_runtime': 341.0837, 'train_samples_per_second': 8.795, 'train_steps_per_second': 0.554, 'total_flos': 789354427392000.0, 'train_loss': 1.1220053264072962, 'epoch': 3.0})




```python
small_test_dataset = tokenized_datasets["test"].shuffle(seed=64).select(range(100))
```


```python
trainer.evaluate(small_test_dataset)
```



<div>

  <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [13/13 00:02]
</div>






    {'eval_loss': 1.0240269899368286,
     'eval_accuracy': 0.49,
     'eval_runtime': 2.9923,
     'eval_samples_per_second': 33.419,
     'eval_steps_per_second': 4.344,
     'epoch': 3.0}



### ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€

- ä½¿ç”¨ `trainer.save_model` æ–¹æ³•ä¿å­˜æ¨¡å‹ï¼Œåç»­å¯ä»¥é€šè¿‡ from_pretrained() æ–¹æ³•é‡æ–°åŠ è½½
- ä½¿ç”¨ `trainer.save_state` æ–¹æ³•ä¿å­˜è®­ç»ƒçŠ¶æ€


```python
trainer.save_model(model_dir)
```


```python

```


```python
trainer.save_state()
```


```python

```


```python
# trainer.model.save_pretrained("./")
```

## Homework: ä½¿ç”¨å®Œæ•´çš„ YelpReviewFull æ•°æ®é›†è®­ç»ƒï¼Œçœ‹ Acc æœ€é«˜èƒ½åˆ°å¤šå°‘


```python
import os
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # ğŸ‘ˆ å¿…é¡»æ”¾åœ¨æ‰€æœ‰importä¹‹å‰
```


```python
# åŠ è½½å®Œæ•´æ•°æ®é›†ï¼ˆå·²ä¸‹è½½æ—¶è‡ªåŠ¨ä½¿ç”¨ç¼“å­˜ï¼‰
from datasets import load_dataset
dataset = load_dataset("yelp_review_full")
```


```python
dataset
```




    DatasetDict({
        train: Dataset({
            features: ['label', 'text'],
            num_rows: 650000
        })
        test: Dataset({
            features: ['label', 'text'],
            num_rows: 50000
        })
    })




```python
dataset["train"][666]
```




    {'label': 2,
     'text': 'Just ate there, right next to GameStop & Google, has 3 small booths, & ordered the pepper steak w/ onion ($10.95). Food is fast fresh & hot, but mine had too much onion & not enough steak. At the end of the meal I was just eating onions with rice, though I hear this is healthy for you. Counter lady was cordial, but didn\'t reply when customers told her, \\"Have a nice day\\" #awkward. I know that English isn\'t her first language but she needs to catch on that people are wishing her well. Wasn\'t stuffed full either despite having eaten a large plate (I usually get this feeling eating Asian). This is basically a nice place to go for lunch that won\'t ruin your appetite for dinner. (Side note: Food is very clean. Brushed my teeth an hour before w/ Tom\'s of Maine fluoride-free peppermint & still had minty fresh breath an hour after eating)'}




```python
# é¢„å¤„ç†æ•°æ®
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")


def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True)


tokenized_datasets = dataset.map(tokenize_function, batched=True)
```

    /root/miniconda3/envs/peft/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
      warnings.warn(



    Map:   0%|          | 0/50000 [00:00<?, ? examples/s]



```python
print("å¯ç”¨åˆ’åˆ†:", list(tokenized_datasets.keys()))
```

    å¯ç”¨åˆ’åˆ†: ['train', 'test']



```python
## å¾®è°ƒè®­ç»ƒé…ç½®
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained("bert-base-cased", num_labels=5)
```

    Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
    You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.



```python
### è®­ç»ƒè¶…å‚æ•°ï¼ˆTrainingArgumentsï¼‰
from transformers import TrainingArguments

model_dir = "models/bert-base-cased-finetune-yelp-full"

# # logging_steps é»˜è®¤å€¼ä¸º500ï¼Œæ ¹æ®æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®å’Œæ­¥é•¿ï¼Œå°†å…¶è®¾ç½®ä¸º100
# training_args = TrainingArguments(output_dir=model_dir,
#                                   per_device_train_batch_size=16,
#                                   num_train_epochs=5,
#                                   logging_steps=100)
```


```python
### è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŒ‡æ ‡è¯„ä¼°ï¼ˆEvaluate)
import numpy as np
import evaluate

metric = evaluate.load("./accuracy.py")
```


```python
# è°ƒç”¨ `compute` å‡½æ•°æ¥è®¡ç®—é¢„æµ‹çš„å‡†ç¡®ç‡ã€‚
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)
```


```python
from tensorboard import version
print("TensorBoard ç‰ˆæœ¬:", version.VERSION)
```

    TensorBoard ç‰ˆæœ¬: 2.19.0



```python
import socket
# åˆ›å»ºTCPå¥—æ¥å­—
sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
# å°è¯•è¿æ¥æœ¬åœ°8001ç«¯å£ï¼ˆéé˜»å¡æ–¹å¼ï¼‰
result = sock.connect_ex(('localhost', 8001))
# æ–­è¨€éªŒè¯ï¼ˆ0è¡¨ç¤ºç«¯å£å¼€æ”¾ï¼‰
assert result == 0, "TensorBoard ç«¯å£ 8001 æœªå¼€å¯ï¼"
```


```python
#### è®­ç»ƒè¿‡ç¨‹æŒ‡æ ‡ç›‘æ§
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    # è¾“å‡ºç›®å½•ï¼šä¿å­˜æ¨¡å‹å’Œæ—¥å¿—çš„æ ¸å¿ƒè·¯å¾„
    output_dir=model_dir,  

    report_to="tensorboard",          # å¯ç”¨TensorBoard
    logging_dir="models/bert-base-cased-finetune-yelp-full/runs",  # æ˜ç¡®æ—¥å¿—è·¯å¾„

    # ç»´æŒå·¥ä½œè¿›ç¨‹
    dataloader_persistent_workers=True,
    
    # è¯„ä¼°ç­–ç•¥ï¼šæ¯ä¸ªepochç»“æŸååœ¨éªŒè¯é›†è¯„ä¼°ï¼ˆæ›´è€—æ—¶ä½†ç»“æœå‡†ç¡®ï¼‰
    evaluation_strategy="epoch",  
    
    # æ‰¹æ¬¡é…ç½®ï¼šç‰©ç†batch_size=8ï¼Œé€šè¿‡2æ¬¡æ¢¯åº¦ç´¯ç§¯ç­‰æ•ˆäº16
    per_device_train_batch_size=12,     # é€‚åˆT4ç­‰ä¸­ç­‰æ˜¾å­˜GPU
    gradient_accumulation_steps=2,     # ç´¯è®¡2ä¸ªbatchçš„æ¢¯åº¦å†æ›´æ–°å‚æ•°ï¼Œç­‰æ•ˆbatch_size=24
    
    # è®­ç»ƒè½®æ¬¡ï¼š3è½®åœ¨650kæ•°æ®ä¸‹å¯èƒ½ç•¥å°‘ï¼ˆæ¨è5-10è½®ï¼‰
    num_train_epochs=8,  
    
    # æ—¥å¿—è®°å½•ï¼šæ¯100æ­¥æ‰“å°æ—¥å¿—ï¼ˆçº¦æ¯100*8=800æ ·æœ¬è®°å½•ä¸€æ¬¡ï¼‰
    logging_steps=200,  
    
    # æ··åˆç²¾åº¦ï¼šå¼€å¯FP16è®­ç»ƒï¼ˆéœ€GPUæ”¯æŒï¼‰
    fp16=True,
    learning_rate=2e-5,          # é‡è¦ï¼BERTå¾®è°ƒé»„é‡‘å­¦ä¹ ç‡ # ä»3e-5â†’2e-5ï¼Œæ›´ç¨³å®šæ”¶æ•›
    weight_decay=0.05,           # é˜²æ­¢è¿‡æ‹Ÿåˆ # ä»0.01â†’0.05ï¼Œå¢å¼ºæ­£åˆ™åŒ–
    warmup_ratio=0.1,            # å‰10%æ­¥æ•°ç”¨äºå­¦ä¹ ç‡é¢„çƒ­
    save_strategy="epoch",       # æ¯ä¸ªepochä¿å­˜æ£€æŸ¥ç‚¹
    load_best_model_at_end=True, # è®­ç»ƒç»“æŸåŠ è½½æœ€ä½³æ¨¡å‹
    metric_for_best_model="accuracy",
    lr_scheduler_type="cosine",         # æ–°å¢ï¼šä½™å¼¦é€€ç«è°ƒåº¦
    # CPUå¹¶è¡Œä¼˜åŒ–é…ç½®ï¼ˆé‡ç‚¹è°ƒæ•´éƒ¨åˆ†ï¼‰
    dataloader_num_workers=4        # â† æ ¹æ®8æ ¸è®¾ç½®ä¸º4ï¼ˆæœ€ä½³å®è·µï¼šæ ¸å¿ƒæ•°çš„50%ï¼‰
)
```


```python
# å®Œæ•´çš„è¶…å‚æ•°é…ç½®
print(training_args)
```

    TrainingArguments(
    _n_gpu=1,
    adafactor=False,
    adam_beta1=0.9,
    adam_beta2=0.999,
    adam_epsilon=1e-08,
    auto_find_batch_size=False,
    bf16=False,
    bf16_full_eval=False,
    data_seed=None,
    dataloader_drop_last=False,
    dataloader_num_workers=4,
    dataloader_persistent_workers=True,
    dataloader_pin_memory=True,
    ddp_backend=None,
    ddp_broadcast_buffers=None,
    ddp_bucket_cap_mb=None,
    ddp_find_unused_parameters=None,
    ddp_timeout=1800,
    debug=[],
    deepspeed=None,
    disable_tqdm=False,
    dispatch_batches=None,
    do_eval=True,
    do_predict=False,
    do_train=False,
    eval_accumulation_steps=None,
    eval_delay=0,
    eval_steps=None,
    evaluation_strategy=epoch,
    fp16=True,
    fp16_backend=auto,
    fp16_full_eval=False,
    fp16_opt_level=O1,
    fsdp=[],
    fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},
    fsdp_min_num_params=0,
    fsdp_transformer_layer_cls_to_wrap=None,
    full_determinism=False,
    gradient_accumulation_steps=2,
    gradient_checkpointing=False,
    gradient_checkpointing_kwargs=None,
    greater_is_better=True,
    group_by_length=False,
    half_precision_backend=auto,
    hub_always_push=False,
    hub_model_id=None,
    hub_private_repo=False,
    hub_strategy=every_save,
    hub_token=<HUB_TOKEN>,
    ignore_data_skip=False,
    include_inputs_for_metrics=False,
    include_num_input_tokens_seen=False,
    include_tokens_per_second=False,
    jit_mode_eval=False,
    label_names=None,
    label_smoothing_factor=0.0,
    learning_rate=2e-05,
    length_column_name=length,
    load_best_model_at_end=True,
    local_rank=0,
    log_level=passive,
    log_level_replica=warning,
    log_on_each_node=True,
    logging_dir=models/bert-base-cased-finetune-yelp-full/runs,
    logging_first_step=False,
    logging_nan_inf_filter=True,
    logging_steps=200,
    logging_strategy=steps,
    lr_scheduler_kwargs={},
    lr_scheduler_type=cosine,
    max_grad_norm=1.0,
    max_steps=-1,
    metric_for_best_model=accuracy,
    mp_parameters=,
    neftune_noise_alpha=None,
    no_cuda=False,
    num_train_epochs=8,
    optim=adamw_torch,
    optim_args=None,
    output_dir=models/bert-base-cased-finetune-yelp-full,
    overwrite_output_dir=False,
    past_index=-1,
    per_device_eval_batch_size=8,
    per_device_train_batch_size=12,
    prediction_loss_only=False,
    push_to_hub=False,
    push_to_hub_model_id=None,
    push_to_hub_organization=None,
    push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
    ray_scope=last,
    remove_unused_columns=True,
    report_to=['tensorboard'],
    resume_from_checkpoint=None,
    run_name=models/bert-base-cased-finetune-yelp-full,
    save_on_each_node=False,
    save_only_model=False,
    save_safetensors=True,
    save_steps=500,
    save_strategy=epoch,
    save_total_limit=None,
    seed=42,
    skip_memory_metrics=True,
    split_batches=False,
    tf32=None,
    torch_compile=False,
    torch_compile_backend=None,
    torch_compile_mode=None,
    torchdynamo=None,
    tpu_metrics_debug=False,
    tpu_num_cores=None,
    use_cpu=False,
    use_ipex=False,
    use_legacy_prediction_loop=False,
    use_mps_device=False,
    warmup_ratio=0.1,
    warmup_steps=0,
    weight_decay=0.05,
    )



```python
from transformers import DataCollatorWithPadding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # â† æ–°å¢è¿™è¡Œ

## å¼€å§‹è®­ç»ƒ
### å®ä¾‹åŒ–è®­ç»ƒå™¨ï¼ˆTrainerï¼‰
train_dataset = tokenized_datasets["train"]
eval_dataset = tokenized_datasets["test"]

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)
```

    Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.



```python
trainer.train(resume_from_checkpoint=True)
```



    <div>

      <progress value='216664' max='216664' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [216664/216664 32:11:53, Epoch 7/8]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Epoch</th>
      <th>Training Loss</th>
      <th>Validation Loss</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2</td>
      <td>0.633500</td>
      <td>0.731120</td>
      <td>0.686660</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.466700</td>
      <td>0.849862</td>
      <td>0.679700</td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.273800</td>
      <td>1.191260</td>
      <td>0.671200</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.208900</td>
      <td>1.318907</td>
      <td>0.669620</td>
    </tr>
  </tbody>
</table><p>





    TrainOutput(global_step=216664, training_loss=0.2978812934675971, metrics={'train_runtime': 115915.1343, 'train_samples_per_second': 44.86, 'train_steps_per_second': 1.869, 'total_flos': 1.2306866400800532e+18, 'train_loss': 0.2978812934675971, 'epoch': 8.0})




```python
# 1. åˆ›å»ºå°å‹æµ‹è¯•é›†
small_test_dataset = tokenized_datasets["test"].shuffle(seed=64).select(range(100))
# 2. ä½¿ç”¨å°å‹æµ‹è¯•é›†è¯„ä¼°
small_results = trainer.evaluate(small_test_dataset)
print("\nğŸ“Š è¯„ä¼°ç»“æœè¯¦æƒ…ï¼š")
for key, value in small_results.items():
    if isinstance(value, float):
        print(f"{key:25} â†’ {value:.4f}")  # æµ®ç‚¹æ•°ä¿ç•™4ä½å°æ•°
    else:
        print(f"{key:25} â†’ {value}")
```



<div>

  <progress value='1860' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>
  [13/13 02:13]
</div>



    
    ğŸ“Š è¯„ä¼°ç»“æœè¯¦æƒ…ï¼š
    eval_loss                 â†’ 0.8131
    eval_accuracy             â†’ 0.6700
    eval_runtime              â†’ 0.9124
    eval_samples_per_second   â†’ 109.5980
    eval_steps_per_second     â†’ 14.2480
    epoch                     â†’ 8.0000



```python
# ä½¿ç”¨å®Œæ•´æµ‹è¯•é›†è¯„ä¼°
full_results = trainer.evaluate(tokenized_datasets["test"])
print("\nğŸ“Š è¯„ä¼°ç»“æœè¯¦æƒ…ï¼š")
for key, value in full_results.items():
    if isinstance(value, float):
        print(f"{key:25} â†’ {value:.4f}")  # æµ®ç‚¹æ•°ä¿ç•™4ä½å°æ•°
    else:
        print(f"{key:25} â†’ {value}")
```


```python
### ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒçŠ¶æ€
trainer.save_model(model_dir)
trainer.save_state()
```

## ä¸­æ–­è®­ç»ƒå¹¶æ¸…ç†æ˜¾å­˜


```python
# åœ¨ä¸­æ–­åç«‹å³æ‰§è¡Œ
trainer.save_model("manual_checkpoint")  # ä¿å­˜æ¨¡å‹æƒé‡
trainer.save_state()  # ä¿å­˜ä¼˜åŒ–å™¨/è°ƒåº¦å™¨çŠ¶æ€
```


```python
# å¤åˆ¶æ—¥å¿—æ–‡ä»¶åˆ°å®‰å…¨ä½ç½®
cp -r models/bert-base-cased-finetune-yelp-full/runs models/bert-base-cased-finetune-yelp-full/backup/

```


      Cell In[16], line 2
        cp -r models/bert-base-cased-finetune-yelp-full/runs models/bert-base-cased-finetune-yelp-full/backup/
              ^
    SyntaxError: invalid syntax




```python
import torch
from IPython import display

display.clear_output(wait=True)  # æ¸…ç†è¾“å‡º
torch.cuda.empty_cache()  # æ¸…ç©ºæ˜¾å­˜
```


```python
# ä¿®æ”¹å‚æ•°é˜²æ­¢æ¢å¤åæ­»é”
training_args.dataloader_persistent_workers = False
```


```python

```
